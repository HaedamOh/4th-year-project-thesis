@article{Fankhauser2018a,
abstract = {Permanent Link: https://doi.},
author = {Fankhauser, P{\'{e}}ter},
file = {:home/matias/Documents/Mendeley Desktop/Fankhauser/2018/Fankhauser - 2018 - Perceptive Locomotion for Legged Robots in Rough Terrain.pdf:pdf},
journal = {ETH Zurich},
pages = {177},
title = {{Perceptive Locomotion for Legged Robots in Rough Terrain}},
year = {2018}
}
@article{Fankhauser2018,
abstract = {Mobile robots build on accurate, real-time mapping with onboard range sensors to achieve autonomous navigation over rough terrain. Existing approaches often rely on absolute localization based on tracking of external geometric or visual features. To circumvent the reliability issues of these approaches, we propose a novel terrain mapping method, which bases on proprioceptive localization from kinematic and inertial measurements only. The proposed method incorporates the drift and uncertainties of the state estimation and a noise model of the distance sensor. It yields a probabilistic terrain estimate as a grid-based elevation map including upper and lower confidence bounds. We demonstrate the effectiveness of our approach with simulated datasets and real-world experiments for real-time terrain mapping with legged robots and compare the terrain reconstruction to ground truth reference maps.},
author = {Fankhauser, P{\'{e}}ter and Bloesch, Michael and Hutter, Marco},
doi = {10.1109/LRA.2018.2849506},
file = {:home/matias/Documents/Mendeley Desktop/Fankhauser, Bloesch, Hutter/2018/Fankhauser, Bloesch, Hutter - 2018 - Probabilistic terrain mapping for mobile robots with uncertain localization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mapping,field robots,legged robots},
number = {4},
pages = {3019--3026},
title = {{Probabilistic terrain mapping for mobile robots with uncertain localization}},
volume = {3},
year = {2018}
}
@article{Villarreal2018,
abstract = {—Legged robots can outperform wheeled machines for most navigation tasks across unknown and rough terrains. For such tasks, visual feedback is a fundamental asset to provide robots with terrain-awareness. However, robust dynamic locomotion on difficult terrains with real-time performance guarantees remains a challenge. We present here a real-time, dynamic foothold adaptation strategy based on visual feedback. Our method adjusts the landing position of the feet in a fully reactive manner, using only on-board computers and sensors. The correction is computed and executed continuously along the swing phase trajectory of each leg. To efficiently adapt the landing position, we implement a self-supervised foothold classifier based on a Convolutional Neural Network (CNN). Our method results in an up to 200 times faster computation with respect to the full-blown heuristics. Our goal is to react to visual stimuli from the environment, bridging the gap between blind reactive locomotion and purely vision-based planning strategies. We assess the performance of our method on the dynamic quadruped robot HyQ, executing static and dynamic gaits (at speeds up to 0.5 m/s) in both simulated and real scenarios; the benefit of safe foothold adaptation is clearly demonstrated by the overall robot behavior.},
author = {Villarreal, Octavio and Barasuol, Victor and Camurri, Marco and Franceschi, Luca and Focchi, Michele and Pontil, Massimiliano and Caldwell, Darwin G. and Semini, Claudio},
file = {:home/matias/Documents/Mendeley Desktop/Villarreal et al/2018/Villarreal et al. - 2018 - Fast and continuous foothold adaptation for dynamic locomotion through CNNs.pdf:pdf},
journal = {arXiv},
keywords = {Automation,Deep Learning in Robotics,Index Terms—Legged Robots,Reactive and Sensor-Based Planning},
number = {2},
pages = {2140--2147},
title = {{Fast and continuous foothold adaptation for dynamic locomotion through CNNs}},
volume = {4},
year = {2018}
}
@article{Gangapurwala2020,
abstract = {We present a unified model-based and data-driven approach for quadrupedal planning and control to achieve dynamic locomotion over uneven terrain. We utilize on-board proprioceptive and exteroceptive feedback to map sensory information and desired base velocity commands into footstep plans using a reinforcement learning (RL) policy trained in simulation over a wide range of procedurally generated terrains. When ran online, the system tracks the generated footstep plans using a model-based controller. We evaluate the robustness of our method over a wide variety of complex terrains. It exhibits behaviors which prioritize stability over aggressive locomotion. Additionally, we introduce two ancillary RL policies for corrective whole-body motion tracking and recovery control. These policies account for changes in physical parameters and external perturbations. We train and evaluate our framework on a complex quadrupedal system, ANYmal version B, and demonstrate transferability to a larger and heavier robot, ANYmal C, without requiring retraining.},
archivePrefix = {arXiv},
arxivId = {2012.03094},
author = {Gangapurwala, Siddhant and Geisert, Mathieu and Orsolino, Romeo and Fallon, Maurice and Havoutis, Ioannis},
eprint = {2012.03094},
file = {:home/matias/Documents/Mendeley Desktop/Gangapurwala et al/2020/Gangapurwala et al. - 2020 - RLOC Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control.pdf:pdf},
journal = {arXiv},
title = {{RLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and Optimal Control}},
url = {http://arxiv.org/abs/2012.03094},
year = {2020}
}
@article{Aoustin2008,
abstract = {Experimental validation of absolute orientation estimation solutions is displayed for the dynamical stable five-link biped robot Rabbit during a walking gait. The objective is to prove the technical feasibility of posture online software estimation in order to remove sensors. Finally, this paper presents the first experimental results of walking biped robot posture estimation. {\textcopyright}2008 IEEE.},
author = {Aoustin, Y. and Plestan, F. and Lebastard, V.},
doi = {10.1109/ROBOT.2008.4543378},
isbn = {9781424416479},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1270--1275},
title = {{Experimental comparison of several posture estimation solutions for biped robot Rabbit}},
year = {2008}
}
@phdthesis{Carranza2012,
abstract = {Important progress has been achieved in recent years with regards to the monocular SLAM problem, which consists of estimating the 6-D pose of a single camera, whilst building a 3-D map representation of scene structure observed by the camera. Nowadays, there exist various monocular SLAM systems capable of outputting camera and map estimates at camera frame rates over long trajectories and for indoor and outdoor scenarios. These systems are attractive due to their low cost - a consequence of using a conventional camera - and have been widely utilised in different applications such as in augmented and virtual reality.\n\n\nHowever, the main utility of the built map has been reduced to work as an effective reference system for robust and fast camera localisation. In order to produce more useful maps, different works have proposed the use of higher-level structures such as lines, planes and even meshes. Planar structure is one of the most popular structures to be incorporated into the map, given that they are abundant in man-made scenes, and because a plane by itself provides implicit semantic cues about the scene structure. Nevertheless, very often planar structure detection is carried out by ad-hoc auxiliary methods delivering a delayed detection and therefore a delayed mapping which becomes a problem when rapid planar mapping is demanded.\n\n\nMy thesis work addresses the problem of planar structure detection and mapping by proposing a novel mapping mechanism called structure-driven mapping. This new approach aims at enabling a monocular SLAM system to perform planar or point mapping according to scene structure observed by the camera. In order to achieve this, we propose to incorporate the plane detection task into the SLAM process. For this purpose, we have developed a novel framework that unifies planar and point mapping under a common parameterisation. This enables map components to evolve according to the incremental visual observations of the scene structure thus providing undelayed planar mapping. Moreover, the plane detection task stops as soon as the camera explores a non planar structure scenario, which avoids wasting unnecessary processing time, starting again as soon as planar structure gets into view.\n\n\nIn my thesis I present a thorough evaluation of this novel approach through simulation experiments and results obtained with real data. I also present a visual odometry application which takes advantage of the efficient way in which the scene structure is mapped by using the novel mapping mechanism presented in this work. Therefore, the results suggest the feasibility of performing simultaneous planar structure detection, localisation and mapping within the same coherent estimation framework.},
author = {Mart{\'{i}}nez-Carranza, Jos{\'{e}}},
number = {May},
school = {University of Bristol},
title = {{Efficient Monocular SLAM by Using a Structure-Driven Mapping}},
url = {http://www.cs.bris.ac.uk/Publications/Papers/2001577.pdf%0Ahttp://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.574263},
year = {2012}
}
@article{Johnson2017,
abstract = {This article presents a retrospective analysis of Team IHMC's experience throughout the DARPA Robotics Challenge (DRC), where we took first or second place overall in each of the three phases. As an extremely demanding challenge typical of DARPA, the DRC required rapid research and development to push the boundaries of robotics and set a new benchmark for complex robotic behavior. We present how we addressed each of the eight tasks of the DRC and review our performance in the Finals. While the ambitious competition schedule limited extensive experimentation, we will review the data we collected during the approximately three years of our participation. We discuss some of the significant lessons learned that contributed to our success in the DRC. These include hardware lessons, software lessons, and human-robot integration lessons. We describe refinements to the coactive design methodology that helped our designers connect human–machine interaction theory to both implementation and empirical data. This approach helped our team focus our limited resources on the issues most critical to success. In addition to helping readers understand our experiences in developing on a Boston Dynamics Atlas robot for the DRC, we hope this article will provide insights that apply more widely to robotics development and design of human–machine systems.},
author = {Johnson, Matthew and Shrewsbury, Brandon and Bertrand, Sylvain and Calvert, Duncan and Wu, Tingfan and Duran, Daniel and Stephen, Douglas and Mertins, Nathan and Carff, John and Rifenburgh, William and Smith, Jesper and Schmidt-Wetekam, Chris and Faconti, Davide and Graber-Tilton, Alex and Eyssette, Nicolas and Meier, Tobias and Kalkov, Igor and Craig, Travis and Payton, Nick and McCrory, Stephen and Wiedebach, Georg and Layton, Brooke and Neuhaus, Peter and Pratt, Jerry},
doi = {10.1002/rob.21674},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {241--261},
title = {{Team IHMC's Lessons Learned from the DARPA Robotics Challenge: Finding Data in the Rubble}},
url = {http://doi.wiley.com/10.1002/rob.21674},
volume = {34},
year = {2017}
}
@inproceedings{Wang2018,
abstract = {Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1803.04228},
author = {Wang, Tsun Hsuan and Huang, Hung Jui and Lin, Juan Ting and Hu, Chan Wei and Zeng, Kuo Hao and Sun, M.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8463173},
eprint = {1803.04228},
file = {:home/matias/Documents/Mendeley Desktop/Wang et al/2018/Wang et al. - 2018 - Omnidirectional CNN for Visual Place Recognition and Navigation.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
pages = {2341--2348},
title = {{Omnidirectional CNN for Visual Place Recognition and Navigation}},
year = {2018}
}
@article{Krivonosov2016,
abstract = {Two the most common tasks for autonomous mobile robots is to explore the environment and locate a target. %In the last case, the objective is either to find a target in the shortest time possible or, alternatively, to find %as many targets as possible for a given amount of time. Targets could range from sources of chemical contamination to people needing assistance in a disaster area. From the very beginning, the quest for most efficient search algorithms was strongly influenced by behavioral science and ecology, where researchers try to figure out the strategies used by leaving beings, from bacteria to mammals. Since then, bio-inspired random search algorithms remain one the most important directions in autonomous robotics. Recently a new wave arrived bringing a specific type of random walks as a universal search strategy exploited by immune cells, insects, mussels, albatrosses, sharks, deers, and a dozen of other animals including humans. These \textit{L\'{e}vy} walks combine two key features, the ability of walkers to spread anomalously fast while moving with a finite velocity. The latter is especially valuable in the context of robotics because it respects the reality autonomous robots live in. There is already an impressive body of publications on L\'{e}vy robotics; yet research in this field is unfolding further, constantly bringing new results, ideas, hypothesis and speculations. In this mini-review we survey the current state of the field, list latest advances, discuss the prevailing trends, and outline further perspectives.},
archivePrefix = {arXiv},
arxivId = {1612.03997},
author = {Krivonosov, M. and Denisov, S. and Zaburdaev, V.},
eprint = {1612.03997},
pages = {1--6},
title = {{L\'{e}vy robotics}},
url = {http://arxiv.org/abs/1612.03997},
year = {2016}
}
@article{Rosen2013,
abstract = {Many online inference problems in robotics and AI are characterized by probability distributions whose factor graph representations are sparse. While there do exist some computationally efficient algorithms (e.g. incremental smoothing and mapping (iSAM) or Robust Incremental least-Squares Estimation (RISE)) for performing online incremental maximum likelihood estimation over these models, they generally require that the distribution of interest factors as a product of Gaussians, a rather restrictive assumption. In this paper, we investigate the possibility of performing efficient incremental online estimation over sparse factor graphs in the non-Gaussian case. Our main result is a method that generalizes iSAM and RISE by removing the assumption of Gaussian factors, thereby significantly expanding the class of distributions to which these algorithms can be applied. The generalization is achieved by means of a simple algebraic reduction that under relatively mild conditions (boundedness of each of the factors in the distribution of interest) enables an instance of the general maximum likelihood estimation problem to be reduced to an equivalent instance of least-squares minimization that can be solved efficiently online by application of iSAM or RISE. Through this construction we obtain robust, computationally efficient, and mathematically correct incremental online maximum likelihood estimators for non-Gaussian distributions over sparse factor graphs. {\textcopyright} 2013 IEEE.},
author = {Rosen, David M. and Kaess, Michael and Leonard, John J.},
doi = {10.1109/ICRA.2013.6630699},
file = {:home/matias/Documents/Mendeley Desktop/Rosen, Kaess, Leonard/2013/Rosen, Kaess, Leonard - 2013 - Robust incremental online inference over sparse factor graphs Beyond the Gaussian case.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1025--1032},
title = {{Robust incremental online inference over sparse factor graphs: Beyond the Gaussian case}},
year = {2013}
}
@article{Censi2014,
abstract = {The agility of a robotic system is ultimately limited by the speed of its processing pipeline. The use of a Dynamic Vision Sensors (DVS), a sensor producing asynchronous events as luminance changes are perceived by its pixels, makes it possible to have a sensing pipeline of a theoretical latency of a few microseconds. However, several challenges must be overcome: A DVS does not provide the grayscale value but only changes in the luminance; and because the output is composed by a sequence of events, traditional frame-based visual odometry methods are not applicable. This paper presents the first visual odometry system based on a DVS plus a normal CMOS camera to provide the absolute brightness values. The two sources of data are automatically spatiotemporally calibrated from logs taken during normal operation. We design a visual odometry method that uses the DVS events to estimate the relative displacement since the previous CMOS frame by processing each event individually. Experiments show that the rotation can be estimated with surprising accuracy, while the translation can be estimated only very noisily, because it produces few events due to very small apparent motion.},
author = {Censi, Andrea and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6906931},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {703--710},
title = {{Low-latency event-based visual odometry}},
year = {2014}
}
@article{Hawke2016,
abstract = {This paper is about building robots that get better through use in their particular environment, improving their perceptual abilities. We approach this from a life long learning perspective: we want the robot's ability to detect objects in its specific operating environment to evolve and improve over time. Our idea, which we call Experience-Based Classification (EBC), builds on the well established practice of performing hard negative mining to train object detectors. Rather than cease mining for data once a detector is trained, EBC continuously seeks to learn from mistakes made while processing data observed during the robot's operation. This process is entirely self-supervised, facilitated by spatial heuristics and the fact that we have additional scene data at our disposal in mobile robotics. In the context of autonomous driving we demonstrate considerable object detector improvement over time using 40Km of data gathered from different driving routes at different times of year.},
author = {Hawke, Jeffrey and Gurău, Corina and Tong, Chi Hay and Posner, Ingmar},
doi = {10.1007/978-3-319-27702-8_12},
file = {:home/matias/Documents/Mendeley Desktop/Hawke et al/2016/Hawke et al. - 2016 - Wrong today, right tomorrow Experience-based classification for robot perception.pdf:pdf},
isbn = {9783319277004},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
pages = {173--186},
title = {{Wrong today, right tomorrow: Experience-based classification for robot perception}},
volume = {113},
year = {2016}
}
@article{Indelman2015,
abstract = {Bundle adjustment (BA) is essential in many robotics and structure-from-motion applications. In robotics, often a bundle adjustment solution is desired to be available incrementally as new poses and 3D points are observed. Similarly in batch structure from motion, cameras are typically added incrementally to allow good initializations. Current incremental BA methods quickly become computationally expensive as more camera poses and 3D points are added into the optimization. In this paper we introduce incremental light bundle adjustment (iLBA), an efficient optimization framework that substantially reduces computational complexity compared to incremental bundle adjustment. First, the number of variables in the optimization is reduced by algebraic elimination of observed 3D points, leading to a structureless BA. The resulting cost function is formulated in terms of three-view constraints instead of re-projection errors and only the camera poses are optimized. Second, the optimization problem is represented using graphical models and incremental inference is applied, updating the solution using adaptive partial calculations each time a new camera is incorporated into the optimization. Typically, only a small fraction of the camera poses are recalculated in each optimization step. The 3D points, although not explicitly optimized, can be reconstructed based on the optimized camera poses at any time. We study probabilistic and computational aspects of iLBA and compare its accuracy against incremental BA and another recent structureless method using real-imagery and synthetic datasets. Results indicate iLBA is 2-10 times faster than incremental BA, depending on number of image observations per frame.},
author = {Indelman, Vadim and Roberts, Richard and Dellaert, Frank},
doi = {10.1016/j.robot.2015.03.009},
isbn = {9781467363587},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Bundle adjustment,SLAM,Vision-aided navigation},
pages = {63--82},
publisher = {Elsevier B.V.},
title = {{Incremental light bundle adjustment for structure from motion and robotics}},
url = {http://dx.doi.org/10.1016/j.robot.2015.03.009},
volume = {70},
year = {2015}
}
@article{Lidar,
author = {Lidar, Velodyne},
file = {:home/matias/Documents/Mendeley Desktop/Lidar/Unknown/Lidar - Unknown - Journal of Intelligent & Robotic Systems Extrinsic calibration of Velodyne LiDAR and monocular camera by 3D-3D point c.pdf:pdf},
title = {{Journal of Intelligent & Robotic Systems Extrinsic calibration of Velodyne LiDAR and monocular camera by 3D-3D point correspondence}}
}
@article{Meng2019a,
abstract = {End-to-end learning for autonomous navigation has received substantial attention recently as a promising method for reducing modeling error. However, its data complexity, especially around generalization to unseen environments, is high. We introduce a novel image-based autonomous navigation technique that leverages in policy structure using the Riemannian Motion Policy (RMP) framework for deep learning of vehicular control. We design a deep neural network to predict control point RMPs of the vehicle from visual images, from which the optimal control commands can be computed analytically. We show that our network trained in the Gibson environment can be used for indoor obstacle avoidance and navigation on a real RC car, and our RMP representation generalizes better to unseen environments than predicting local geometry or predicting control commands directly.},
archivePrefix = {arXiv},
arxivId = {1904.01762},
author = {Meng, Xiangyun and Ratliff, Nathan and Xiang, Yu and Fox, Dieter},
doi = {10.1109/ICRA.2019.8794223},
eprint = {1904.01762},
file = {:home/matias/Documents/Mendeley Desktop/Meng et al/2019/Meng et al. - 2019 - Neural autonomous navigation with riemannian motion policy.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {8860--8866},
title = {{Neural autonomous navigation with riemannian motion policy}},
volume = {2019-May},
year = {2019}
}
@article{Kaess2012,
abstract = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of the matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, named iSAM2, which achieves improvements in efficiency through incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. We analyze various properties of iSAM2 in detail, and show on a range of real and simulated datasets that our algorithm compares favorably with other recent mapping algorithms in both quality and efficiency. {\textcopyright} SAGE Publications 2011.},
author = {Kaess, Michael and Johannsson, Hordur and Roberts, Richard and Ila, Viorela and Leonard, John J. and Dellaert, Frank},
doi = {10.1177/0278364911430419},
file = {:home/matias/Documents/Mendeley Desktop/Kaess et al/2012/Kaess et al. - 2012 - ISAM2 Incremental smoothing and mapping using the Bayes tree(2).pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {SLAM,clique tree,graphical models,junction tree,nonlinear optimization,probabilistic inference,smoothing and mapping,sparse linear algebra},
number = {2},
pages = {216--235},
title = {{ISAM2: Incremental smoothing and mapping using the Bayes tree}},
volume = {31},
year = {2012}
}
@book{Aggarwal2018,
abstract = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.},
author = {Aggarwal, Charu C.},
booktitle = {Neural Networks and Deep Learning},
doi = {10.1007/978-3-319-94463-0},
file = {:home/matias/Documents/Mendeley Desktop/Aggarwal/2018/Aggarwal - 2018 - Neural Networks and Deep Learning.pdf:pdf},
isbn = {9783319944623},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@incollection{Fojtu2012a,
abstract = {Nao humanoid robot from Aldebaran Robotics is equipped with an odometry sensor providing rather inaccurate robot pose estimates. We propose using Structure from Motion (SfM) to enable visual odometry from Nao camera without the necessity to add artificial markers to the scene and show that the robot pose estimates can be significantly improved by fusing the data from the odometry sensor and visual odometry. The implementation consists of the sensor modules streaming robot data, the mapping module creating a 3D model, the visual localization module estimating camera pose w.r.t. the model, and the navigation module planning robot trajectories and performing the actual movement. All of the modules are connected through the RSB middleware, which makes the solution independent on the given robot type. {\textcopyright} Springer-Verlag Berlin Heidelberg 2012.},
author = {Fojtů, {\v{S}}imon and Havlena, Michal and Pajdla, Tom{\'{a}}{\v{s}}},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33515-0_43},
isbn = {9783642335143},
issn = {03029743},
keywords = {Nao humanoid robot,Robot localization,Robot navigation,Structure from motion},
number = {PART 2},
pages = {427--438},
title = {{Nao robot localization and navigation using fusion of odometry and visual sensor data}},
url = {http://link.springer.com/10.1007/978-3-642-33515-0_43},
volume = {7507 LNAI},
year = {2012}
}
@article{Pretto2008,
abstract = {This paper describes a visual feature detector and descriptor scheme designed to address the specific problems of humanoid robots in the tasks of visual odometry, localization, and SLAM (Simultaneous Localization And Mapping). During walking, turning, and squatting movements, the camera of a humanoid robot moves in jerky and sometimes unpredictable way. This causes an undesired motion blur in the images grabbed by the robot camera, that negatively affects the performance of the image processing algorithms. Indeed, the classical features detector and descriptor filtering techniques, that proved to work so well for wheeled robots, do not perform so reliably in humanoid robots. This paper presents a method to detect image interest points (invariant to scale transformation and rotations) robust to motion-blur introduced by the camera motion. Our approach is based on a preprocessing step to estimate the Point Spread Function (PSF) of the motion blur. The PSF is used to deconvolve the image reducing the blur. Then, we apply a feature detector inspired by SURF approach and the feature descriptor from SIFT. Experiments performed on standard datasets corrupted with motion blur and on images taken by a camera mounted on a small humanoid robot show the effectiveness of the proposed technique. Our approach presents higher performances and higher reliability in matching features in the different images of a sequence affected by motion-blur. {\textcopyright} 2008 IEEE.},
author = {Pretto, Alberto and Menegatti, Emanuele and Pagello, Enrico},
doi = {10.1109/ICHR.2007.4813922},
file = {::},
isbn = {9781424418626},
journal = {Proceedings of the 2007 7th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS 2007},
keywords = {[Electronic Manuscript]},
pages = {532--538},
title = {{Reliable features matching for humanoid robots}},
year = {2007}
}
@article{Campusano2015,
abstract = {In a robotics context, visualizing the data scanned by a robot is crucial to understand what the robot's sensors perceive about its environment. Consequently, robotic visualizations show these values in a 3-D world, such that they can be compared with the real world. However these visualizations do not allow developers to see this data in a manner that allows it to be interpreted for program construction. As a result, these visualizations are in many cases ineffective for programming robot behaviors. To address this issue, we have built several visualizations of robot sensor data for the programming of behaviors, and we report on them here. Our visualizations focus on better revealing the hard data, which allows developers to faster understand it and consequently to faster create and adapt robot behaviors.},
author = {Campusano, Miguel and Fabry, Johan},
doi = {10.1109/VISSOFT.2015.7332424},
isbn = {9781467375269},
journal = {2015 IEEE 3rd Working Conference on Software Visualization, VISSOFT 2015 - Proceedings},
keywords = {Arrays,Context,Data visualization,Programming,Tactile sensors},
number = {Dcc},
pages = {135--139},
title = {{From robots to humans: Visualizations for robot sensor data}},
year = {2015}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti {\textcopyright} 2012 IEEE.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@article{Ates2014,
abstract = {For interactive humanoids, rehabilitation robots, and orthotic and prosthetic devices, the human-robot interaction is an essential but challenging element. Compliant Series- Elastic Actuators (SEAs) are ideal to power such devices due to their low impedance and smoothness of generated forces. In this paper we present the ServoSEA, which is a miniature Series-Elastic Actuator (SEA) based on cheap RC servos, and which is useful for actuation of orthotic, prosthetic or robotic hands. RC servos are complete packages that come with rotary motor and sensor and have an integrated control board to control the output angle. In the ServoSEA, a small rotational spring is attached to the output shaft and the internal rotary sensor is relocated to measure the spring deflection. These small modifications immediately make the integrated control board behave as a series-elastic torque controller. Here we present several design alternatives and report on the performance of our implementation that will be used in the active SCRIPT wrist and hand orthosis. The performance measurements showed that feedforward control of the example implementation of the ServoSEA results in acceptable, though not perfect, force tracking behavior. It is clear that although the ServoSEA concept is universal, final performance strongly depends on the quality of the original RC servo.},
author = {Ates, Serdar and Sluiter, Victor I. and Lammertse, Piet and Stienen, Arno H.A.},
doi = {10.1109/biorob.2014.6913868},
file = {::},
isbn = {9781479931262},
issn = {21551774},
journal = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
number = {1},
pages = {752--757},
title = {{ServoSEA concept: Cheap, miniature series-elastic actuators for orthotic, prosthetic and robotic hands}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6913868},
year = {2014}
}
@inproceedings{Schops2014,
abstract = {We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.},
author = {Schops, Thomas and Enge, Jakob and Cremers, Daniel},
booktitle = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
doi = {10.1109/ISMAR.2014.6948420},
isbn = {9781479961849},
keywords = {3D Reconstruction,AR,Direct Visual Odometry,Mapping,Mobile Devices,NEON,Semi-Dense,Tracking},
month = {sep},
pages = {145--150},
publisher = {IEEE},
title = {{Semi-dense visual odometry for AR on a smartphone}},
url = {http://ieeexplore.ieee.org/document/6948420/},
year = {2014}
}
@book{Sutton2017,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
author = {Sutton, Richard S. and Barto, Andrew G.},
file = {:home/matias/Documents/Mendeley Desktop/Sutton, Barto/2017/Sutton, Barto - 2017 - Reinforcement Learning An Introduction.pdf:pdf},
isbn = {9780262039246},
issn = {13646613},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introduction}},
year = {2017}
}
@article{Soudry2014,
abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a "mean-field" factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.},
author = {Soudry, Daniel and Hubara, Itay and Meir, Ron},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {963--971},
title = {{Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights}},
volume = {2},
year = {2014}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
doi = {10.1007/9783642532580?COVERIMAGEURL=HTTPS://STATICCONTENT.SPRINGER.COM/COVER/BOOK/9783642532580.JPG},
eprint = {1012.2599},
file = {::},
isbn = {0671631985},
issn = {09420940},
pmid = {27489955},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Zhang2019a,
abstract = {For mobile robots to localize robustly, actively considering the perception requirement at the planning stage is essential. In this paper, we propose a novel representation for active visual localization. By formulating the Fisher information and sensor visibility carefully, we are able to summarize the localization information into a discrete grid, namely the Fisher information field. The information for arbitrary poses can then be computed from the field in constant time, without the need of costly iterating all the 3D landmarks. Experimental results on simulated and real-world data show the great potential of our method in efficient active localization and perception-aware planning. To benefit related research, we release our implementation of the information field to the public.},
author = {Zhang, Zichao and Scaramuzza, Davide},
doi = {10.1109/ICRA.2019.8793680},
file = {:home/matias/Documents/Mendeley Desktop/Zhang, Scaramuzza/2019/Zhang, Scaramuzza - 2019 - Beyond point clouds Fisher information field for active visual localization.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5986--5992},
title = {{Beyond point clouds: Fisher information field for active visual localization}},
volume = {2019-May},
year = {2019}
}
@article{Vega-Brown2013,
abstract = {We present CELLO (Covariance Estimation and Learning through Likelihood Optimization), an algorithm for predicting the covariances of measurements based on any available informative features. This algorithm is intended to improve the accuracy and reliability of on-line state estimation by providing a principled way to extend the conventional fixed-covariance Gaussian measurement model. We show that in experiments, CELLO learns to predict measurement covariances that agree with empirical covariances obtained by manually annotating sensor regimes. We also show that using the learned covariances during filtering provides substantial quantitative improvement to the overall state estimate. {\textcopyright} 2013 IEEE.},
author = {Vega-Brown, William and Bachrach, Abraham and Bry, Adam and Kelly, Jonathan and Roy, Nicholas},
doi = {10.1109/ICRA.2013.6631017},
file = {:home/matias/Documents/Mendeley Desktop/Vega-Brown et al/2013/Vega-Brown et al. - 2013 - CELLO A fast algorithm for Covariance Estimation.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {Icra},
pages = {3160--3167},
title = {{CELLO: A fast algorithm for Covariance Estimation}},
year = {2013}
}
@book{Farrell2008,
abstract = {We study a family of 'classical' orthogonal polynomials which satisfy (apart from a three-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl type. These polynomials can be obtained from the little q-Jacobi polynomials in the limit q = -1. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for q = -1. {\textcopyright} 2011 IOP Publishing Ltd.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Vinet, Luc and Zhedanov, Alexei},
booktitle = {Journal of Physics A: Mathematical and Theoretical},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {1011.1669},
file = {::},
isbn = {0071642668},
issn = {17518113},
keywords = {icle},
number = {8},
pages = {530},
title = {{A 'missing' family of classical orthogonal polynomials}},
volume = {44},
year = {2011}
}
@article{Rebecq2017,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. We propose a novel, accurate tightly-coupled visual-inertial odometry pipeline for such cameras that leverages their outstanding properties to estimate the camera ego-motion in challenging conditions, such as high-speed motion or high dynamic range scenes. The method tracks a set of features (extracted on the image plane) through time. To achieve that, we consider events in overlapping spatio-temporal windows and align them using the current camera motion and scene structure, yielding motion-compensated event frames. We then combine these feature tracks in a keyframe-based, visual-inertial odometry algorithm based on nonlinear optimization to estimate the camera's 6-DOF pose, velocity, and IMU biases. The proposed method is evaluated quantitatively on the public Event Camera Dataset [19] and significantly outperforms the state-of-the-art [28], while being computationally much more efficient: our pipeline can run much faster than real-time on a laptop and even on a smartphone processor. Furthermore, we demonstrate qualitatively the accuracy and robustness of our pipeline on a large-scale dataset, and an extremely high-speed dataset recorded by spinning an event camera on a leash at 850 deg/s.},
author = {Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
doi = {10.5244/c.31.16},
isbn = {190172560X},
journal = {British Machine Vision Conference 2017, BMVC 2017},
number = {September},
pages = {1--8},
title = {{Real-time visual-inertial odometry for event cameras using keyframe-based nonlinear optimization}},
year = {2017}
}
@article{Underhauf2018,
abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
archivePrefix = {arXiv},
arxivId = {1711.02223},
author = {S{\"{u}}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"{u}}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
doi = {10.1177/0278364918770733},
eprint = {1711.02223},
file = {:home/matias/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/2018/S{\"{u}}nderhauf et al. - 2018 - The limits and potentials of deep learning for robotics.pdf:pdf},
isbn = {0037549716666},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
month = {apr},
number = {4-5},
pages = {405--420},
title = {{The limits and potentials of deep learning for robotics}},
url = {http://mc.manuscriptcentral.com/ijrr%0Awww.sagepub.com/ http://journals.sagepub.com/doi/10.1177/0278364918770733},
volume = {37},
year = {2018}
}
@article{Gurau2018,
abstract = {Despite significant advances in machine learning and perception over the past few decades, perception algorithms can still be unreliable when deployed in challenging time-varying environments. When these systems are used for autonomous decision-making, such as in self-driving vehicles, the impact of their mistakes can be catastrophic. As such, it is important to characterize the performance of the system and predict when and where it may fail in order to take appropriate action. While similar in spirit to the idea of introspection, this work introduces a new paradigm for predicting the likely performance of a robot's perception system based on past experience in the same workspace. In particular, we propose two models that probabilistically predict perception performance from observations gathered over time. While both approaches are place-specific, the second approach additionally considers appearance similarity when incorporating past observations. We evaluate our method in a classical decision-making scenario in which the robot must choose when and where to drive autonomously in 60km of driving data from an urban environment. Results demonstrate that both approaches lead to fewer false decisions (in terms of incorrectly offering or denying autonomy) for two different detector models, and show that leveraging visual appearance within a state-of-the-art navigation framework increases the accuracy of our performance predictions.},
author = {Gurău, Corina and Rao, Dushyant and Tong, Chi Hay and Posner, Ingmar},
doi = {10.1177/0278364917730603},
file = {:home/matias/Documents/Mendeley Desktop/Gurău et al/2018/Gurău et al. - 2018 - Learn from experience Probabilistic prediction of perception performance to avoid failure.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Robotics,autonomous driving,introspection,object detection,performance estimation},
number = {9},
pages = {981--995},
title = {{Learn from experience: Probabilistic prediction of perception performance to avoid failure}},
volume = {37},
year = {2018}
}
@article{Tourani2020,
author = {Tourani, Satyajit and Desai, Dhagash and Parihar, Udit Singh and Garg, Sourav},
file = {:home/matias/Documents/Mendeley Desktop/Tourani et al/2020/Tourani et al. - 2020 - Early Bird Loop Closures from Opposing Viewpoints for Perceptually-Aliased Indoor Environments.pdf:pdf},
title = {{Early Bird : Loop Closures from Opposing Viewpoints for Perceptually-Aliased Indoor Environments}},
year = {2020}
}
@inproceedings{Loianno2016,
abstract = {The combination of on-board sensors measurements with different statistical characteristics can be employed in robotics for localization and control, especially in GPS-denied environments. In particular, most aerial vehicles are packaged with low cost sensors, important for aerial robotics, such as camera, a gyroscope, and an accelerometer. In this work, we develop a visual inertial odometry system based on the Unscented Kalman Filter (UKF) acting on the Lie group SE(3), such to obtain an unique, singularity-free representation of a rigid body pose. We model this pose with the Lie group SE(3) and model the noise on the corresponding Lie algebra. Moreover, we extend the concepts used in the standard UKF formulation, such as state uncertainty and modeling, to correctly incorporate elements that do not belong to an Euclidean space such as the Lie group members. In this analysis, we use the parallel transport, which requires us to explicitly consider SE(3) as representing rigid bodies though the use of the affine connection. We present experimental results to show the effectiveness of the proposed approach for state estimation of a quadrotor platform.},
author = {Loianno, Giuseppe and Watterson, Michael and Kumar, Vijay},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487292},
isbn = {9781467380263},
issn = {10504729},
month = {may},
number = {3},
pages = {1544--1551},
publisher = {IEEE},
title = {{Visual inertial odometry for quadrotors on SE(3)}},
url = {http://ieeexplore.ieee.org/document/7487292/},
volume = {2016-June},
year = {2016}
}
@article{Wisth2019,
abstract = {Legged robots, specifically quadrupeds, are becoming increasingly attractive for industrial applications such as inspection. However, to leave the laboratory and to become useful to an end user requires reliability in harsh conditions. From the perspective of state estimation, it is essential to be able to accurately estimate the robot's state despite challenges such as uneven or slippery terrain, textureless and reflective scenes, as well as dynamic camera occlusions. We are motivated to reduce the dependency on foot contact classifications, which fail when slipping, and to reduce position drift during dynamic motions such as trotting. To this end, we present a factor graph optimization method for state estimation which tightly fuses and smooths inertial navigation, leg odometry and visual odometry. The effectiveness of the approach is demonstrated using the ANYmal quadruped robot navigating in a realistic outdoor industrial environment. This experiment included trotting, walking, crossing obstacles and ascending a staircase. The proposed approach decreased the relative position error by up to 55% and absolute position error by 76% compared to kinematic-inertial odometry.},
archivePrefix = {arXiv},
arxivId = {1904.03048},
author = {Wisth, David and Camurri, Marco and Fallon, Maurice},
doi = {10.1109/LRA.2019.2933768},
eprint = {1904.03048},
file = {:home/matias/Documents/Mendeley Desktop/Wisth, Camurri, Fallon/2019/Wisth, Camurri, Fallon - 2019 - Robust Legged Robot State Estimation Using Factor Graph Optimization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Legged Robots,Localization,Sensor Fusion},
number = {4},
pages = {4507--4514},
title = {{Robust Legged Robot State Estimation Using Factor Graph Optimization}},
volume = {4},
year = {2019}
}
@inproceedings{Zhang2018,
abstract = {Vision-based navigation is extremely susceptible to natural scene changes. This can result in localization failures in less than a few hours after map creation. To combat short-term illumination changes as well as long-term seasonal variations, we propose using a place-and-time-dependent binary descriptor that adapts to different scenarios in an online fashion. This is achieved by extending the GRIEF [6] evolution algorithm in two ways: correspondence generation using a known pose change and the inclusion of LATCH triplets in addition to BRIEF comparisons for descriptor generation. We show the adaptive descriptor outperforms a single descriptor scheme for localization within a single-experience Visual Teach and Repeat (VTR) system while maintaining the efficiency of binary descriptors. By adapting the description function to different environmental conditions, it allows the system to operate for a longer period before a new experience is required. In the presence of extreme illumination changes from day to night, we obtain 40% more inlier matches compared to SURF. In the case of seasonal variations, a 70% increase is demonstrated. The increased correspondences result in more localizable sections along the paths, amounting to a 25% and 150% increase in the lighting and seasonal cases, respectively.},
author = {Zhang, Nan and Warren, Michael and Barfoot, Timothy D.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8460674},
file = {:home/matias/Documents/Mendeley Desktop/Zhang, Warren, Barfoot/2018/Zhang, Warren, Barfoot - 2018 - Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual Localization.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
pages = {828--835},
publisher = {IEEE},
title = {{Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual Localization}},
year = {2018}
}
@article{Paton2015a,
abstract = {Stereo Visual Teach & Repeat (VT&R) is a system for long-range, autonomous route following in unstructured 3D environments. As this system relies on a passive sensor to localize, it is highly susceptible to changes in lighting conditions. Recent work in the optics community has provided a method to transform images collected from a three-channel passive sensor into color-constant images that are resistant to changes in outdoor lighting conditions. This paper presents a lighting-resistant VT&R system that uses experimentally trained color-constant images to autonomously navigate difficult outdoor terrain despite changes in lighting. We show through an extensive field trial that our algorithm is capable of autonomously following a 1km outdoor route spanning sandy/rocky terrain, grassland, and wooded areas. Using a single visual map created at midday, the route was autonomously repeated 26 times over a period of four days, from sunrise to sunset with an autonomy rate (by distance) of over 99.9%. These experiments show that a simple image transformation can extend the operation of VT&R from a few hours to multiple days.},
author = {Paton, Michael and MacTavish, Kirk and Ostafew, Chris J. and Barfoot, Timothy D.},
doi = {10.1109/ICRA.2015.7139391},
file = {:home/matias/Documents/Mendeley Desktop/Paton et al/2015/Paton et al. - 2015 - It ' s Not Easy Seeing Green Lighting-Resistant Stereo Visual Teach & Repeat Using Color-Constant Images.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {1519--1526},
publisher = {IEEE},
title = {{It's not easy seeing green: Lighting-resistant stereo Visual Teach & Repeat using color-constant images}},
volume = {2015-June},
year = {2015}
}
@article{Jones1998,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
doi = {10.1023/A:1008306431147},
eprint = {0005074v1},
file = {::},
isbn = {0925-5001},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
number = {4},
pages = {455--492},
pmid = {21858987},
primaryClass = {arXiv:astro-ph},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
volume = {13},
year = {1998}
}
@article{Lin2015,
abstract = {For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks.},
archivePrefix = {arXiv},
arxivId = {1510.03009},
author = {Lin, Zhouhan and Courbariaux, Matthieu and Memisevic, Roland and Bengio, Yoshua},
eprint = {1510.03009},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
pages = {7},
title = {{Neural networks with few multiplications}},
url = {http://arxiv.org/abs/1510.03009},
year = {2016}
}
@article{Yanco2015,
abstract = {In December 2013, the Defense Advanced Research Projects Agency (DARPA) Robotics Challenge (DRC) Trials were held in Homestead, Florida. The DRC Trials were designed to test the capabilities of humanoid robots in disaster response scenarios with degraded communications. Each team created their own interaction method to control their robot, either the Boston Dynamics Atlas robot or a robot built by the team itself. Of the 15 competing teams, eight participated in our study of human-robot interaction. We observed the participating teams from the field (with the robot) and in the control room (with the operators), noting many performance metrics, such as critical incidents and utterances, and categorizing their interaction methods according to the number of operators, control methods, and amount of interaction. We decomposed each task into a series of subtasks, different from the DRC Trials official subtasks for points, to gain a better understanding of each team's performance in varying complexities of mobility and manipulation. Each team's interaction methods have been compared to their performance, and correlations have been analyzed to understand why some teams ranked higher than others. We discuss lessons learned from this study, and we have found in general that the guidelines for human-robot interaction for unmanned ground vehicles still hold true: more sensor fusion, fewer operators, and more automation lead to better performance.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Yanco, Holly A. and Norton, Adam and Ober, Willard and Shane, David and Skinner, Anna and Vice, Jack},
doi = {10.1002/rob.21568},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {3},
pages = {420--444},
pmid = {22164016},
title = {{Analysis of human-robot interaction at the DARPA robotics challenge trials}},
volume = {32},
year = {2015}
}
@inproceedings{Quigley2009,
abstract = {In recent years research in the planning community has moved increasingly towards application of planners to realistic problems involving both time and many types of resources. For example, interest in planning demonstrated by the space research community has inspired work in observation scheduling, planetary rover exploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application. The International Planning Competitions have acted as an important motivating force behind the progress that has been made in planning since 1998. The third competition (held in 2002) set the planning community the challenge of handling time and numeric resources. This necessitated the development of a modelling language capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power -exceeding the capabilities of current planning technology -and presents a number of important challenges to the research community. {\textcopyright} 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1106.4561},
author = {Fox, Maria and Long, Derek},
booktitle = {Journal of Artificial Intelligence Research},
doi = {10.1613/jair.1129},
eprint = {1106.4561},
file = {::},
isbn = {0165-022X (Print)\r0165-022X (Linking)},
issn = {10769757},
pages = {61--124},
pmid = {8844323},
title = {{PDDL2.1: An extension to PDDL for expressing temporal planning domains}},
url = {http://pub1.willowgarage.com/$\sim$konolige/cs225B/docs/quigley-icra2009-ros.pdf},
volume = {20},
year = {2003}
}
@article{McManus2015,
abstract = {This paper presents an alternative approach to the problem of outdoor, persistent visual localisation against a known map. Instead of blindly applying a feature detector/descriptor combination over all images of all places, we leverage prior experiences of a place to learn place-dependent feature detectors (i.e., features that are unique to each place in our map and used for localisation). Furthermore, as these features do not represent low-level structure, like edges or corners, but are in fact mid-level patches representing distinctive visual elements (e.g., windows, buildings, or silhouettes), we are able to localise across extreme appearance changes. Note that there is no requirement that the features posses semantic meaning, only that they are optimal for the task of localisation. This work is an extension on previous work (McManus et al. in Proceedings of robotics science and systems, 2014b) in the following ways: (i) we have included a landmark refinement and outlier rejection step during the learning phase, (ii) we have implemented an asynchronous pipeline design, (iii) we have tested on data collected in an urban environment, and (iv) we have implemented a purely monocular system. Using over 100 km worth of data for training, we present localisation results from Begbroke Science Park and central Oxford.},
author = {McManus, Colin and Upcroft, Ben and Newman, Paul},
doi = {10.1007/s10514-015-9463-y},
file = {:home/matias/Documents/Mendeley Desktop/McManus, Upcroft, Newman/2015/McManus, Upcroft, Newman - 2015 - Learning place-dependant features for long-term vision-based localisation.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Appearance changes,Cross seasonal,Feature learning,Long-term autonomy,Outdoor localisation,Visual localisation},
number = {3},
pages = {363--387},
publisher = {Springer US},
title = {{Learning place-dependant features for long-term vision-based localisation}},
volume = {39},
year = {2015}
}
@article{Carlone2014a,
abstract = {This work investigates the pose graph optimization problem, which arises in maximum likelihood approaches to simultaneous localization and mapping (SLAM). State-of-the-art approaches have been demonstrated to be very efficient in medium- and large-sized scenarios; however, their convergence to the maximum likelihood estimate heavily relies on the quality of the initial guess. We show that, in planar scenarios, pose graph optimization has a very peculiar structure. The problem of estimating robot orientations from relative orientation measurements is a quadratic optimization problem (after computing suitable regularization terms); moreover, given robot orientations, the overall optimization problem becomes quadratic. We exploit these observations to design an approximation of the maximum likelihood estimate, which does not require the availability of an initial guess. The approximation, named LAGO (Linear Approximation for pose Graph Optimization), can be used as a stand-alone tool or can bootstrap state-of-the-art techniques, reducing the risk of being trapped in local minima. We provide analytical results on existence and sub-optimality of LAGO, and we discuss the factors influencing its quality. Experimental results demonstrate that LAGO is accurate in common SLAM problems. Moreover, it is remarkably faster than state-of-the-art techniques, and is able to solve very large-scale problems in a few seconds. {\textcopyright} The Author(s) 2014.},
author = {Carlone, Luca and Aragues, Rosario and Castellanos, Jos{\'{e}} A. and Bona, Basilio},
doi = {10.1177/0278364914523689},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Pose graph optimization,graph theory,linear estimation,mobile robots,simultaneous localization and mapping},
number = {7},
pages = {965--987},
title = {{A fast and accurate approximation for planar pose graph optimization}},
url = {http://ijr.sagepub.com/content/early/2014/05/01/0278364914523689?papetoc},
volume = {33},
year = {2014}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {::},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@inproceedings{Wang2017a,
abstract = {An approach for incorporation of a wheel odometer in monocular visual SLAM (Simultaneous Localization and Mapping) for indoor robots is proposed in this paper. It is based on nonlinear optimization parameterized in 6DoF (Degrees-of-Freedom) manifold SE(3), and constrains the robot poses to be in 3DoF space. Furthermore, to deal with the problem that in complicated indoor environment where the tracking of visual SLAM may easily get lost in some scenes with hardly any features, the approach creates new maps when new features are available and thus continuing estimating poses and feature positions to improve the robustness. The odometer can provide relative poses between maps, and when image overlaps are detected, the maps can be merged to a consistent one. Experiments are conducted on an indoor robot platform equipped with an upward-looking camera, and experimental results show the effectiveness of the proposed approach.},
author = {Wang, Jing and Shi, Zongying and Zhong, Yisheng},
booktitle = {Chinese Control Conference, CCC},
doi = {10.23919/ChiCC.2017.8028171},
file = {::},
isbn = {9789881563934},
issn = {21612927},
keywords = {SLAM,indoor robots,optimization based sensor fusion},
pages = {5167--5172},
title = {{Visual SLAM incorporating wheel odometer for indoor robots}},
year = {2017}
}
@article{Martinez-Carranza2013,
abstract = {Relocalisation in 6D is relevant to a variety of Robotics applications and in particular to agile cameras exploring a 3D environment. While the use of geometry has commonly helped to validate appearance as a back-end process in several relocalisation systems before, we are interested in using 3D information to assist fast pose relocalisation computation as part of a front-end task. Our approach rapidly searches for a reduced number of visual descriptors, previously observed and stored in a database, that can be used to effectively compute the camera pose corresponding to the current view. We guide the search by means of constructing validated candidate sets using a 3D test involving the depth information obtained with an RGB-D camera (e.g. stereo of with structured light). Our experiments demonstrate that this process returns a compact quality set that works better for the pose estimation stage than when using a typical Nearest-Neighbor search over appearance only. The improvements are observed in terms of percentage of relocalised frames and speed, where the latter goes up to two orders of magnitude w.r.t. the conventional search. {\textcopyright} 2013 IEEE.},
author = {Martinez-Carranza, Jose and Calway, Andrew and Mayol-Cuevas, Walterio},
doi = {10.1109/IROS.2013.6696457},
file = {:home/matias/Documents/Mendeley Desktop/Martinez-Carranza, Calway, Mayol-Cuevas/2013/Martinez-Carranza, Calway, Mayol-Cuevas - 2013 - Enhancing 6D visual relocalisation with depth cameras.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {899--906},
title = {{Enhancing 6D visual relocalisation with depth cameras}},
year = {2013}
}
@inproceedings{Newcombe2011a,
abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision. {\textcopyright} 2011 IEEE.},
author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2011},
doi = {10.1109/ISMAR.2011.6092378},
isbn = {9781457721830},
issn = {<null>},
keywords = {AR,Dense Reconstruction,Depth Cameras,GPU,Real-Time,SLAM,Tracking,Volumetric Representation},
month = {oct},
pages = {127--136},
pmid = {6162880},
publisher = {IEEE},
title = {{KinectFusion: Real-time dense surface mapping and tracking}},
url = {http://ieeexplore.ieee.org/document/6162880/},
year = {2011}
}
@article{Lee2008,
abstract = {This paper introduces a global uncertainty propagation scheme for the attitude dynamics of a rigid body, through a combination of numerical parametric uncertainty techniques, noncommutative harmonic analysis, and geometric numerical integration. This method is distinguished from prior approaches, as it allows one to consider probability densities that are global, and are not supported on only a single coordinate chart on the manifold. It propagates a global probability density through the full attitude dynamics, instead of replacing angular velocity dynamics with a gyro bias model. The use of Lie group variational integrators, that are symplectic and remain on the Lie group, as the underlying numerical propagator ensures that the advected probability densities respect the geometric properties of uncertainty propagation in Hamiltonian systems, which arise as consequence of the Gromov nonsqueezing theorem from symplectic geometry. We also describe how the global uncertainty propagation scheme can be applied to the problem of global attitude estimation. {\textcopyright} 2008 IEEE.},
archivePrefix = {arXiv},
arxivId = {0803.1515},
author = {Lee, Taeyoung and Leok, Melvin and McClamroch, N. Harris},
doi = {10.1109/CDC.2008.4739058},
eprint = {0803.1515},
file = {::},
isbn = {9781424431243},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
keywords = {Algebraic/geometric methods,Estimation,Stochastic systems},
number = {3},
pages = {61--66},
title = {{Global symplectic uncertainty propagation on SO(3)}},
year = {2008}
}
@inproceedings{Oriolo2012,
abstract = {We propose an odometric system for localizing a walking humanoid robot using standard sensory equipment, i.e., a camera, an Inertial Measurement Unit, joint encoders and foot pressure sensors. Our method has the prediction-correction structure of an Extended Kalman Filter. At each sampling instant, position and orientation of the torso are predicted on the basis of the differential kinematic map from the support foot to the torso, using encoder data from the support joints. The actual measurements coming from the camera (head position and orientation reconstructed by a V-SLAM algorithm) and the Inertial Measurement Unit (torso orientation) are then compared with their predicted values to correct the estimate. The filter is made aware of the current placement of the support foot by an asynchronous update mechanism triggered by the pressure sensors. An experimental validation on the humanoid NAO shows the satisfactory performance of the proposed method. {\textcopyright} 2012 IEEE.},
author = {Oriolo, Giuseppe and Paolillo, Antonio and Rosa, Lorenzo and Vendittelli, Marilena},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2012.6651513},
isbn = {9781467313698},
issn = {21640572},
month = {nov},
pages = {153--158},
publisher = {IEEE},
title = {{Vision-based Odometric Localization for humanoids using a kinematic EKF}},
url = {http://ieeexplore.ieee.org/document/6651513/},
year = {2012}
}
@article{Cully2016,
abstract = {Limbo is an open-source C++11 library for Bayesian optimization which is designed to be both highly flexible and very fast. It can be used to optimize functions for which the gradient is unknown, evaluations are expensive, and runtime cost matters (e.g., on embedded systems or robots). Benchmarks on standard functions show that Limbo is about 2 times faster than BayesOpt (another C++ library) for a similar accuracy.},
archivePrefix = {arXiv},
arxivId = {1611.07343},
author = {Cully, Antoine and Chatzilygeroudis, Konstantinos and Allocati, Federico and Mouret, Jean-Baptiste},
eprint = {1611.07343},
file = {::},
pages = {1--3},
title = {{Limbo: A Fast and Flexible Library for Bayesian Optimization}},
url = {http://arxiv.org/abs/1611.07343},
year = {2016}
}
@article{Clement2019,
abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the 'appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
archivePrefix = {arXiv},
arxivId = {1904.01080},
author = {Clement, Lee and Gridseth, Mona and Tomasi, Justin and Kelly, Jonathan},
doi = {10.1109/LRA.2020.2967659},
eprint = {1904.01080},
file = {:home/matias/Documents/Mendeley Desktop/Clement et al/2020/Clement et al. - 2020 - Learning Matchable Colorspace Transformations for Long-term Metric Visual Localization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,localization,visual learning,visual-based navigation},
number = {2},
pages = {1492--1499},
title = {{Learning Matchable Image Transformations for Long-Term Metric Visual Localization}},
url = {http://arxiv.org/abs/1904.01080},
volume = {5},
year = {2020}
}
@inproceedings{Plagianakos1999,
abstract = {Presents neural network training algorithms which are based on the differential evolution (DE) strategies introduced by Storn and Price (J. of Global Optimization, vol. 11, pp. 341-59, 1997). These strategies are applied to train neural networks with small integer weights. Such neural networks are better suited for hardware implementation than the real weight ones. Furthermore, we constrain the weights and biases in the range [-2/sup k/+1, 2/sup k/-1], for k=3,4,5. Thus, they can be represented by just k bits. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are more easily implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, the network training procedure can be more effective and efficient when large weights are allowed. Thus, for a given application, a trade-off between effectiveness and memory consumption has to be considered. We present the results of evolution algorithms for this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable. {\textcopyright} 1999 IEEE.},
author = {Plagianakos, V. P. and Vrahatis, M. N.},
booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation, CEC 1999},
doi = {10.1109/CEC.1999.785521},
isbn = {0-7803-5536-9},
pages = {2007--2013},
publisher = {IEEE},
title = {{Neural network training with constrained integer weights}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=785521 http://ieeexplore.ieee.org/document/785521/},
volume = {3},
year = {1999}
}
@article{Austin2017,
abstract = {Legged robots are capable of navigating rough terrain, but have traditionally been restricted to slow speeds. New robots combine the power density necessary for rapid motions with increasingly sophisticated leg designs. Developing controllers that effectively coordinate these high-DOF legs to generate fast, agile motions is challenging. In this paper we examine a pair of control approaches to generate high-speed trotting for the direct-drive quadruped robot Minitaur. We first show that optimization of a redesigned feed-forward trajectory improves the robot's running speed by 45%, from 1.52m/s to 1.93m/s. We then utilize a monopod version of Minitaur's 5-bar leg to directly compare this control approach to a dynamic, model-based strategy. We find gaits with the optimized trajectory are able to achieve speeds up to 2.44m/s, but the model-based dynamic controller is able to find gaits that are more robust to parameter changes, nearly as fast, and up to 70% more efficient.},
author = {Austin, Max and Brown, Jason and Geidel, Kaylee and Wang, Wenxuan and Clark, Jonathan},
doi = {10.1117/12.2262898},
file = {::},
isbn = {9781510608917},
issn = {1996756X},
journal = {Unmanned Systems Technology XIX},
pages = {1019504},
title = {{Gait design and optimization for efficient running of a direct-drive quadrupedal robot}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2262898},
volume = {10195},
year = {2017}
}
@article{Baker2004,
abstract = {Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion, mosaic construction, and face coding. Numerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. We present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. We concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. We examine which of the extensions to Lucas-Kanade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. In this paper, Part 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. In future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters.},
author = {Baker, Simon and Matthews, Iain},
doi = {10.1023/B:VISI.0000011205.11775.fd},
file = {::},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {A unifying framework,Additive vs. compositional algorithms,Efficiency,Forwards vs. inverse algorithms,Gauss-Newton,Image alignment,Levenberg-Marquardt,Lucas-Kanade,Newton,Steepest descent,The inverse compositional algorithm},
number = {3},
pages = {221--255},
title = {{Lucas-Kanade 20 years on: A unifying framework}},
volume = {56},
year = {2004}
}
@article{Churchill2012,
abstract = {This paper is about long-term navigation in environments whose appearance changes over time - suddenly or gradually. We describe, implement and validate an approach which allows us to incrementally learn a model whose complexity varies naturally in accordance with variation of scene appearance. It allows us to leverage the state of the art in pose estimation to build over many runs, a world model of sufficient richness to allow simple localisation despite a large variation in conditions. As our robot repeatedly traverses its workspace, it accumulates distinct visual experiences that in concert, implicitly represent the scene variation - each experience captures a visual mode. When operating in a previously visited area, we continually try to localise in these previous experiences while simultaneously running an independent vision based pose estimation system. Failure to localise in a sufficient number of prior experiences indicates an insufficient model of the workspace and instigates the laying down of the live image sequence as a new distinct experience. In this way, over time we can capture the typical time varying appearance of an environment and the number of experiences required tends to a constant. Although we focus on vision as a primary sensor throughout, the ideas we present here are equally applicable to other sensor modalities. We demonstrate our approach working on a road vehicle operating over a three month period at different times of day, in different weather and lighting conditions. In all, we process over 136,000 frames captured from 37km of driving. {\textcopyright} 2012 IEEE.},
author = {Churchill, Winston and Newman, Paul},
doi = {10.1109/ICRA.2012.6224596},
file = {:home/matias/Documents/Mendeley Desktop/Churchill, Newman/2012/Churchill, Newman - 2012 - Practice makes perfect Managing and leveraging visual experiences for lifelong navigation.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4525--4532},
publisher = {IEEE},
title = {{Practice makes perfect? Managing and leveraging visual experiences for lifelong navigation}},
year = {2012}
}
@article{Ma2015,
abstract = {We present a real-time system that enables a highly capable dynamic quadruped robot to maintain an accurate six-degree-of-freedom pose estimate (within a 1.0% error of distance traveled) over long distances traversed through complex, dynamic outdoor terrain, during day and night, in the presence of camera occlusion and saturation, and occasional large external disturbances, such as slips or falls. The system fuses a stereo-camera sensor, inertial measurement unit, leg odometry, and optional intermittent GPS position updates with an extended Kalman filter to ensure robust, low-latency performance. To maintain a six-degree-of-freedom local positioning accuracy alongside the global positioning knowledge, two reference frames are used; a local reference frame and a global reference frame, with the former benefiting obstacle detection and mapping and the latter for operator-specified and autonomous way-point following. Extensive experimental results obtained from multiple field tests are presented to illustrate the performance and robustness of the system over hours of continuous runs and hundreds of kilometers of distance traveled in a wide variety of terrains and conditions.},
author = {Ma, Jeremy and Bajracharya, Max and Susca, Sara and Matthies, Larry and Malchano, Matt},
doi = {10.1177/0278364915587333},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Localization,design and control,field and service robotics,field robots,legged robots,mechanics,mobile and distributed robotics SLAM,sensing and perception computer vision,sensor fusion,visual tracking},
number = {6},
pages = {631--653},
title = {{Real-time pose estimation of a dynamic quadruped in GPS-denied environments for 24-hour operation}},
url = {http://ijr.sagepub.com/cgi/content/long/35/6/631},
volume = {35},
year = {2016}
}
@phdthesis{Strasdat2012b,
abstract = {This thesis is concerned with the problem of Simultaneous Localisation and Mapping (SLAM) using visual data only. Given the video stream of a moving camera, we wish to estimate the structure of the environment and the motion of the device most accurately and in real-time. Two effective approaches were presented in the past. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods rely on the optimisation approach of bundle adjustment, but computationally must select only a small number of past frames to process. We perform a rigorous comparison between the two approaches for visual SLAM. Especially, we show that accuracy comes from a large number of points, while the number of intermediate frames only has a minor impact. We conclude that keyframe bundle adjustment is superior to filtering due to a smaller computational cost. Based on these experimental results, we develop an efficient framework for large-scale visual SLAM using the keyframe strategy. We demonstrate that SLAM using a single camera does not only drift in rotation and translation, but also in scale. In particular, we perform large-scale loop closure correction using a novel variant of pose-graph optimisation which also takes scale drift into account. Starting from this two stage approach which tackles local mo- tion estimation and loop closures separately, we develop a unified framework for real-time visual SLAM. By employing a novel double window scheme, we present a constant-time approach which enables the local accuracy of bundle adjustment while ensuring global consistency. Furthermore, we suggest a new scheme for local registration using metric loop closures and present several im- provements for the visual front-end of SLAM. Our contributions are evaluated exhaustively on a number of synthetic experiments and real-image data-set from single cameras and range imaging devices.},
author = {Strasdat, H},
booktitle = {Thesis (Ph.D.)},
number = {October},
pages = {213},
school = {Imperial College London},
title = {{Local accuracy and global consistency for efficient visual SLAM}},
year = {2012}
}
@article{Mukadam2019,
abstract = {We present a unified probabilistic framework for simultaneous trajectory estimation and planning. Estimation and planning problems are usually considered separately, however, within our framework we show that solving them simultaneously can be more accurate and efficient. The key idea is to compute the full continuous-time trajectory from start to goal at each time-step. While the robot traverses the trajectory, the history portion of the trajectory signifies the solution to the estimation problem, and the future portion of the trajectory signifies a solution to the planning problem. Building on recent probabilistic inference approaches to continuous-time localization and mapping and continuous-time motion planning, we solve the joint problem by iteratively recomputing the maximum a posteriori trajectory conditioned on all available sensor data and cost information. Our approach can contend with high-degree-of-freedom trajectory spaces, uncertainty due to limited sensing capabilities, model inaccuracy, the stochastic effect of executing actions, and can find a solution in real-time. We evaluate our framework empirically in both simulation and on a mobile manipulator.},
archivePrefix = {arXiv},
arxivId = {1807.10425},
author = {Mukadam, Mustafa and Dong, Jing and Dellaert, Frank and Boots, Byron},
doi = {10.1007/s10514-018-9770-1},
eprint = {1807.10425},
file = {:home/matias/Documents/Mendeley Desktop/Mukadam et al/2019/Mukadam et al. - 2019 - STEAP simultaneous trajectory estimation and planning.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Estimation,Factor graphs,Gaussian processes,Motion planning,Probabilistic inference,Replanning,Trajectory optimization},
month = {feb},
number = {2},
pages = {415--434},
title = {{STEAP: simultaneous trajectory estimation and planning}},
url = {http://link.springer.com/10.1007/s10514-018-9770-1},
volume = {43},
year = {2019}
}
@article{Levine2018,
abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {1805.00909},
author = {Levine, Sergey},
eprint = {1805.00909},
file = {:home/matias/Documents/Mendeley Desktop/Levine/2018/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf:pdf},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
url = {http://arxiv.org/abs/1805.00909},
year = {2018}
}
@article{Tang2019,
abstract = {Simultaneous trajectory estimation and mapping (STEAM) offers an efficient approach to continuous-time trajectory estimation, by representing the trajectory as a Gaussian process (GP). Previous formulations of the STEAM framework use a GP prior that assumes white-noise-on-acceleration, with the prior mean encouraging constant body-centric velocity. We show that such a prior cannot sufficiently represent trajectory sections with nonzero acceleration, resulting in a bias to the posterior estimates. This letter derives a novel motion prior that assumes white-noise-on-jerk, where the prior mean encourages constant body-centric acceleration. With the new prior, we formulate a variation of STEAM that estimates the pose, body-centric velocity, and body-centric acceleration. By evaluating across several datasets, we show that the new prior greatly outperforms the white-noise-on-acceleration prior in terms of the solution accuracy.},
archivePrefix = {arXiv},
arxivId = {1809.06518},
author = {Tang, Tim Yuqing and Yoon, David Juny and Barfoot, Timothy D.},
doi = {10.1109/LRA.2019.2891492},
eprint = {1809.06518},
file = {:home/matias/Documents/Mendeley Desktop/Tang, Yoon, Barfoot/2019/Tang, Yoon, Barfoot - 2019 - A White-Noise-on-Jerk Motion Prior for Continuous-Time Trajectory Estimation on SE(2).pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,localization},
number = {2},
pages = {594--601},
title = {{A White-Noise-on-Jerk Motion Prior for Continuous-Time Trajectory Estimation on SE(3)}},
volume = {4},
year = {2019}
}
@article{Blackman2016,
abstract = {This paper describes the development of a dynamic, quadrupedal robot designed for rapid traversal and interaction in human environments. We explore improvements to both physical and control methods to a legged robot (Minitaur) in order to improve the speed and stability of its gaits and increase the range of obstacles that it can overcome, with an eye toward negotiating man-made terrains such as stairs. These modifications include an analysis of physical compliance, an investigation of foot and leg design, and the implementation of ground and obstacle contact sensing for inclusion in the control schemes. Structural and mechanical improvements were made to reduce undesired compliance for more consistent agreement with dynamic models, which necessitated refinement of foot design for greater durability. Contact sensing was implemented into the control scheme for identifying obstacles and deviations in surface level for negotiation of varying terrain. Overall the incorporation of these features greatly enhances the mobility of the dynamic quadrupedal robot and helps to establish a basis for overcoming obstacles. {\textcopyright} 2016 SPIE.},
author = {Blackman, Daniel J. and Nicholson, John V. and Ordonez, Camilo and Miller, Bruce D. and Clark, Jonathan E.},
doi = {10.1117/12.2231105},
file = {::},
isbn = {9781510600782},
issn = {1996756X},
journal = {Unmanned Systems Technology XVIII},
keywords = {5-bar kinematics,detection,gait development,obstacle,quadrupedal dynamics,quadrupedal locomotion,terrain negotiation},
pages = {98370I},
title = {{Gait development on Minitaur, a direct drive quadrupedal robot}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2231105},
volume = {9837},
year = {2016}
}
@phdthesis{DarioJimenez2016,
author = {{Dario Jimenez}, Ivan},
file = {:home/matias/Documents/Mendeley Desktop/Dario Jimenez/2016/Dario Jimenez - 2016 - A Factor Graph Approach To Constrained Optimization.pdf:pdf},
number = {December},
school = {Georgia Institute of Technology},
title = {{A Factor Graph Approach To Constrained Optimization}},
year = {2016}
}
@article{Sommer2019,
abstract = {Continuous-time trajectory representation has recently gained popularity for tasks where the fusion of high-frame-rate sensors and multiple unsynchronized devices is required. Lie group cumulative B-splines are a popular way of representing continuous trajectories without singularities. They have been used in near real-time SLAM and odometry systems with IMU, LiDAR, regular, RGB-D and event cameras, as well as for offline calibration. These applications require efficient computation of time derivatives (velocity, acceleration), but all prior works rely on a computationally suboptimal formulation. In this work we present an alternative derivation of time derivatives based on recurrence relations that needs $\mathcal{O}(k)$ instead of $\mathcal{O}(k^2)$ matrix operations (for a spline of order $k$) and results in simple and elegant expressions. While producing the same result, the proposed approach significantly speeds up the trajectory optimization and allows for computing simple analytic derivatives with respect to spline knots. The results presented in this paper pave the way for incorporating continuous-time trajectory representations into more applications where real-time performance is required.},
archivePrefix = {arXiv},
arxivId = {1911.08860},
author = {Sommer, Christiane and Usenko, Vladyslav and Schubert, David and Demmel, Nikolaus and Cremers, Daniel},
doi = {10.1109/cvpr42600.2020.01116},
eprint = {1911.08860},
file = {:home/matias/Documents/Mendeley Desktop/Sommer et al/2020/Sommer et al. - 2020 - Efficient Derivative Computation for Cumulative B-Splines on Lie Groups.pdf:pdf},
pages = {11145--11153},
title = {{Efficient Derivative Computation for Cumulative B-Splines on Lie Groups}},
url = {http://arxiv.org/abs/1911.08860},
year = {2020}
}
@article{Xinjilefu2014,
abstract = {We propose a framework for using full-body dynamics for humanoid state estimation. It is formulated as an optimization problem and solved with Quadratic Programming (QP). This formulation provides two main advantages over a nonlinear Kalman filter for dynamic state estimation. QP does not require the dynamic system to be written in the state space form, and it handles equality and inequality constraints naturally. The QP state estimator considers modeling error as part of the optimization vector and includes it in the cost function. The proposed QP state estimator is tested on a Boston Dynamics Atlas humanoid robot.},
author = {Xinjilefu, X. and Feng, Siyuan and Atkeson, Christopher G.},
doi = {10.1109/IROS.2014.6942679},
file = {::},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {989--994},
title = {{Dynamic state estimation using Quadratic Programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6942679},
year = {2014}
}
@article{Rader2017,
abstract = {We present highly integrated sensor-Actuator-controller units (SAC units), addressing the increasing need for easy to use components in the design of modern high-performance robotic systems. Following strict design principles and an electro-mechanical co-design from the beginning on, our development resulted in highly integrated SAC units. Each SAC unit includes a motor, a gear unit, an IMU, sensors for torque, position and temperature as well as all necessary embedded electronics for control and communication over a high-speed EtherCAT bus. Key design considerations were easy to use interfaces and a robust cabling system. Using slip rings to electrically connect the input and output side, the units allow continuous rotation even when chained along a robotic arm. The experimental validation shows the potential of the new SAC units regarding the design of humanoid robots.},
author = {Rader, Samuel and Kaul, Lukas and Weiner, Pascal and Asfour, Tamim},
doi = {10.1109/AIM.2017.8014175},
file = {::},
isbn = {9781509059980},
journal = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM},
pages = {1160--1166},
title = {{Highly integrated sensor-Actuator-controller units for modular robot design}},
year = {2017}
}
@article{Mathur2016,
abstract = {Android robots are entering human social life. However, human-robot interactions may be complicated by a hypothetical Uncanny Valley (UV) in which imperfect human-likeness provokes dislike. Previous investigations using unnaturally blended images reported inconsistent UV effects. We demonstrate an UV in subjects' explicit ratings of likability for a large, objectively chosen sample of 80 real-world robot faces and a complementary controlled set of edited faces. An "investment game" showed that the UV penetrated even more deeply to influence subjects' implicit decisions concerning robots' social trustworthiness, and that these fundamental social decisions depend on subtle cues of facial expression that are also used to judge humans. Preliminary evidence suggests category confusion may occur in the UV but does not mediate the likability effect. These findings suggest that while classic elements of human social psychology govern human-robot social interaction, robust UV effects pose a formidable android-specific problem.},
author = {Mathur, Maya B. and Reichling, David B.},
doi = {10.1016/j.cognition.2015.09.008},
issn = {18737838},
journal = {Cognition},
keywords = {Facial appearance,Game theory,Human-robot interaction,Social decision-making,Social interaction,Trust},
month = {jan},
pages = {22--32},
pmid = {26402646},
title = {{Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027715300640},
volume = {146},
year = {2016}
}
@article{Zhu2009,
abstract = {Most state-of-the-art nonrigid shape recovery methods usually use explicit deformable mesh models to regularize surface deformation and constrain the search space. These triangulated mesh models heavily relying on the quadratic regularization term are difficult to accurately capture large deformations, such as severe bending. In this paper, we propose a novel Gaussian process regression approach to the nonrigid shape recovery problem, which does not require to involve a predefined triangulated mesh model. By taking advantage of our novel Gaussian process regression formulation together with a robust coarse-to-fine optimization scheme, the proposed method is fully automatic and is able to handle large deformations and outliers. We conducted a set of extensive experiments for performance evaluation in various environments. Encouraging experimental results show that our proposed approach is both effective and robust to nonrigid shape recovery with large deformations. {\textcopyright} 2009 IEEE.},
author = {Zhu, Jianke and Hoi, Steven C.H. and Lyu, Michael R.},
doi = {10.1109/CVPRW.2009.5206512},
isbn = {9781424439935},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {1319--1326},
title = {{Nonrigid shape recovery by gaussian process regression}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206512},
volume = {2009 IEEE },
year = {2009}
}
@article{Cvpr2018,
abstract = {With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision I$\mu$GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose 'Composite Transformation Constraints (CTCs)', that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.},
archivePrefix = {arXiv},
arxivId = {1804.03789},
author = {Iyer, Ganesh and {Krishna Murthy}, J. and Gupta, Gunshi and {Madhava Krishna}, K. and Paull, Liam},
doi = {10.1109/CVPRW.2018.00064},
eprint = {1804.03789},
file = {::},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {380--388},
title = {{Geometric consistency for self-supervised end-to-end visual odometry}},
volume = {2018-June},
year = {2018}
}
@book{Selig2005,
abstract = {Geometric Fundamentals of Robotics provides an elegant introduction to the geometric concepts that are important to applications in robotics. This second edition is still unique in providing a deep understanding of the subject: rather than focusing on computational results in kinematics and robotics, it includes significant state-of-the art material that reflects important advances in the field, connecting robotics back to mathematical fundamentals in group theory and geometry.Key features: Begins with a brief survey of basic notions in algebraic and differential geometry, Lie groups and Lie algebras Examines how, in a new chapter, Clifford algebra is relevant to robot kinematics and Euclidean geometry in 3D Introduces mathematical concepts and methods using examples from robotics Solves substantial problems in the design and control of robots via new methods Provides solutions to well-known enumerative problems in robot kinematics using intersection theory on the group of rigid body motions Extends dynamics, in another new chapter, to robots with end-effector constraints, which lead to equations of motion for parallel manipulatorsGeometric Fundamentals of Robotics serves a wide audience of graduate students as well as researchers in a variety of areas, notably mechanical engineering, computer science, and applied mathematics. It is also an invaluable reference text.---From a Review of the First Edition:"The majority of textbooks dealing with this subject cover various topics in kinematics, dynamics, control, sensing, and planning for robot manipulators. The distinguishing feature of this book is that it introduces mathematical tools, especially geometric ones, for solving problems in robotics. In particular, Lie groups and allied algebraic and geometric concepts are presented in a comprehensive manner to an audience interested in robotics. The aim of the author is to show the power and elegance of these methods as they apply to problems in robotics."-MathSciNet},
author = {Mann, C. J.H.},
booktitle = {Kybernetes},
doi = {10.1108/k.2005.06734gae.004},
file = {::},
isbn = {0387208747},
issn = {0368492X},
number = {7-8},
pages = {35--37},
publisher = {Springer Science+Business Media Inc.},
title = {{Geometric Fundamentals of Robotics}},
url = {http://books.google.com/books?hl=en&lr=&id=9FljXoISr8AC&oi=fnd&pg=PR7&dq=geometric+fundamentals+of+robotics&ots=4rDiBRo6ea&sig=RvtGOQTxmzJnBOoEDeEGwDaeYz0},
volume = {34},
year = {2005}
}
@article{Aravkin2012,
abstract = {Bundle adjustment (BA) is the problem of refining viewing and structure estimates in multi-view scene reconstruction subject to a scene model (e.g. a set of geometric constraints). Mismatched interest points cause serious problems for the standard least squares approach, as a single mismatch (i.e. outlier) will affect the entire reconstruction. We propose a novel robust Student's t BA algorithm (RST-BA), using the heavy tailed t-distribution to model reprojection errors. We design a custom algorithm to find the maximum a posteriori (MAP) estimates of the camera and viewing parameters. The algorithm exploits the same structure as L2-BA, matching the performance of fast L2 implementations. RST-BA is more accurate than either L2-BA or L 2-BA with a s-edit outlier removal rule for a range of simulated error generation scenarios. RST-BA also achieved better median reproduction error recovery than SBA [1] or SBA with outlier removal for large publicly available datasets. {\textcopyright} 2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {1111.1400v1},
author = {Aravkin, Aleksandr and Styer, Michael and Moratto, Zachary and Nefian, Ara and Broxton, Michael},
doi = {10.1109/ICIP.2012.6467220},
eprint = {1111.1400v1},
isbn = {9781467325332},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Bundle Adjustment,Optimization,Robustness,Structure from Motion,Student's t},
pages = {1757--1760},
title = {{Student's t robust bundle adjustment algorithm}},
year = {2012}
}
@article{Richner2001,
abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Es- tablished leaders in the field are the SIFT and SURF al- gorithms which exhibit great performance under a variety of image transformations, with SURF in particular consid- ered as the most computationally efficient amongst the high- performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A compre- hensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art al- gorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
author = {Tovaglieri, Alessio},
doi = {10.3929/ethz-a-010782581},
file = {::},
isbn = {8610828378018},
journal = {BRISK Binary Robust Invariant Scalable Keypoints},
number = {3},
pages = {12--19},
title = {{Research Collection}},
url = {https://doi.org/10.3929/ethz-a-010025751},
volume = {15},
year = {2011}
}
@article{Pratt2013,
author = {Pratt, Gill and Manzo, Justin},
doi = {10.1109/MRA.2013.2255424},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
month = {jun},
number = {2},
pages = {10--12},
title = {{The DARPA robotics challenge}},
url = {http://ieeexplore.ieee.org/document/6524134/},
volume = {20},
year = {2013}
}
@article{Kaess2012a,
abstract = {This paper presents a novel algorithm for integrating real-time filtering of navigation data with full map/trajectory smoothing. Unlike conventional mapping strategies, the result of loop closures within the smoother serve to correct the real-time navigation solution in addition to the map. This solution views filtering and smoothing as different operations applied within a single graphical model known as a Bayes tree. By maintaining all information within a single graph, the optimal linear estimate is guaranteed, while still allowing the filter and smoother to operate asynchronously. This approach has been applied to simulated aerial vehicle sensors consisting of a high-speed IMU and stereo camera. Loop closures are extracted from the vision system in an external process and incorporated into the smoother when discovered. The performance of the proposed method is shown to approach that of full batch optimization while maintaining real-time operation. {\textcopyright} 2012 ISIF (Intl Society of Information Fusi).},
author = {Kaess, Michael and Williams, Stephen and Indelman, Vadim and Roberts, Richard and Leonard, John J. and Dellaert, Frank},
isbn = {9780982443859},
journal = {15th International Conference on Information Fusion, FUSION 2012},
keywords = {Bayes tree,Navigation,factor graph,filtering,loop closing,smoothing},
pages = {1300--1307},
title = {{Concurrent filtering and smoothing}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6289957},
year = {2012}
}
@phdthesis{Leutenegger2014,
abstract = {Solar airplanes offer the unique capability of staying airborne for extremely long times: to date, both unmanned and manned systems have proved sustained flight, in the sense of flying through several day and night cycles. During the day, the solar module powers the airplane and re-charges a battery, which must take the airplane through the following night. Small-scale unmanned solar airplanes have thus been suggested for a plethora of non-military application scenarios, ranging from disaster response to Search and Rescue (SaR) as well as general large-scale mapping missions. This thesis addresses many aspects related to long-term autonomous operation of this special class of Unmanned Aerial Systems (UAS) in close proximity to the ground. We start in the very beginning with asking the question of how large a solar airplane should be and how it would perform, in order to accomplish a target mission. A methodo- logy is presented that performs actual aerodynamics and structural calculations of either a simplified shell or rib wing concept. The performance evaluation part also accounts for flying optimized altitude profiles, in order to allow for potential energy storage. The output of this conceptual design tool has motivated the design of the senseSoar solar airplane prototype that is equipped with enhanced sensing and processing components. We describe the details associated with the design of the different components—an engineering effort that spans various disciplines from aerodynamics to solar technology, electronics and avionics as well as structures. Furthermore, we introduce a modular sensing and processing unit that can be attached to a second solar airplane prototype, AtlantikSolar, aimed at record flying. The airplane was developed outside the scope of this work, but its realization was again motivated by the conceptual design tool. Throug- hout the design process, but also for subsequent simulations and autopilot development, aerodynamics and flight kinematics models play an important role; we present a complete toolchain for such analysis. Developing efficient components is key to any successful solar airplane design. Long- term operation, however, will only be enabled, if the aircraft additionally exhibits sufficient robustness. These two central concepts do not only apply to design, but equally to algo- rithms, which eventually turn the airplane into a system that can operate autonomously. As a basis for any autonomy, the aircraft needs to have an estimate about its internal states, as well as about its surroundings, specifically in the form of a map. Large parts of the thesis at hand address precisely the associated challenge of fusing various sensor sources under hard real-time and computational constraints. Specifically, two algorithms are presented and analyzed in detail that share a common element: namely inertial measurements, i.e. accelerometer and rate gyro readings subjected to their kinematics equations. A first fusion strategy complements this inertial module with magnetometer, static and dynamic pressure, as well as GPS measurements that are combined in an Extended Kalman Filtering (EKF) framework. The approach comes with robustness as it can withstand long-term GPS outage. Respective results show how the crucial states of orientation and airspeed including Angle of Attack (AoA) and sideslip are still tracked sufficiently well in such a case. The latter is mainly enabled by the use of an aerodynamic airplane model. The algorithm was designed to be lightweight, so it can run our microcontroller boards as an input to the autopilot. A second estimation algorithm complements the inertial module with visual cues: the combination has gained increasing attention lately since it enables accurate state estimation as well as situational awareness. We chose an approach to fuse the landmark reprojection error with inertial terms in nonlinear optimization. The concept of keyframes is implemented by resorting to marginalization, i.e. partial linearization and variable elimination, in order to keep the optimization problem bounded—while it may still span a long time interval. The framework can be used with a monocular, stereo or multi- camera setup. As we demonstrate in extensive experiments, our algorithm outperforms a competitive filtering-based approach consistently in terms of accuracy, whilst admittedly demanding more computation. In further experiments, we address calibration of the camera pose(s) relative to the inertial measurement unit; and we validate the scalability of the method—which allows to size the optimization window to the available hardware— such that real-time constraints are respected. In order to address the specific challenge of forming visual keypoint associations, we propose BRISK: Binary Robust Invariant Scalable Keypoints that aims at providing a high-speed yet high quality alternative to proven algorithms like SIFT and SURF. The scheme consists of scale-space corner detection, keypoint orientation estimation and extraction of a binary descriptor string. The latter two steps employ a sampling pattern for local brightness gradient computation and descriptor assembly from brightness comparisons. The evaluation comprises detection repeatability and descriptor similarity as well as timings when compared to SIFT, SURF, BRIEF, and FREAK. The code has been released and has found broad adoption. The presented platforms with their sensing and processing capabilities are finally used in flight tests to run the suggested algorithms. We show online operation of the inertial navigation filter. Moreover, the stereo-visual-inertial fusion was run on-board a multicopter in the control loop, which has enabled a high level of autonomy. Finally, we demonstrate the application of the mono-visual-inertial algorithm to an AtlantikSolar flight. The algorithm was augmented to accept GPS and magnetometer measurements, outputting airplane state and a consistent map—as a first milestone towards autonomous operations of small solar airplanes close to the terrain.},
author = {Leutenegger, Stefan},
booktitle = {Thesis},
keywords = {Airplane Design,Descriptor Extraction,Image Features,Image Keypoints,Inertial Navigation System,Keyframes,Keypoint Detection,Nonlinear Optimization,Sensor Fusion,Simultaneous Localization and Mapping,Solar Airplanes,State Estimation,Unmanned Aerial Systems,Visual-Inertial Navigation System,Visual-Inertial Odometry},
number = {22113},
pages = {163},
school = {ETH Zurich},
title = {{Unmanned Solar Airplanes: Design and Algorithms for Efficient and Robust Autonomous Operation}},
year = {2014}
}
@article{Sibley2006,
abstract = {This work develops a sliding window filter for incremental simultaneous localization and mapping (SLAM) that focuses computational resources on accurately estimating the immediate spatial surroundings using a sliding time window of the most recent sensor measurements. Ideally, we would like a constant time algorithm that closely approximates the all-time maximum-likelihood estimate as well as the minimum variance Cramer Rao lower bound (CRLB) - that is we would like an estimator that achieves some notion of statistical optimality (quickly converges), efficiency (quickly reduces uncertainty) and consistency (avoids over-confidence). To this end we give a derivation of the SLAM problem from the Gaussian non-linear least squares optimization perspective.We find that this results in a simple, yet general, take on the SLAM problem; we think this is a useful contribution. Our approach is inspired by the results from the photogrammetry community, dating back to the late 1950's [1], and later derivatives like Mikhail's least squares treatment [6], the Variable state dimension filter(VSDF) [5], visual odometry( VO) [4], modern bundle adjustment(BA) [10, 3] and of course extended Kalman filter (EKF) SLAM [9]. We apply the sliding window filter to SLAM with stereo vision and inertial measurements. Experiments show that the best approximate method comes close to matching the performance of the optimal estimator while attaining constant time complexity - empirically, it is often the case that the difference in their performance is indistinguishable. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
author = {Sibley, Gabe and Matthies, Larry and Sukhatme, Gaurav},
doi = {10.1007/978-0-387-75523-6_7},
institution = {University of Southern California},
isbn = {9780387755212},
issn = {18761100},
journal = {Lecture Notes in Electrical Engineering},
pages = {103--112},
title = {{A sliding window filter for incremental SLAM}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.2961&rep=rep1&type=pdf},
volume = {8 LNEE},
year = {2008}
}
@inproceedings{Da2017,
abstract = {Supervised learning is used to build a control policy for robust, stable, dynamic walking of an underactuated bipedal robot. The training and testing sets consist of controllers based on a full dynamic model, virtual constraints, and parameter optimization to meet torque limits, friction cone, and environmental conditions. The controllers are designed to induce locally exponentially stable periodic walking gaits at various speeds, both forward and backward, and for various constant ground slopes. They are also designed to induce aperiodic gaits that transition among a subset of the periodic gaits in a fixed number of steps. In experiments, the learned policy allows a 3D bipedal robot to recover from a significant kick. It also enables the robot to walk down a 22 degree slope and walk on sinusoidally varying terrain, all without using a camera. During the development of these results, it is demonstrated that supervised learning of locally exponentially stable controllers can result in a loss of stability and a means to avoid this is suggested.},
author = {Da, Xingye and Hartley, Ross and Grizzle, Jessy W.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989397},
file = {::},
isbn = {9781509046331},
issn = {10504729},
month = {may},
pages = {3476--3483},
publisher = {IEEE},
title = {{Supervised learning for stabilizing underactuated bipedal robot locomotion, with outdoor experiments on the wave field}},
url = {http://ieeexplore.ieee.org/document/7989397/},
year = {2017}
}
@article{Tobar2017,
abstract = {In sensing applications, sensors cannot always measure the latent quantity of interest at the required resolution, sometimes they can only acquire a blurred version of it due the sensor's transfer function. To recover latent signals when only noisy mixed measurements of the signal are available, we propose the Gaussian process mixture of measurements (GPMM), which models the latent signal as a Gaussian process (GP) and allows us to perform Bayesian inference on such signal conditional to a set of noisy mixture of measurements. We describe how to train GPMM, that is, to find the hyperparameters of the GP and the mixing weights, and how to perform inference on the latent signal under GPMM; additionally, we identify the solution to the underdetermined linear system resulting from a sensing application as a particular case of GPMM. The proposed model is validated in the recovery of three signals: A smooth synthetic signal, a real-world heart-rate time series and a step function, where GPMM outperformed the standard GP in terms of estimation error, uncertainty representation, and recovery of the spectral content of the latent signal.},
archivePrefix = {arXiv},
arxivId = {1707.05909},
author = {Tobar, Felipe and Rios, Gonzalo and Valdivia, Tomas and Guerrero, Pablo},
doi = {10.1109/LSP.2016.2637312},
eprint = {1707.05909},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Bayesian inference,Gaussian process (GP),convolution process,sensing applications,underdetermined linear systems},
number = {2},
pages = {231--235},
title = {{Recovering Latent Signals from a Mixture of Measurements Using a Gaussian Process Prior}},
volume = {24},
year = {2017}
}
@article{Bailey2006b,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
file = {:home/matias/Documents/Mendeley Desktop/Bailey, Durrant-Whyte/2006/Bailey, Durrant-Whyte - 2006 - Simultaneous localization and mapping (SLAM) Part II.pdf:pdf},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {108--117},
title = {{Simultaneous localization and mapping (SLAM): Part II}},
url = {http://ieeexplore.ieee.org/document/1678144/},
volume = {13},
year = {2006}
}
@article{Mangelson2018,
abstract = {This paper reports on a method for robust selection of inter-map loop closures in multi-robot simultaneous localization and mapping (SLAM). Existing robust SLAM methods assume a good initialization or an 'odometry backbone' to classify inlier and outlier loop closures. In the multi-robot case, these assumptions do not always hold. This paper presents an algorithm called Pairwise Consistency Maximization (PCM) that estimates the largest pairwise internally consistent set of measurements. Finding the largest pairwise internally consistent set can be transformed into an instance of the maximum clique problem from graph theory, and by leveraging the associated literature it can be solved in realtime. This paper evaluates how well PCM approximates the combinatorial gold standard using simulated data. It also evaluates the performance of PCM on synthetic and real-world data sets in comparison with DCS, SCGP, and RANSAC, and shows that PCM significantly outperforms these methods.},
author = {Mangelson, Joshua G. and Dominic, Derrick and Eustice, Ryan M. and Vasudevan, Ram},
doi = {10.1109/ICRA.2018.8460217},
file = {:home/matias/Documents/Mendeley Desktop/Mangelson et al/2018/Mangelson et al. - 2018 - Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2916--2923},
title = {{Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging}},
year = {2018}
}
@phdthesis{Bloesch2017,
abstract = {The primary aim of this thesis is to endow legged robots with a reliable sense of ego-motion. Just like humans or other legged beings, many legged robots require estimates of their posture and velocity in order to keep balance and move through the environment. The estimates need to exhibit both sufficiently high bandwidth and accuracy in order to allow for a controlled execution of these tasks. Furthermore, due to this dependency, failures of the state estimation may quickly lead to damaging of the robot or its surroundings, which emphasizes the importance of the reliability of the employed estimation algorithms. The associated research question may be formu- lated as finding an appropriate combination of sensor modalities and state estimation algorithms such that the ego-motion can reliably and accurately be estimated with financially and computationally reasonable costs. Furthermore, the state estimation should not limit the capabilities of the robot and thus the use of restrictive assump- tions such as a horizontal terrain, specific gait patterns, or the availability of external sensing is undesirable. In a first step the focus will be set on proprioceptive sensing in order to keep the data processing simple and thus keep time delays small and avoid additional error sources. In contrast to other types of robots, legged robots interact with their en- vironment through intermittent foot-ground contacts. Assuming stationary ground contacts and measurable forward kinematics, this interaction can provide the legged robot with a very valuable source of information. Since most modern robotic platforms are also equipped with inertial sensing devices, the combination of inertial and kine- matic data becomes an apparent approach for solving the state estimation problem for legged robots. However, careful modeling and sensor fusion design are prerequi- sites for achieving good performance. Concurrently, further challenges such as online calibration of Inertial Measurement Unit (IMU) biases, handling of slipping feet, or mitigating numerical inconsistencies have to be taken care of before truly reliable ego- motion estimation can be attained. In order to tackle these challenges we propose to co-estimate the foothold locations within a Kalman filter framework. We thereby also circumvent the use of restrictive assumptions such as a flat environment or a fixed gait pattern. In a second part we investigate the use of exteroceptive sensing in order to reduce drift that occurs when employing proprioception only. We focus on the use of cameras due to the rich information they provide while exhibiting low weight and low power consumption. In order to achieve high robustness, it is important to incorporate iner- tial data during visual processing for reducing vision-only related failure modes, such as caused by fast motion or missing texture. Furthermore, the handling of fast motion during which the scene may pass rapidly trough the field of view, is improved if visual information extraction occurs from a feature's second observation onwards. Ideally, the handling of visual features is kept simple such that no cumbersome initialization routine is required, which again improves robustness since the system can easily be reset if failures occur. Targeting these design concepts a first framework tightly com- bines optical flow measurements with inertial data. To this end an optical flow based residual is derived which relies on the co-estimation of the mean scene depth. The residual is then integrated in the update step of an IMU-driven Kalman filter. Based on a similar IMU-driven Kalman filter approach we also investigate the pos- sibility to tightly integrate the photometric information itself instead of relying on a visual pre-processing. The idea is to use the raw pixel intensity measurements directly in the Kalman filter update step by associating every landmark with a multilevel image patch. In subsequent camera frames, a photometric residual is derived by projecting the previously extracted patches into the images and computing a pixel-wise intensity error. If used within an Iterated Extended Kalman Filter (IEKF), this process di- rectly takes care of the landmark tracking and no additional data association method is required. Furthermore, since this inherent landmark tracking relies on the use of inertial and visual information simultaneously it allows the inclusion of non-corner visual features such as line segments. The overall filter framework is formulated in a fully robot-centric way where landmark locations are partitioned into bearing vec- tors and distance parameters. This allows an undelayed and stochastically accurate initialization of new landmarks leading to a truly power-up-and-go state estimation framework. Strong emphasis is set on the consistency and cleanliness of the developed methods. To this end differential geometric concepts are employed for the representation and handling of non-vector space quantities such as three-dimensional (3D) orientations and bearing vectors. The application of these concepts allows a minimal representa- tion of differences and derivatives and thereby decreases the computational costs while leading to simple and singularity-free state estimation models. Furthermore, the cor- responding “minimal” Jacobians can be used for performing a nonlinear observability analysis in order to identify the observable sub-space. All state estimation algorithms are evaluated on real datasets. In many cases they have also been implemented on real robots and employed for feedback control. For instance, the proposed kinematic and inertial sensor fusion approach has become an inherent part of the software framework running on the quadrupedal robots Star- lETH and ANYmal (see Figure 2.1). Likewise, the proposed visual inertial odometry has been applied in various Unmanned Aerial Vehicle (UAV)-related projects and is available as open-source software.},
author = {Bloesch, Michael},
doi = {10.3929/ETHZ-B-000225616},
isbn = {9783033046412},
number = {23},
school = {ETH Zurich},
title = {{State Estimation for Legged Robots - Kinematics, Inertial Sensing, and Computer Vision}},
volume = {44},
year = {2017}
}
@article{Alcantarilla2012,
abstract = {In this paper, we introduce KAZE features, a novelmultiscale 2D fea- ture detection and description algorithm in nonlinear scale spaces. Previous ap- proaches detect and describe features at different scale levels by building or ap- proximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same de- gree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In thisway, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining su- perior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable con- ductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the con- struction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods.},
author = {Alcantarilla, Pablo Fern{\'{a}}ndez and Bartoli, Adrien and Davison, Andrew J.},
doi = {10.1007/978-3-642-33783-3_16},
file = {:home/matias/Documents/Mendeley Desktop/Alcantarilla, Bartoli, Davison/2012/Alcantarilla, Bartoli, Davison - 2012 - KAZE features.pdf:pdf},
isbn = {9783642337826},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 6},
pages = {214--227},
title = {{KAZE features}},
volume = {7577 LNCS},
year = {2012}
}
@article{Faessler2015,
abstract = {Autonomous, vision-based quadrotor flight is widely regarded as a challenging perception and control problem since the accuracy of a flight maneuver is strongly influenced by the quality of the on-board state estimate. In addition, any vision-based state estimator can fail due to the lack of visual information in the scene or due to the loss of feature tracking after an aggressive maneuver. When this happens, the robot should automatically re-initialize the state estimate to maintain its autonomy and, thus, guarantee the safety for itself and the environment. In this paper, we present a system that enables a monocular-vision-based quadrotor to automatically recover from any unknown, initial attitude with significant velocity, such as after loss of visual tracking due to an aggressive maneuver. The recovery procedure consists of multiple stages, in which the quadrotor, first, stabilizes its attitude and altitude, then, re-initializes its visual state-estimation pipeline before stabilizing fully autonomously. To experimentally demonstrate the performance of our system, we aggressively throw the quadrotor in the air by hand and have it recover and stabilize all by itself. We chose this example as it simulates conditions similar to failure recovery during aggressive flight. Our system was able to recover successfully in several hundred throws in both indoor and outdoor environments.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Scaramuzza, Davide},
doi = {10.1109/ICRA.2015.7139420},
eprint = {0402594v3},
isbn = {978-1-4799-6923-4},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {1722--1729},
primaryClass = {arXiv:cond-mat},
title = {{Automatic re-initialization and failure recovery for aggressive flight with a monocular vision-based quadrotor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7139420},
volume = {2015-June},
year = {2015}
}
@article{Ball2013,
abstract = {RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. Seminal experiments using RatSLAM include mapping an entire suburb with a web camera and a long term robot delivery trial. This paper describes OpenRatSLAM, an open-source version of RatSLAM with bindings to the Robot Operating System framework to leverage advantages such as robot and sensor abstraction, networking, data playback, and visualization. OpenRatSLAM comprises connected ROS nodes to represent RatSLAM's pose cells, experience map, and local view cells, as well as a fourth node that provides visual odometry estimates. The nodes are described with reference to the RatSLAM model and salient details of the ROS implementation such as topics, messages, parameters, class diagrams, sequence diagrams, and parameter tuning strategies. The performance of the system is demonstrated on three publicly available open-source datasets. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Ball, David and Heath, Scott and Wiles, Janet and Wyeth, Gordon and Corke, Peter and Milford, Michael},
doi = {10.1007/s10514-012-9317-9},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Appearance-based,Brain-based,Hippocampus,Mapping,Navigation,Open-source,OpenRatSLAM,ROS,RatSLAM,SLAM},
number = {3},
pages = {149--176},
title = {{OpenRatSLAM: An open source brain-based SLAM system}},
url = {http://link.springer.com/10.1007/s10514-012-9317-9},
volume = {34},
year = {2013}
}
@article{Estefo,
author = {Estef{\'{o}}, Pablo},
number = {Dcc},
title = {{Restructuring Unit Tests with TestSurge.pdf}}
}
@article{Toussaint2006,
abstract = {Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing inference technique in DBNs now becomes available for answering behavioral questions-including those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems.},
author = {Toussaint, Marc and Storkey, Amos},
doi = {10.1145/1143844.1143963},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint, Storkey/2006/Toussaint, Storkey - 2006 - Probabilistic inference for solving discrete and continuous state Markov Decision Processes.pdf:pdf},
isbn = {1595933832},
journal = {ACM International Conference Proceeding Series},
pages = {945--952},
title = {{Probabilistic inference for solving discrete and continuous state Markov Decision Processes}},
volume = {148},
year = {2006}
}
@article{Lowry2016,
abstract = {Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines-particularly recognition in computer vision and animal navigation in neuroscience-have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape.We start by introducing the concepts behind place recognition-the role of place recognition in the animal kingdom, howa "place" is defined in a robotics context, and themajor components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.},
author = {Lowry, Stephanie and Sunderhauf, Niko and Newman, Paul and Leonard, John J. and Cox, David and Corke, Peter and Milford, Michael J.},
doi = {10.1109/TRO.2015.2496823},
file = {:home/matias/Documents/Mendeley Desktop/Lowry et al/2016/Lowry et al. - 2016 - Visual Place Recognition A Survey.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Place recognition,Visual place recognition},
number = {1},
pages = {1--19},
title = {{Visual Place Recognition: A Survey}},
volume = {32},
year = {2016}
}
@article{Linegar2016,
abstract = {This paper is about camera-only localisation in challenging outdoor environments, where changes in lighting, weather and season cause traditional localisation systems to fail. Conventional approaches to the localisation problem rely on point-features such as SIFT, SURF or BRIEF to associate landmark observations in the live image with landmarks stored in the map; however, these features are brittle to the severe appearance change routinely encountered in outdoor environments. In this paper, we propose an alternative to traditional point-features: we train place-specific linear SVM classifiers to recognise distinctive elements in the environment. The core contribution of this paper is an unsupervised mining algorithm which operates on a single mapping dataset to extract distinct elements from the environment for localisation. We evaluate our system on 205km of data collected from central Oxford over a period of six months in bright sun, night, rain, snow and at all times of the day. Our experiment consists of a comprehensive N-vs-N analysis on 22 laps of the approximately 10km route in central Oxford. With our proposed system, the portion of the route where localisation fails is reduced by a factor of 6, from 33.3% to 5.5%.},
author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
doi = {10.1109/ICRA.2016.7487208},
file = {:home/matias/Documents/Mendeley Desktop/Linegar, Churchill, Newman/2016/Linegar, Churchill, Newman - 2016 - Made to measure Bespoke landmarks for 24-hour, all-weather localisation with a camera(2).pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {787--794},
publisher = {IEEE},
title = {{Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Rublee2011,
abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone. {\textcopyright} 2011 IEEE.},
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126544},
isbn = {9781457711015},
issn = {1550-5499},
pages = {2564--2571},
pmid = {20033598},
title = {{ORB: An efficient alternative to SIFT or SURF}},
year = {2011}
}
@article{Dinh2017,
abstract = {Despite their overwhelming capacity to ovcrfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparamelrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
archivePrefix = {arXiv},
arxivId = {1703.04933},
author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
eprint = {1703.04933},
file = {::},
isbn = {9781510855144},
issn = {1938-7228},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {1705--1714},
title = {{Sharp minima can generalize for deep nets}},
url = {http://arxiv.org/abs/1703.04933},
volume = {3},
year = {2017}
}
@article{Murphy2015,
abstract = {The DARPA Robotics Challenge trials offer insights into how the robotics community approaches the design of intelligent systems and the role of supervision and simulation. A survey of the teams suggests that the design process may be hampered by the lack of a recognized canon of intelligent design principles and references and by the underrepresentation of artificial intelligence experts on the teams. The teams generally approached supervision and simulation as a fine-grained execution approval activity rather than as task rehearsal for the entire action sequence.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Murphy, Robin R.},
doi = {10.1002/rob.21578},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {189--191},
pmid = {22164016},
title = {{Meta-analysis of autonomy at the DARPA Robotics Challenge trials}},
volume = {32},
year = {2015}
}
@article{Milford2010,
abstract = {The challenge of persistent navigation and mapping is to develop an autonomous robot system that can simultaneously localize, map and navigate over the lifetime of the robot with little or no human intervention. Most solutions to the simultaneous localization and mapping (SLAM) problem aim to produce highly accurate maps of areas that are assumed to be static. In contrast, solutions for persistent navigation and mapping must produce reliable goal-directed navigation outcomes in an environment that is assumed to be in constant flux. We investigate the persistent navigation and mapping problem in the context of an autonomous robot that performs mock deliveries in a working office environment over a two-week period. The solution was based on the biologically inspired visual SLAM system, RatSLAM. RatSLAM performed SLAM continuously while interacting with global and local navigation systems, and a task selection module that selected between exploration, delivery, and recharging modes. The robot performed 1,143 delivery tasks to 11 different locations with only one delivery failure (from which it recovered), traveled a total distance of more than 40 km over 37 hours of active operation, and recharged autonomously a total of 23 times. {\textcopyright} 2010 The Author(s).},
author = {Milford, Michael and Wyeth, Gordon},
doi = {10.1177/0278364909340592},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {RatSLAM,SLAM,biologically inspired,persistent navigation and mapping},
number = {9},
pages = {1131--1153},
title = {{Persistent navigation and mapping using a biologically inspired slam system}},
volume = {29},
year = {2010}
}
@article{Triggs2000,
abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
author = {Triggs, Bill and McLauchlan, Philip F. and Hartley, Richard I. and Fitzgibbon, Andrew W.},
doi = {10.1007/3-540-44480-7_21},
isbn = {9783540679738},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bundle Adjustment,Gauge Freedom,Optimization,Scene Reconstruction,Sparse Matrices},
pages = {298--372},
title = {{Bundle adjustment – a modern synthesis}},
url = {http://dx.doi.org/10.1007/3-540-44480-7_21%5Cnhttp://link.springer.com/10.1007/3-540-44480-7_21},
volume = {1883},
year = {2000}
}
@article{Mendes2017,
abstract = {ISSN: 2359-3024},
author = {Mendes, Augusto B and Guimar{\~{a}}es, Felipe V and Eirado-silva, Clara B P and Silva, Edson P},
file = {::},
journal = {Journal of Geek Studies},
number = {1},
pages = {39--67},
title = {{The ichthyological diversity of Pok{\'{e}}mon}},
volume = {4},
year = {2017}
}
@article{VonStumberg2018a,
abstract = {We present VI-DSO, a novel approach for visual-inertial odometry, which jointly estimates camera poses and sparse scene geometry by minimizing photometric and IMU measurement errors in a combined energy functional. The visual part of the system performs a bundle-adjustment like optimization on a sparse set of points, but unlike key-point based systems it directly minimizes a photometric error. This makes it possible for the system to track not only corners, but any pixels with large enough intensity gradients. IMU information is accumulated between several frames using measurement preintegration, and is inserted into the optimization as an additional constraint between keyframes. We explicitly include scale and gravity direction into our model and jointly optimize them together with other variables such as poses. As the scale is often not immediately observable using IMU data this allows us to initialize our visual-inertial system with an arbitrary scale instead of having to delay the initialization until everything is observable. We perform partial marginalization of old variables so that updates can be computed in a reasonable time. In order to keep the system consistent we propose a novel strategy which we call 'dynamic marginalization'. This technique allows us to use partial marginalization even in cases where the initial scale estimate is far from the optimum. We evaluate our method on the challenging EuRoC dataset, showing that VI-DSO outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1804.05625},
author = {{Von Stumberg}, Lukas and Usenko, Vladyslav and Cremers, Daniel},
doi = {10.1109/ICRA.2018.8462905},
eprint = {1804.05625},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {apr},
number = {section 1},
pages = {2510--2517},
title = {{Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization}},
url = {http://arxiv.org/abs/1804.05625},
year = {2018}
}
@article{Li2014,
abstract = {In this paper, we focus on the problem of pose estimation using measurements from an inertial measurement unit and a rolling-shutter (RS) camera. The challenges posed by RS image capture are typically addressed by using approximate, low-dimensional representations of the camera motion. However, when the motion contains significant accelerations (common in small-scale systems) these representations can lead to loss of accuracy. By contrast, we here describe a different approach, which exploits the inertial measurements to avoid any assumptions on the nature of the trajectory. Instead of parameterizing the trajectory, our approach parameterizes the errors in the trajectory estimates by a low-dimensional model. A key advantage of this approach is that, by using prior knowledge about the estimation errors, it is possible to obtain upper bounds on the modeling inaccuracies incurred by different choices of the parameterization's dimension. These bounds can provide guarantees for the performance of the method, and facilitate addressing the accuracy-efficiency tradeoff. This RS formulation is used in an extended-Kalman-filter estimator for localization in unknown environments. Our results demonstrate that the resulting algorithm outperforms prior work, in terms of accuracy and computational cost. Moreover, we demonstrate that the algorithm makes it possible to use low-cost consumer devices (i.e. smartphones) for high-precision navigation on multiple platforms.},
author = {Li, Mingyang and Mourikis, Anastasios I.},
doi = {10.1177/0278364914538326},
file = {::},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {EKF-based localization,Vision-aided inertial navigation,cellphone localization,rolling-shutter camera},
number = {11},
pages = {1490--1507},
title = {{Vision-aided inertial navigation with rolling-shutter cameras}},
url = {http://ijr.sagepub.com/content/33/11/1490.short},
volume = {33},
year = {2014}
}
@inproceedings{Strasdat2011,
abstract = {We present a novel and general optimisation framework for visual SLAM, which scales for both local, highly accurate reconstruction and large-scale motion with long loop closures. We take a two-level approach that combines accurate pose-point constraints in the primary region of interest with a stabilising periphery of pose-pose soft constraints. Our algorithm automatically builds a suitable connected graph of keyposes and constraints, dynamically selects inner and outer window membership and optimises both simultaneously. We demonstrate in extensive simulation experiments that our method approaches the accuracy of offline bundle adjustment while maintaining constant-time operation, even in the hard case of very loopy monocular camera motion. Furthermore, we present a set of real experiments for various types of visual sensor and motion, including large scale SLAM with both monocular and stereo cameras, loopy local browsing with either monocular or RGB-D cameras, and dense RGB-D object model building. {\textcopyright} 2011 IEEE.},
author = {Strasdat, Hauke and Davison, Andrew J. and Montiel, J. M.M. and Konolige, Kurt},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126517},
isbn = {9781457711015},
issn = {1550-5499},
pages = {2352--2359},
title = {{Double window optimisation for constant time visual SLAM}},
year = {2011}
}
@article{Nitsche2019,
abstract = {This paper presents a Teach Repeat (TR) algorithm from stereo and inertial data, targeting Unmanned Aerial Vehicles with limited on-board computational resources. We propose a tightly-coupled, relative formulation of the visual-inertial constraints that fits the TR application. In order to achieve real-time operation on limited hardware, we constraint it to motion-only visual-inertial Bundle Adjustment and solve for the minimal set of states. For the repeat phase, we show how to generate a trajectory and smoothly follow it with a constantly changing reference frame. The proposed method is validated with the sequences of the EuRoC dataset as well as within a simulated environment, running on a standard laptop PC and on a low-cost Odroid X-U4 computer.},
author = {Nitsche, Matias and Pessacg, Facundo and Civera, Javier},
doi = {10.1109/ECMR.2019.8870926},
file = {:home/matias/Documents/Mendeley Desktop/Nitsche, Pessacg, Civera/2019/Nitsche, Pessacg, Civera - 2019 - Visual-inertial teach repeat for aerial robot navigation.pdf:pdf},
isbn = {9781728136059},
journal = {2019 European Conference on Mobile Robots, ECMR 2019 - Proceedings},
publisher = {IEEE},
title = {{Visual-inertial teach repeat for aerial robot navigation}},
year = {2019}
}
@incollection{Stefan2014,
abstract = {Mapping, localization and navigation are major topics and challenges for mobile robotics. To perform tasks and to interact efficiently in the environment, a robot needs knowledge about its surroundings. Many robots today are capable of performing simultaneous mapping and localization to generate own world representations. Most assume an array of highly sophisticated artificial sensors to track landmarks placed in the environment. Recently, there has been significant interest in research approaches inspired by nature and RatSLAM is one of them. It has been introduced and tested on wheeled robots with good results. To examine how RatSLAM behaves on humanoid robots, we adapt this model for the first time to this platform by adjusting the given constraints. Furthermore, we introduce a multiple hypotheses mapping technique which improves mapping robustness in open spaces with features visible from several distant locations. {\textcopyright} 2014 Springer International Publishing Switzerland.},
author = {M{\"{u}}ller, Stefan and Weber, Cornelius and Wermter, Stefan},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-11179-7_99},
isbn = {9783319111780},
issn = {16113349},
keywords = {Humanoid robot,Localization,Mapping,RatSLAM,SLAM,visual SLAM},
number = {Icann},
pages = {789--796},
title = {{RatSLAM on humanoids - A bio-inspired SLAM model adapted to a humanoid robot}},
url = {http://link.springer.com/10.1007/978-3-319-11179-7_99},
volume = {8681 LNCS},
year = {2014}
}
@inproceedings{Forster2014,
abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6906584},
isbn = {978-1-4799-3685-4},
issn = {10504729},
month = {may},
pages = {15--22},
pmid = {6576973927449638915},
publisher = {IEEE},
title = {{SVO: Fast semi-direct monocular visual odometry}},
url = {http://ieeexplore.ieee.org/document/6906584/},
year = {2014}
}
@article{Yang2020a,
abstract = {We propose the first fast and certifiable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. Towards this goal, we first reformulate the registration problem using a Truncated Least Squares (TLS) cost that makes the estimation insensitive to spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second certifiable algorithm, named TEASER++, that circumvents the need to solve an SDP and runs in milliseconds. For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard benchmarks, object detection datasets, and the 3DMatch scan matching dataset, and show that (i) both algorithms dominate the state of the art (e.g., RANSAC, branch-&-bound, heuristics) and are robust to more than 99% outliers, (ii) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm, (iii) TEASER++ is so robust it can also solve problems without correspondences (e.g., hypothesizing all-to-all correspondences) where it largely outperforms ICP. We release a fast open-source C++ implementation of TEASER++.},
archivePrefix = {arXiv},
arxivId = {2001.07715},
author = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
eprint = {2001.07715},
file = {:home/matias/Documents/Mendeley Desktop/Yang, Shi, Carlone/2020/Yang, Shi, Carlone - 2020 - TEASER Fast and Certifiable Point Cloud Registration.pdf:pdf},
number = {c},
pages = {1--42},
title = {{TEASER: Fast and Certifiable Point Cloud Registration}},
url = {http://arxiv.org/abs/2001.07715},
year = {2020}
}
@phdthesis{Klingensmith2016a,
author = {Klingensmith, Matthew},
booktitle = {Camera},
number = {1011344},
school = {Carnegie Mellon University},
title = {{Automatically Tracking and Calibrating Articulated Robots Using Slam Techniques}},
year = {2008}
}
@article{Pomerleau2015,
abstract = {The topic of this review is geometric registration in robotics. Registration algorithms associate sets of data into a common coordinate system. They have been used extensively in object reconstruction, inspection, medical application, and localization of mobile robotics. We focus on mobile robotics applications in which point clouds are to be registered. While the underlying principle of those algorithms is simple, many variations have been proposed for many different applications. In this review, we give a historical perspective of the registration problem and show that the plethora of solutions can be organized and differentiated according to a few elements. Accordingly, we present a formalization of geometric registration and cast algorithms proposed in the literature into this framework. Finally, we review a few applications of this framework in mobile robotics that cover different kinds of platforms, environments, and tasks. These examples allow us to study the specific requirements of each use case and the necessary configuration choices leading to the registration implementation. Ultimately, the objective of this review is to provide guidelines for the choice of geometric registra- tion configuration},
author = {Pomerleau, Fran{\c{c}}ois and Colas, Francis and Siegwart, Roland},
doi = {10.1561/2300000035},
isbn = {9781680830},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
number = {1},
pages = {1--104},
title = {{A Review of Point Cloud Registration Algorithms for Mobile Robotics}},
url = {http://www.nowpublishers.com/article/Details/ROB-035},
volume = {4},
year = {2015}
}
@article{Vedaldi2007,
abstract = {It is well-known that forward motion induces a large number of local minima in the instantaneous least-squares reprojection error. This is caused in part by singularities in the error landscape around the forward direction, and presents a challenge in using existing algorithms for structure-from-motion in autonomous navigation applications. In this paper we prove that imposing a bound on the reconstructed depth of the scene makes the least-squares reprojection error continuous. This has implications for autonomous navigation, as it suggests simple modifications for existing algorithms to minimize the effects of local minima in forward translation. {\textcopyright} 2007 IEEE.},
author = {Vedaldi, Andrea and Guidi, Gregorio and Soatto, Stefano},
doi = {10.1109/CVPR.2007.383117},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--7},
title = {{Moving forward in structure from motion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270142},
year = {2007}
}
@article{Gouaillier2008,
abstract = {This article presents the design of the autonomous humanoid robot called NAO that is built by the French company Aldebaran-Robotics. With its height of 0.57 m and its weight about 4.5 kg, this innovative robot is lightweight and compact. It distinguishes itself from its existing Japanese, American, and other counterparts thanks to its pelvis kinematics design, its proprietary actuation system based on brush DC motors, its electronic, computer and distributed software architectures. This robot has been designed to be affordable without sacrificing quality and performance. It is an open and easy-to-handle platform where the user can change all the embedded system software or just add some applications to make the robot adopt specific behaviours. The robot's head and forearms are modular and can be changed to promote further evolution. The comprehensive and functional design is one of the reasons that helped select NAO to replace the AIBO quadrupeds in the 2008 RoboCup standard league.},
archivePrefix = {arXiv},
arxivId = {0807.3223},
author = {Gouaillier, David and Hugel, Vincent and Blazevic, Pierre and Kilner, Chris and Monceaux, Jerome and Lafourcade, Pascal and Marnier, Brice and Serre, Julien and Maisonnier, Bruno},
eprint = {0807.3223},
file = {::},
journal = {arXiv},
month = {jul},
pages = {1--10},
title = {{The NAO humanoid: a combination of performance and affordability}},
url = {http://arxiv.org/abs/0807.3223},
year = {2008}
}
@article{Scaramuzza2011a,
abstract = {This paper presents a new method to estimate the relative motion of a vehicle from images of a single camera. The computational cost of the algorithm is limited only by the feature extraction and matching process, as the outlier removal and the motion estimation steps take less than a fraction of millisecond with a normal laptop computer. The biggest problem in visual motion estimation is data association; matched points contain many outliers that must be detected and removed for the motion to be accurately estimated. In the last few years, a very established method for removing outliers has been the "5-point RANSAC" algorithm which needs a minimum of 5 point correspondences to estimate the model hypotheses. Because of this, however, it can require up to several hundreds of iterations to find a set of points free of outliers. In this paper, we show that by exploiting the nonholonomic constraints of wheeled vehicles it is possible to use a restrictive motion model which allows us to parameterize the motion with only 1 point correspondence. Using a single feature correspondence for motion estimation is the lowest model parameterization possible and results in the two most efficient algorithms for removing outliers: 1-point RANSAC and histogram voting. To support our method we run many experiments on both synthetic and real data and compare the performance with a state-of-the-art approach. Finally, we show an application of our method to visual odometry by recovering a 3 Km trajectory in a cluttered urban environment and in real-time. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Scaramuzza, Davide},
doi = {10.1007/s11263-011-0441-3},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Outlier removal,Ransac,Structure from motion},
number = {1},
pages = {74--85},
title = {{1-point-RANSAC structure from motion for vehicle-mounted cameras by exploiting non-holonomic constraints}},
volume = {95},
year = {2011}
}
@article{Martinelli2014,
abstract = {This paper investigates the visual-inertial structure from motion problem. A simple closed form solution to this problem is introduced. Special attention is devoted to identify the conditions under which the problem has a finite number of solutions. Specifically, it is shown that the problem can have a unique solution, two distinct solutions and infinite solutions depending on the trajectory, on the number of point-features and on their layout and on the number of camera images. The investigation is also performed in the case when the inertial data are biased, showing that, in this latter case, more images and more restrictive conditions on the trajectory are required for the problem resolvability. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Martinelli, Agostino},
doi = {10.1007/s11263-013-0647-7},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Inertial sensors,Robotics,Sensor fusion,Structure from motion},
number = {2},
pages = {138--152},
title = {{Closed-form solution of visual-inertial structure from motion}},
volume = {106},
year = {2014}
}
@article{Ghahramani2004,
abstract = {We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised learning, including factor analysis, PCA, mixtures of Gaussians, ICA, hidden Markov models, state-space models, and many variants and extensions. We derive the EM algorithm and give an overview of fundamental concepts in graphical models, and inference algorithms on graphs. This is followed by a quick tour of approximate Bayesian inference, including Markov chain Monte Carlo (MCMC), Laplace approximation, BIG, variational approximations, and expectation propagation (EP). The aim of this chapter is to provide a high-level view of the field. Along the way, many state-ofthe-art ideas and future directions are also reviewed. {\textcopyright} Springer-Verlag 2004.},
author = {Ghahramani, Zoubin},
doi = {10.1007/978-3-540-28650-9_5},
file = {::},
isbn = {978-3-540-23122-6},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Chapter 5},
pages = {72--112},
pmid = {24729780},
title = {{Unsupervised learning}},
url = {http://link.springer.com/10.1007/978-3-540-28650-9_5%5Cnpapers3://publication/doi/10.1007/978-3-540-28650-9_5},
volume = {3176},
year = {2004}
}
@book{Szeliski2011,
abstract = {4th ed. "Computer and Machine Vision: Theory, Algorithms, Practicalities (previously entitled Machine Vision) clearly and systematically presents the basic methodology of computer and machine vision, covering the essential elements of the theory while emphasizing algorithmic and practical design constraints. This fully revised fourth edition has brought in more of the concepts and applications of computer vision, making it a very comprehensive and up-to-date tutorial text suitable for graduate students, researchers and R & D engineers working in this vibrant subject. Key features include: Practical examples and case studies give the 'ins and outs' of developing real-world vision systems, giving engineers the realities of implementing the principles in practice; New chapters containing case studies on surveillance and driver assistance systems give practical methods on these cutting-edge applications in computer vision; Necessary mathematics and essential theory are made approachable by careful explanations and well-illustrated examples; Updated content and new sections cover topics such as human iris location, image stitching, line detection using RANSAC, performance measures, and hyperspectral imaging; The 'recent developments' section now included in each chapter will be useful in bringing students and practitioners up to date with the subject."--Publisher's description. Chapter 1. Vision, the Challenge -- Part 1. Low-level Vision -- Chapter 2. Images and Imaging Operations -- Chapter 3. Basic Image Filtering Operations -- Chapter 4. Thresholding Techniques -- Chapter 5. Edge Detection -- Chapter 6. Corner and Interest Point Detection -- Chapter 7. Mathematical Morphology -- Chapter 8. Texture. Part 2. Intermediate-level Vision -- Chapter 9. Binary Shape Analysis -- Chapter 10. Boundary Pattern Analysis -- Chapter 11. Line Detection -- Chapter 12. Circle and Ellipse Detection -- Chapter 13. The Hough Transform and Its Nature -- Chapter 14. Pattern Matching Techniques. Part 3. 3-D Vision and Motion -- Chapter 15. The Three-Dimensional World -- Chapter 16. Tackling the Perspective n-point Problem -- Chapter 17. Invariants and Perspective -- Chapter 18. Image Transformations and Camera Calibration -- Chapter 19. Motion. Part 4. Toward Real-time Pattern Recognition Systems -- Chapter 20. Automated Visual Inspection -- Chapter 21. Inspection of Cereal Grains -- Chapter 22. Surveillance -- Chapter 23. In-Vehicle Vision Systems -- Chapter 24. Statistical Pattern Recognition -- Chapter 25. Image Acquisition -- Chapter 26. Real-Time Hardware and Systems Design Considerations -- Chapter 27. Epilogue--Perspectives in Vision -- Appendix A. Robust Statistics.},
address = {London},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Szeliski, Richard},
booktitle = {Zhurnal Eksperimental'noi i Teoreticheskoi Fiziki},
doi = {10.1007/978-1-84882-935-0},
eprint = {arXiv:1011.1669v3},
file = {:home/matias/Documents/Mendeley Desktop/Davies/2012/Davies - 2012 - Computer Vision.pdf:pdf},
isbn = {978-1-84882-934-3},
issn = {10636919},
keywords = {Davies,E.,R.},
pages = {1--912},
pmid = {16259003},
publisher = {Springer London},
series = {Texts in Computer Science},
title = {{Computer Vision}},
url = {https://inspirit.net.in/books/academic/Computer and Machine Vision 4e - Theory, Algorithms, Practicalities By E R Davies.pdf%0Awww.elsevier.com/permissions. http://link.springer.com/10.1007/978-1-84882-935-0},
volume = {73},
year = {2011}
}
@inproceedings{Georg2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems. {\textcopyright}2007 IEEE.},
author = {Klein, Georg and Murray, David},
booktitle = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
doi = {10.1109/ISMAR.2007.4538852},
isbn = {9781424417506},
month = {nov},
pages = {1--10},
publisher = {IEEE},
title = {{Parallel tracking and mapping for small AR workspaces}},
url = {http://www.youtube.com/watch?v=Y9HMn6bd-v8 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4538852},
year = {2007}
}
@phdthesis{Williams2009,
abstract = {This thesis describes a system which is able to track the pose of a hand-held cam- era as it moves around a scene. The system builds a 3D map of point landmarks in the world while tracking the pose of the camera relative to this map using a process called simultaneous localisation and mapping (SLAM). To achieve real- time performance, the map must be kept sparse, but rather than observing only the mapped landmarks like previous systems, observations are made of features across the entire image. Their deviation from the predicted epipolar geometry is used to further constrain the estimated inter-frame motion and so improves the overall accuracy. The consistency of the estimation is also improved by perform- ing the estimation in a camera-centred coordinate frame. As with any such system, tracking failure is inevitable due to occlusion or sudden motion of the camera. A relocalisation module is presented which monitors the SLAM system, detects tracking failure, and then resumes tracking as soon as the conditions have improved. This relocalisation process is achieved using a new landmark recognition algorithm which is trained on-line and provides high recall and a fast recognition time. The relocalisation module can also be used to achieve place recognition for a loop closure detection system. By taking into account both the geometry and appearance information when determining a loop closure this module is able to outperform previous loop closure detection techniques used in monocular SLAM. After recognising an overlap, the map is then corrected using a novel trajectory alignment technique that is able to cope with the inherent scale ambiguity in monocular SLAM. By incorporating all of these new techniques, the system presented can perform as a robust augmented reality system, or act as a navigation tool which could be used on a mobile robot in indoor and outdoor environments.},
author = {Williams, Brian P and Reid, Ian D},
school = {University of Oxford},
title = {{Simultaneous Localisation and Mapping Using a Single Camera}},
url = {http://www.robots.ox.ac.uk/ActiveVision/Publications/williams%7B%7B%7D%7B%7B%7D%7B%7D%7D%7B%7B%7D%7B_%7D%7B%7D%7D%7B%7B%7D%7B%7D%7D%7B%7D%7Ddphil2009/williams%7B%7B%7D%7B%7B%7D%7B%7D%7D%7B%7B%7D%7B_%7D%7B%7D%7D%7B%7B%7D%7B%7D%7D%7B%7D%7Ddphil2009.bib%5Cnh},
year = {2009}
}
@article{Glover2012,
abstract = {Appearance-based loop closure techniques, which leverage the high information content of visual images and can be used independently of pose, are now widely used in robotic applications. The current state-of-the-art in the field is Fast Appearance-Based Mapping (FAB-MAP) having been demonstrated in several seminal robotic mapping experiments. In this paper, we describe OpenFABMAP, a fully open source implementation of the original FAB-MAP algorithm. Beyond the benefits of full user access to the source code, OpenFABMAP provides a number of configurable options including rapid codebook training and interest point feature tuning. We demonstrate the performance of OpenFABMAP on a number of published datasets and demonstrate the advantages of quick algorithm customisation. We present results from OpenFABMAP's application in a highly varied range of robotics research scenarios. {\textcopyright} 2012 IEEE.},
author = {Glover, Arren and Maddern, William and Warren, Michael and Reid, Stephanie and Milford, Michael and Wyeth, Gordon},
doi = {10.1109/ICRA.2012.6224843},
file = {:home/matias/Documents/Mendeley Desktop/Glover et al/2012/Glover et al. - 2012 - OpenFABMAP An open source toolbox for appearance-based loop closure detection(2).pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4730--4735},
title = {{OpenFABMAP: An open source toolbox for appearance-based loop closure detection}},
year = {2012}
}
@article{Camurri2017,
abstract = {Reliable state estimation is crucial for stable planning and control of legged locomotion. A fundamental component of a state estimator in legged platforms is Leg Odometry, which only requires information about kinematics and contacts. Many legged robots use dedicated sensors on each foot to detect ground contacts. However, this choice is impractical for many agile legged robots in field operations, as these sensors often degrade and break. Instead, this paper focuses on the development of a robust Leg Odometry module, which does not require contact sensors. The module estimates the probability of reliable contact and detects foot impacts using internal force sensing. This knowledge is then used to improve the kinematics-inertial state estimate of the robot's base. We show how our approach can reach comparable performance to systems with foot sensors. Extensive experimental results lasting over 1 h are presented on our 85 \textkg quadrupedal robot HyQ carrying out a variety of gaits.},
author = {Camurri, Marco and Fallon, Maurice and Bazeille, Stephane and Radulescu, Andreea and Barasuol, Victor and Caldwell, Darwin G. and Semini, Claudio},
doi = {10.1109/LRA.2017.2652491},
file = {::},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Localization,multilegged robots,sensor fusion},
number = {2},
pages = {1023--1030},
title = {{Probabilistic Contact Estimation and Impact Detection for State Estimation of Quadruped Robots}},
url = {http://ieeexplore.ieee.org/document/7815333/},
volume = {2},
year = {2017}
}
@article{Slam2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2008.02274v1},
author = {Slam, Map-centric Dense Lidar and Park, Chanoh and Member, Student and Moghadam, Peyman and Williams, Jason},
eprint = {arXiv:2008.02274v1},
file = {:home/matias/Documents/Mendeley Desktop/Slam et al/2020/Slam et al. - 2020 - Elasticity Meets Continuous-Time.pdf:pdf},
number = {X},
pages = {1--19},
title = {{Elasticity Meets Continuous-Time :}},
volume = {XX},
year = {2020}
}
@inproceedings{Fallon2014,
abstract = {This paper describes an algorithm for the probabilistic fusion of sensor data from a variety of modalities (inertial, kinematic and LIDAR) to produce a single consistent position estimate for a walking humanoid. Of specific interest is our approach for continuous LIDAR-based localization which maintains reliable drift-free alignment to a prior map using a Gaussian Particle Filter. This module can be bootstrapped by constructing the map on-the-fly and performs robustly in a variety of challenging field situations. We also discuss a two-tier estimation hierarchy which preserves registration to this map and other objects in the robot's vicinity while also contributing to direct low-level control of a Boston Dynamics Atlas robot. Extensive experimental demonstrations illustrate how the approach can enable the humanoid to walk over uneven terrain without stopping (for tens of minutes), which would otherwise not be possible. We characterize the performance of the estimator for each sensor modality and discuss the computational requirements.},
author = {Fallon, Maurice F. and Antone, Matthew and Roy, Nicholas and Teller, Seth},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2014.7041346},
file = {::},
isbn = {9781479971749},
issn = {21640580},
month = {nov},
pages = {112--119},
publisher = {IEEE},
title = {{Drift-free humanoid state estimation fusing kinematic, inertial and LIDAR sensing}},
volume = {2015-Febru},
year = {2015}
}
@book{Jagannathan1989,
abstract = {The application of blackboard architectures to control problems, along with associated issues is discussed. Issues of time scales, real-time responsiveness, and hierarchical control are considered, along with, two control paradigms that are being used in conjunction with blackboard systems: the cognitive-servo approach and the hierarchical-controller approach.},
address = {Orlando, FL, USA},
author = {Corkill, Daniel D.},
doi = {10.1109/isic.1990.128436},
isbn = {0818621087},
pages = {36--38},
publisher = {Academic Press, Inc.},
title = {{Blackboard architectures and control applications}},
year = {1990}
}
@article{Seok2015,
abstract = {This paper presents the design principles for highly efficient legged robots, the implementation of the principles in the design of the MIT Cheetah, and the analysis of the high-speed trotting experimental results. The design principles were derived by analyzing three major energy-loss mechanisms in locomotion: heat losses from the actuators, friction losses in transmission, and the interaction losses caused by the interface between the system and the environment. Four design principles that minimize these losses are discussed: employment of high torque-density motors, energy regenerative electronic system, low loss transmission, and a low leg inertia. These principles were implemented in the design of the MIT Cheetah; the major design features are large gap diameter motors, regenerative electric motor drivers, single-stage low gear transmission, dual coaxial motors with composite legs, and the differential actuated spine. The experimental results of fast trotting are presented; the 33-kg robot runs at 22 km/h (6 m/s). The total power consumption from the battery pack was 973 W and resulted in a total cost of transport of 0.5, which rivals running animals' at the same scale. 76% of the total energy consumption is attributed to heat loss from the motor, and the remaining 24% is used in mechanical work, which is dissipated as interaction loss as well as friction losses at the joint and transmission.},
author = {Seok, Sangok and Wang, Albert and Chuah, Meng Yee and Hyun, Dong Jin and Lee, Jongwoo and Otten, David M. and Lang, Jeffrey H. and Kim, Sangbae},
doi = {10.1109/TMECH.2014.2339013},
isbn = {1083-4435 VO - 20},
issn = {10834435},
journal = {IEEE/ASME Transactions on Mechatronics},
keywords = {Cost of transport (CoT),Efficiency,Energy regeneration,Legged locomotion,Quadrupeds robot},
number = {3},
pages = {1117--1129},
title = {{Design principles for energy-efficient legged locomotion and implementation on the MIT Cheetah robot}},
volume = {20},
year = {2015}
}
@article{Atkeson2020,
author = {Atkeson, Chris},
file = {:home/matias/Documents/Mendeley Desktop/Atkeson/2020/Atkeson - 2020 - What advice would I give a starting graduate student interested in robot learning Models ! Model-free ! ... Both !.pdf:pdf},
pages = {1--40},
title = {{What advice would I give a starting graduate student interested in robot learning ? Models !}},
year = {2020}
}
@article{Schwendner2014,
abstract = {Mobile autonomous robots have finally emerged from the confined spaces of structured and controlled indoor environments. To fulfill the promises of ubiquitous robotics in unstructured outdoor environments, robust navigation is a key requirement. The research in the simultaneous localization and mapping (SLAM) community has largely focused on optical sensors to solve this problem, and the fact that the robot is a physical entity has largely been ignored. In this paper, a hierarchical SLAM framework is proposed that takes the interaction of the robot with the environment into account. A sequential Monte Carlo filter is used to generate local map segments with a combination of visual and embodied data associations. Constraints between segments are used to generate globally consistent maps with a focus on suitability for navigation tasks. The proposed method is experimentally verified on two different outdoor robots. The results show that the approach is viable and that the rich modeling of the robot with its environment provides a new modality with the potential for improving existing visual methods and extending the availability of SLAM in domains where visual processing alone is not sufficient. {\textcopyright} 2013 Wiley Periodicals, Inc.},
author = {Schwendner, Jakob and Joyeux, Sylvain and Kirchner, Frank},
doi = {10.1002/rob.21489},
file = {:home/matias/Documents/Mendeley Desktop/Schwendner, Joyeux, Kirchner/2014/Schwendner, Joyeux, Kirchner - 2014 - Using embodied data for localization and mapping.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {263--295},
title = {{Using embodied data for localization and mapping}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/rob.21514/abstract http://doi.wiley.com/10.1002/rob.21489},
volume = {31},
year = {2014}
}
@article{Hausler2020,
abstract = {Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpoint-invariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {2002.03895},
author = {Hausler, Stephen and Milford, Michael},
eprint = {2002.03895},
file = {:home/matias/Documents/Mendeley Desktop/Hausler, Milford/2020/Hausler, Milford - 2020 - Hierarchical Multi-Process Fusion for Visual Place Recognition.pdf:pdf},
isbn = {9781728173955},
pages = {3327--3333},
title = {{Hierarchical Multi-Process Fusion for Visual Place Recognition}},
url = {http://arxiv.org/abs/2002.03895},
year = {2020}
}
@misc{Campbell2020,
abstract = {Blind Perspective-n-Point (PnP) is the problem of estimating the position and orientation of a camera relative to a scene, given 2D image points and 3D scene points, without prior knowledge of the 2D–3D correspondences. Solving for pose and correspondences simultaneously is extremely challenging since the search space is very large. Fortunately it is a coupled problem: the pose can be found easily given the correspondences and vice versa. Existing approaches assume that noisy correspondences are provided, that a good pose prior is available, or that the problem size is small. We instead propose the first fully end-to-end trainable network for solving the blind PnP problem efficiently and globally, that is, without the need for pose priors. We make use of recent results in differentiating optimization problems to incorporate geometric model fitting into an end-to-end learning framework, including Sinkhorn, RANSAC and PnP algorithms. Our proposed approach significantly outperforms other methods on synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {2007.14628},
author = {Campbell, Dylan and Liu, Liu and Gould, Stephen},
booktitle = {arXiv},
doi = {10.1007/978-3-030-58536-5_15},
eprint = {2007.14628},
file = {:home/matias/Documents/Mendeley Desktop/Differentiable, Jul/Unknown/Differentiable, Jul - Unknown - Solving the Blind Perspective-n-Point Problem End-To-End With Robust Differentiable Geometric Optimizati.pdf:pdf},
keywords = {Camera pose estimation,Implicit differentiation,PnP},
month = {jul},
title = {{Solving the blind Perspective-n-Point problem End-to-end with robust differentiable geometric optimization}},
url = {http://arxiv.org/abs/2007.14628},
year = {2020}
}
@inproceedings{Kummerle2011,
abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state-of-the-art approaches for the specific problems. {\textcopyright} 2011 IEEE.},
author = {K{\"{u}}mmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5979949},
isbn = {9781612843865},
issn = {10504729},
pages = {3607--3613},
pmid = {5979949},
title = {{G2o: A general framework for graph optimization}},
year = {2011}
}
@article{Sunderhauf2018,
abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.},
archivePrefix = {arXiv},
arxivId = {1804.06557},
author = {S{\"{u}}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"{u}}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
doi = {10.1177/0278364918770733},
eprint = {1804.06557},
file = {:home/matias/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/2018/S{\"{u}}nderhauf et al. - 2018 - The limits and potentials of deep learning for robotics(2).pdf:pdf},
isbn = {0278364918},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Robotics,deep learning,machine learning,robotic vision},
month = {apr},
number = {4-5},
pages = {405--420},
title = {{The limits and potentials of deep learning for robotics}},
url = {http://journals.sagepub.com/doi/10.1177/0278364918770733},
volume = {37},
year = {2018}
}
@article{Ho2018,
abstract = {In this paper, we propose a mapping approach that constructs a globally deformable virtual occupancy grid map (VOG-map) based on local submaps. Such a representation allows pose graph SLAM systems to correct globally accumulated drift via loop closures while maintaining free space information for the purpose of path planning. We demonstrate use of such a representation for implementing an underwater SLAM system in which the robot actively plans paths to generate accurate 3D scene reconstructions. We evaluate performance on simulated as well as real-world experiments. Our work furthers capabilities of mobile robots actively mapping and exploring unstructured, three dimensional environments.},
author = {Ho, Bing Jui and Sodhi, Paloma and Teixeira, Pedro and Hsiao, Ming and Kusnur, Tushar and Kaess, Michael},
doi = {10.1109/IROS.2018.8594234},
file = {:home/matias/Documents/Mendeley Desktop/Ho et al/2018/Ho et al. - 2018 - Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {2},
pages = {2175--2182},
title = {{Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments}},
year = {2018}
}
@article{Goedeme2005,
abstract = {Vision sensors are attractive for autonomous robots because they are a rich source of environment information. The main challenge in using images for mobile robots is managing this wealth of information. A relatively recent approach is the use of fast wide baseline local features, which we developed and used in the novel approach to sparse visual path following described in this paper. These local features have the great advantage that they can be recognized even if the viewpoint differs significantly. This opens the door to a memory efficient description of a path by descriptors of sparse images. We propose a method for reexecution of these paths by a series of visual homing operations which yield a navigation method with unique properties: it is accurate, robust, fast, and without odometry error build-up. {\textcopyright} 2005 IEEE.},
author = {Goedem{\'{e}}, Toon and Tuytelaars, Tinne and {Van Gool}, Luc and Vanacker, Gerolf and Nuttin, Marnix},
doi = {10.1109/IROS.2005.1545111},
file = {::},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Computer vision,Omnidirectional images,Path following,Robot navigation},
pages = {1003--1008},
title = {{Feature based omnidirectional sparse visual path following}},
year = {2005}
}
@book{Lee2012,
address = {New York, NY},
author = {Lee, John M},
doi = {10.1007/978-1-4419-9982-5},
file = {:home/matias/Documents/Mendeley Desktop/Lee/2011/Lee - 2011 - Introduction to Smooth Manifolds.pdf:pdf},
isbn = {978-1-4419-9981-8},
publisher = {Springer New York},
series = {Graduate Texts in Mathematics},
title = {{Introduction to Smooth Manifolds}},
url = {http://link.springer.com/10.1007/978-1-4419-9982-5},
volume = {218},
year = {2011}
}
@inproceedings{Bloesch2013,
abstract = {This paper presents a state estimation approach for legged robots based on stochastic filtering. The key idea is to extract information from the kinematic constraints given through the intermittent contacts with the ground and to fuse this information with inertial measurements. To this end, we design an unscented Kalman filter based on a consistent formulation of the underlying stochastic model. To increase the robustness of the filter, an outliers rejection methodology is included into the update step. Furthermore, we present the nonlinear observability analysis of the system, where, by considering the special nature of 3D rotations, we obtain a relatively simple form of the corresponding observability matrix. This yields, that, except for the global position and the yaw angle, all states are in general observable. This also holds if only one foot is in contact with the ground. The presented filter is evaluated on a real quadruped robot trotting over an uneven and slippery terrain. {\textcopyright} 2013 IEEE.},
author = {Bloesch, Michael and Gehring, Christian and Fankhauser, Peter and Hutter, Marco and Hoepflinger, Mark A. and Siegwart, Roland},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2013.6697236},
file = {::;::},
isbn = {9781467363587},
issn = {21530858},
pages = {6058--6064},
title = {{State estimation for legged robots on unstable and slippery terrain}},
year = {2013}
}
@article{Oskiper2007a,
abstract = {Over the past decade, tremendous amount of research activity has focused around the problem of localization in GPS denied environments. Challenges with localization are highlighted in human wearable systems where the operator can freely move through both indoors and outdoors. In this paper, we present a robust method that addresses these challenges using a human wearable system with two pairs of backward and forward looking stereo cameras together with an inertial measurement unit (IMU). This algorithm can run in real-time with 15Hz update rate on a dual-core 2GHz laptop PC and it is designed to be a highly accurate local (relative) pose estimation mechanism acting as the front-end to a Simultaneous Localization and Mapping (SLAM) type method capable of global corrections through landmark matching. Extensive tests of our prototype system so far, reveal that without any global landmark matching, we achieve between 0.5% and 1% accuracy in localizing a person over a 500 meter travel indoors and outdoors. To our knowledge, such performance results with a real time system have not been reported before. {\textcopyright} 2007 IEEE.},
author = {Oskiper, Taragay and Zhu, Zhiwei and Samarasekera, Supun and Kumar, Rakesh},
doi = {10.1109/CVPR.2007.383087},
file = {:home/matias/Documents/Mendeley Desktop/Oskiper, Zhu/2007/Oskiper, Zhu - 2007 - Visual Odometry System Using Multiple Stereo Cameras and Inertial Measurement Unit Visual Odometry System Using Mu.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {June},
title = {{Visual odometry system using multiple stereo cameras and inertial measurement unit}},
year = {2007}
}
@inproceedings{Scaramuzza2011,
abstract = {This paper presents a new method to estimate the relative motion of a vehicle from images of a single camera. The biggest problem in visual motion estimation is data association; matched points contain many outliers that must be detected and removed so that the motion can be estimated accurately. A very established method for robust motion estimation in the presence of outliers is the five-point RANSAC algorithm. Five-point RANSAC operates by generating motion hypotheses from randomly-sampled minimal sets of five-point correspondences. These hypotheses are then tested against all data points and the motion hypothesis that after a given number of iterations returns the largest number of inliers is taken as the solution to the problem. A typical drawback of RANSAC is that the number of iterations required to find a suitable solution grows exponentially with the number of outliers, often requiring thousands of iterations for typical data from urban environments. Another problem is that - due to its random nature - sometimes the found solution is not the "best" solution to the motion estimation problem. In this paper, we describe an algorithm for relative motion estimation in the presence of outliers, which does not rely on RANSAC. Contrary to RANSAC, motion hypotheses are not generated from randomly-sampled point correspondences, but from a "proposal distribution" that is built by exploiting the vehicle non-holonomic constraints. We show that not only is the proposed algorithm significantly faster than RANSAC, but that the returned solution may also be better in that it favors the underlying motion model of the vehicle, thus overcoming the typical limitations of RANSAC. Additionally, the proposed algorithm provides the likelihood of the motion estimate, which can be very useful in all those applications where a probability distribution of the position of the vehicle is required (e.g., SLAM). Finally, the performance of the proposed method is compared to that of the standard five-point RANSAC on real images collected from a vehicle moving in a cluttered, urban environment. {\textcopyright} 2011 IEEE.},
author = {Scaramuzza, Davide and Censi, Andrea and Daniilidis, Kostas},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048856},
file = {::},
isbn = {9781612844541},
issn = {2153-0858},
month = {sep},
pages = {4469--4476},
publisher = {IEEE},
title = {{Exploiting motion priors in visual odometry for vehicle-mounted cameras with non-holonomic constraints}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6048856},
year = {2011}
}
@article{Milford2008,
abstract = {This paper describes a biologically inspired approach to vision-only simultaneous localization and mapping (SLAM) on ground-based platforms. The core SLAM system, dubbed RatSLAM, is based on computational models of the rodent hippocampus, and is coupled with a lightweight vision system that provides odometry and appearance information. RatSLAM builds a map in an online manner, driving loop closure and relocalization through sequences of familiar visual scenes. Visual ambiguity is managed by maintaining multiple competing vehicle pose estimates, while cumulative errors in odometry are corrected after loop closure by a map correction algorithm. We demonstrate the mapping performance of the system on a 66 km car journey through a complex suburban road network. Using only a web camera operating at 10 Hz, RatSLAM generates a coherent map of the entire environment at real-time speed, correctly closing more than 51 loops of up to 5 km in length. {\textcopyright} 2008 IEEE.},
author = {Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/TRO.2008.2004520},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Bio-inspired robotics,Monocular vision simultaneous localization and map},
number = {5},
pages = {1038--1053},
title = {{Mapping a suburb with a single camera using a biologically inspired SLAM system}},
volume = {24},
year = {2008}
}
@inproceedings{Li2018,
abstract = {The calibration of the relative pose between cameras with disjoint fields of view mounted on a common rigid object is a prerequisite for many applications. For the extrinsic calibration of such cameras, we develop an improved optimization method applicable to different calibration setups, in which the reprojection error of 3D-2D point correspondences is used as the objective constrained by rigid 3D-3D closed-loop pose transformations AX = YB. We introduce a quality measure for all entries of A and B based on the projection size of known planar calibration patterns on the image. The quality measure serves as an additional weighting in the optimization objective that improves calibration accuracy and increases robustness against noise. We compare our method to state-of-the-art methods both in simulation and real-world experiment, and we demonstrate the improvement in terms of accuracy and robustness. In addition, we show that the spatial distribution of transforms A, B, and their measurement quality also have an influence on the estimation accuracy.},
author = {Li, Zaijuan and Willert, Volker},
booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
doi = {10.1109/ITSC.2018.8569457},
file = {:home/matias/Documents/Mendeley Desktop/Li, Willert/2018/Li, Willert - 2018 - Eye-to-eye Calibration for Cameras with Disjoint Fields of View.pdf:pdf},
isbn = {9781728103235},
keywords = {Calibration and validation of agent-based models f,Camera},
pages = {2631--2638},
title = {{Eye-to-eye Calibration for Cameras with Disjoint Fields of View}},
volume = {2018-Novem},
year = {2018}
}
@article{Zhang2017,
abstract = {We propose an active exposure control method to improve the robustness of visual odometry in HDR (high dynamic range) environments. Our method evaluates the proper exposure time by maximizing a robust gradient-based image quality metric. The optimization is achieved by exploiting the photometric response function of the camera. Our exposure control method is evaluated in different real world environments and outperforms the built-in auto-exposure function of the camera. To validate the benefit of our approach, we adapt a state-of-the-art visual odometry pipeline (SVO) to work with varying exposure time and demonstrate improved performance using our exposure control method in challenging HDR environments.},
author = {Zhang, Zichao and Forster, Christian and Scaramuzza, Davide},
doi = {10.1109/ICRA.2017.7989449},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3894--3901},
title = {{Active exposure control for robust visual odometry in HDR environments}},
year = {2017}
}
@article{Bloesch2016a,
abstract = {The proper handling of 3D orientations is a central element in many optimization problems in engineering. Unfortunately many researchers and engineers struggle with the formulation of such problems and often fall back to suboptimal solutions. The existence of many different conventions further complicates this issue, especially when interfacing multiple differing implementations. This document discusses an alternative approach which makes use of a more abstract notion of 3D orientations. The relative orientation between two coordinate systems is primarily identified by the coordinate mapping it induces. This is combined with the standard exponential map in order to introduce representation-independent and minimal differentials, which are very convenient in optimization based methods.},
archivePrefix = {arXiv},
arxivId = {1606.05285},
author = {Bloesch, Michael and Sommer, Hannes and Laidlow, Tristan and Burri, Michael and Nuetzi, Gabriel and Fankhauser, P{\'{e}}ter and Bellicoso, Dario and Gehring, Christian and Leutenegger, Stefan and Hutter, Marco and Siegwart, Roland},
doi = {10.3929/ethz-a-010666114},
eprint = {1606.05285},
journal = {arXiv},
number = {1},
title = {{A Primer on the Differential Calculus of 3D Orientations}},
url = {http://arxiv.org/abs/1606.05285},
year = {2016}
}
@article{Hartley2018c,
abstract = {The factor graph framework is a convenient modeling technique for robotic state estimation where states are represented as nodes, and measurements are modeled as factors. When designing a sensor fusion framework for legged robots, one often has access to visual, inertial, joint encoder, and contact sensors. While visual-inertial odometry has been studied extensively in this framework, the addition of a preintegrated contact factor for legged robots has been only recently proposed. This allowed for integration of encoder and contact measurements into existing factor graphs, however, new nodes had to be added to the graph every time contact was made or broken. In this work, to cope with the problem of switching contact frames, we propose a hybrid contact preintegration theory that allows contact information to be integrated through an arbitrary number of contact switches. The proposed hybrid modeling approach reduces the number of required variables in the nonlinear optimization problem by only requiring new states to be added alongside camera or selected keyframes. This method is evaluated using real experimental data collected from a Cassie-series robot where the trajectory of the robot produced by a motion capture system is used as a proxy for ground truth. The evaluation shows that inclusion of the proposed preintegrated hybrid contact factor alongside visual-inertial navigation systems improves estimation accuracy as well as robustness to vision failure, while its generalization makes it more accessible for legged platforms.},
archivePrefix = {arXiv},
arxivId = {1803.07531},
author = {Hartley, Ross and Jadidi, Maani Ghaffari and Gan, Lu and Huang, Jiunn Kai and Grizzle, Jessy W. and Eustice, Ryan M.},
doi = {10.1109/IROS.2018.8593801},
eprint = {1803.07531},
file = {:home/matias/Documents/Mendeley Desktop/Hartley et al/2018/Hartley et al. - 2018 - Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation Using Factor Graphs.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3783--3790},
title = {{Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation Using Factor Graphs}},
year = {2018}
}
@article{Houben2016a,
abstract = {Visual SLAM is an area of vivid research and bears countless applications for moving robots. In particular, micro aerial vehicles benefit from visual sensors due to their low weight. Their motion is, however, often faster and more complex than that of ground-based robots which is why systems with multiple cameras are currently evaluated and deployed. This, in turn, drives the computational demand for visual SLAM algorithms. We present an extension of the recently introduced monocular ORB-SLAM for multiple cameras alongside an inertial measurement unit (IMU). Our main contributions are: Embedding the multi-camera setup into the underlying graph SLAM approach that defines the upcoming sparse optimization problems on several adjusted subgraphs, integration of an IMU filter that supports visual tracking, and enhancements of the original algorithm in local map estimation and keyframe creation. The SLAM system is evaluated on a public stereo SLAM dataset for flying robots and on a new dataset with three mounted cameras. The main advantages of the proposed method are its restricted computational load, high positional accuracy, and low number of parameters.},
author = {Houben, Sebastian and Quenzel, Jan and Krombach, Nicola and Behnke, Sven},
doi = {10.1109/IROS.2016.7759261},
file = {:home/matias/Documents/Mendeley Desktop/Houben et al/2016/Houben et al. - 2016 - Efficient multi-camera visual-inertial SLAM for micro aerial vehicles.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1616--1622},
publisher = {IEEE},
title = {{Efficient multi-camera visual-inertial SLAM for micro aerial vehicles}},
volume = {2016-Novem},
year = {2016}
}
@article{Mazuran2018,
abstract = {Mobile robot localization is a mature field that over the years has demonstrated its effectiveness and robust-ness. The majority of the approaches, however, rely on a globally consistent map, and localize on it in an absolute coordinate frame. This global consistency cannot be guaranteed when the map is estimated by the robot itself, and an error in the map will likely result in the failure of the localization subsystem. In this paper we introduce a novel paradigm for localization, namely relative topometric localization, by which we forgo the need for a globally consistent map. We adopt a graph-based representation of the environment, and estimate both the topological location on the graph and the relative metrical position with respect to it. We extensively evaluated our approach and tested it against Monte Carlo localization on both simulated and real data. The results show significant improvements in scenarios where there is no globally consistent map.},
author = {Mazuran, Mladen and Boniardi, Federico and Burgard, Wolfram and Tipaldi, Gian Diego},
doi = {10.1007/978-3-319-60916-4_25},
file = {:home/matias/Documents/Mendeley Desktop/Mazuran et al/2018/Mazuran et al. - 2018 - Relative Topometric Localization in Globally Inconsistent Maps(2).pdf:pdf},
pages = {435--451},
title = {{Relative Topometric Localization in Globally Inconsistent Maps}},
year = {2018}
}
@article{Mactavish2017,
abstract = {Our work builds upon Visual Teach & Repeat 2 (VT&R2): a vision-in-the-loop autonomous navigation system that enables the rapid construction of route networks, safely built through operator-controlled driving. Added routes can be followed autonomously using visual localization. To enable long-term operation that is robust to appearance change, its Multi-Experience Localization (MEL) leverages many previously driven experiences when localizing to the manually taught network. While this multi-experience method is effective across appearance change, the computation becomes intractable as the number of experiences grows into the tens and hundreds. This paper introduces an algorithm that prioritizes experiences most relevant to live operation, limiting the number of experiences required for localization. The proposed algorithm uses a visual Bag-of-Words description of the live view to select relevant experiences based on what the vehicle is seeing right now, without having to factor in all possible environmental influences on scene appearance. This system runs in the loop, in real time, does not require bootstrapping, can be applied to any pointfeature MEL paradigm, and eliminates the need for visual training using an online, local visual vocabulary. By picking a subset of visually similar experiences to the live view, we demonstrate safe, vision-in-the-loop route following over a 31 hour period, despite appearance as different as night and day.},
author = {Mactavish, Kirk and Paton, Michael and Barfoot, Timothy D.},
doi = {10.1109/ICRA.2017.7989238},
file = {:home/matias/Documents/Mendeley Desktop/Mactavish, Paton, Barfoot/2017/Mactavish, Paton, Barfoot - 2017 - Visual triage A bag-of-words experience selector for long-term visual route following.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2065--2072},
publisher = {IEEE},
title = {{Visual triage: A bag-of-words experience selector for long-term visual route following}},
year = {2017}
}
@article{Roweis1999,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chakraborty, Apurba and Ghosh, Saptarsi and Mukhopadhyay, Partha and Dinara, Syed Mukulika and Bag, Ankush and Mahata, Mihir K and Kumar, Rahul and Das, Subhashis and Sanjay, Jana and Majumdar, Shubhankar and Biswas, Dhrubes},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {0717-6163},
journal = {MRS Proceedings},
keywords = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics & numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics & numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,youth},
month = {feb},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{Trapping effect analysis of AlGaN/InGaN/GaN Heterostructure by conductance frequency measurement}},
url = {http://www.americanbanker.com/issues/179_124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161%5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991%5Cnhttp://www.scielo},
volume = {XXXIII},
year = {2014}
}
@article{Rosinol2019,
abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
archivePrefix = {arXiv},
arxivId = {1910.02490},
author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
eprint = {1910.02490},
file = {:home/matias/Documents/Mendeley Desktop/Rosinol et al/2019/Rosinol et al. - 2019 - Kimera an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping.pdf:pdf},
title = {{Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping}},
url = {http://arxiv.org/abs/1910.02490},
year = {2019}
}
@article{Rotella2017,
abstract = {This work presents a method for contact state estimation using fuzzy clustering to learn contact probability for full, six-dimensional humanoid contacts. The data required for training is solely from proprioceptive sensors - endeffector contact wrench sensors and inertial measurement units (IMUs) - and the method is completely unsupervised. The resulting cluster means are used to efficiently compute the probability of contact in each of the six endeffector degrees of freedom (DoFs) independently. This clustering-based contact probability estimator is validated in a kinematics-based base state estimator in a simulation environment with realistic added sensor noise for locomotion over rough, low-friction terrain on which the robot is subject to foot slip and rotation. The proposed base state estimator which utilizes these six DoF contact probability estimates is shown to perform considerably better than that which determines kinematic contact constraints purely based on measured normal force.},
archivePrefix = {arXiv},
arxivId = {1709.07472},
author = {Rotella, Nicholas and Schaal, Stefan and Righetti, Ludovic},
doi = {10.1109/ICRA.2018.8462864},
eprint = {1709.07472},
file = {::},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {411--417},
title = {{Unsupervised Contact Learning for Humanoid Estimation and Control}},
url = {http://arxiv.org/abs/1709.07472},
year = {2018}
}
@article{Lobo2004,
abstract = {Inertial sensors attached to a camera can provide valuable data about camera pose and movement. In biological vision systems, inertial cues provided by the vestibular system are fused with vision at an early processing stage. In this article we set a framework for the combination of these two sensing modalities. Cameras can be seen as ray direction measuring devices, and in the case of stereo vision, depth along the ray can also be computed. The ego-motion can be sensed by the inertial sensors, but there are limitations determined by the sensor noise level. Keeping track of the vertical direction is required, so that gravity acceleration can be compensated for, and provides a valuable spatial reference. Results are shown of stereo depth map alignment using the vertical reference. The depth map points are mapped to a vertically aligned world frame of reference. In order to detect the ground plane, a histogram is performed for the different heights. Taking the ground plane as a reference plane for the acquired maps, the fusion of multiple maps reduces to a 2D translation and rotation problem. The dynamic inertial cues can be used as a first approximation for this transformation, allowing a fast depth map registration method. They also provide an image independent location of the image focus of expansion and center of rotation useful during visual based navigation tasks.},
author = {Lobo, Jorge and Dias, Jorge},
doi = {10.1002/rob.10122},
file = {::},
issn = {07412223},
journal = {Journal of Robotic Systems},
month = {jan},
number = {1},
pages = {3--12},
title = {{Inertial sensed ego-motion for 3D vision}},
url = {http://doi.wiley.com/10.1002/rob.10122},
volume = {21},
year = {2004}
}
@article{Martinez-Cantin2013,
abstract = {The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for "expensive functions". The software is free and it can be used in many operating systems and computer languages.},
archivePrefix = {arXiv},
arxivId = {1309.0671},
author = {Martinez-Cantin, Ruben},
eprint = {1309.0671},
file = {::},
number = {9},
title = {{BayesOpt: A Library for Bayesian optimization with Robotics Applications}},
url = {http://arxiv.org/abs/1309.0671},
year = {2013}
}
@article{Lee2011,
abstract = {Handling motion blur is one of important issues in visual SLAM. For a fast-moving camera, motion blur is an unavoidable effect and it can degrade the results of localization and reconstruction severely. In this paper, we present a unified algorithm to handle motion blur for visual SLAM, including the blur-robust data association method and the fast deblurring method. In our framework, camera motion and 3-D point structures are reconstructed by SLAM, and the information from SLAM makes the estimation of motion blur quite easy and effective. Reversely, estimating motion blur enables robust data association and drift-free localization of SLAM with blurred images. The blurred images are recovered by fast deconvolution using SLAM data, and more features are extracted and registered to the map so that the SLAM procedure can be continued even with the blurred images. In this way, visual SLAM and deblurring are solved simultaneously, and improve each other's results significantly. {\textcopyright} 2011 IEEE.},
author = {Lee, Hee Seok and Kwon, Junghyun and Lee, Kyoung Mu},
doi = {10.1109/ICCV.2011.6126370},
file = {::},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1203--1210},
title = {{Simultaneous localization, mapping and deblurring}},
year = {2011}
}
@article{Smith2003,
abstract = {This paper addresses the problem of computing the trajectory of a camera from sparse positional measurements that have been obtained from visual localisation, and dense differential measurements from odometry or inertial sensors. A fast method is presented for fusing these two sources of information to obtain the maximum a posteriori estimate of the trajectory. A formalism is introduced for representing probability density functions over Euclidean transformations, and it is shown how these density functions can be propagated along the data sequence and how multiple estimates of a transformation can be combined. A three-pass algorithm is described which makes use of these results to yield the trajectory of the camera. Simulation results are presented which are validated against a physical analogue of the vision problem, and results are then shown from sequences of approximately 1,800 frames captured from a video camera mounted on a go-kart. Several of these frames are processed using computer vision to obtain estimates of the position of the go-kart. The algorithm fuses these estimates with odometry from the entire sequence in I50 mS to obtain the trajectory of the kart.},
author = {Smith, Paul and Drummond, Tom and Roussopoulos, Kimon},
doi = {10.1109/iccv.2003.1238637},
isbn = {0-7695-1950-4},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {Iccv},
pages = {1275--1282},
title = {{Computing MAP trajectories by representing, propagating and combining PDFs over groups}},
volume = {2},
year = {2003}
}
@inproceedings{Wisth2020,
abstract = {In this paper, we present a novel factor graph formulation to estimate the pose and velocity of a quadruped robot on slippery and deformable terrain. The factor graph introduces a preintegrated velocity factor that incorporates velocity inputs from leg odometry and also estimates related biases. From our experimentation we have seen that it is difficult to model uncertainties at the contact point such as slip or deforming terrain, as well as leg flexibility. To accommodate for these effects and to minimize leg odometry drift, we extend the robot's state vector with a bias term for this preintegrated velocity factor. The bias term can be accurately estimated thanks to the tight fusion of the preintegrated velocity factor with stereo vision and IMU factors, without which it would be unobservable. The system has been validated on several scenarios that involve dynamic motions of the ANYmal robot on loose rocks, slopes and muddy ground. We demonstrate a 26% improvement of relative pose error compared to our previous work and 52% compared to a state-of-the-art proprioceptive state estimator.},
archivePrefix = {arXiv},
arxivId = {1910.09875},
author = {Wisth, David and Camurri, Marco and Fallon, Maurice},
booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA40945.2020.9197214},
eprint = {1910.09875},
file = {:home/matias/Documents/Mendeley Desktop/Wisth, Camurri, Fallon/Unknown/Wisth, Camurri, Fallon - Unknown - Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry.pdf:pdf},
isbn = {978-1-7281-7395-5},
month = {may},
pages = {392--398},
publisher = {IEEE},
title = {{Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry}},
url = {http://arxiv.org/abs/1910.09875 http://dx.doi.org/10.1109/ICRA40945.2020.9197214 https://ieeexplore.ieee.org/document/9197214/},
year = {2020}
}
@article{Toussaint2009,
abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.},
author = {Toussaint, Marc},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint/2009/Toussaint - 2009 - Robot trajectory optimization using approximate inference.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th International Conference On Machine Learning, ICML 2009},
pages = {1049--1056},
title = {{Robot trajectory optimization using approximate inference}},
year = {2009}
}
@article{Churchill2013,
abstract = {This paper is about long-term navigation in environments whose appearance changes over time, suddenly or gradually. We describe, implement and validate an approach which allows us to incrementally learn a model whose complexity varies naturally in accordance with variation of scene appearance. It allows us to leverage the state of the art in pose estimation to build over many runs, a world model of sufficient richness to allow simple localisation despite a large variation in conditions. As our robot repeatedly traverses its workspace, it accumulates distinct visual experiences that in concert, implicitly represent the scene variation: each experience captures a visual mode. When operating in a previously visited area, we continually try to localise in these previous experiences while simultaneously running an independent vision-based pose estimation system. Failure to localise in a sufficient number of prior experiences indicates an insufficient model of the workspace and instigates the laying down of the live image sequence as a new distinct experience. In this way, over time we can capture the typical time-varying appearance of an environment and the number of experiences required tends to a constant. Although we focus on vision as a primary sensor throughout, the ideas we present here are equally applicable to other sensor modalities. We demonstrate our approach working on a road vehicle operating over a 3-month period at different times of day, in different weather and lighting conditions. We present extensive results analysing different aspects of the system and approach, in total processing over 136,000 frames captured from 37 km of driving. {\textcopyright} The Author(s) 2013.},
author = {Churchill, Winston and Newman, Paul},
doi = {10.1177/0278364913499193},
file = {:home/matias/Documents/Mendeley Desktop/Churchill, Newman/2013/Churchill, Newman - 2013 - Experience-based navigation for long-term localisation.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Localisation,SLAM,field and service robotics,field robots,mapping,mobile and distributed robotics},
number = {14},
pages = {1645--1661},
title = {{Experience-based navigation for long-term localisation}},
volume = {32},
year = {2013}
}
@article{Todorov2002,
abstract = {A central problem in motor control is understanding how the many biomechanical degrees of freedom are coordinated to achieve a common goal. An especially puzzling aspect of coordination is that behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Existing theoretical frameworks emphasize either goal achievement or the richness of motor variability, but fail to reconcile the two. Here we propose an alternative theory based on stochastic optimal feedback control. We show that the optimal strategy in the face of uncertainty is to allow variability in redundant (task-irrelevant) dimensions. This strategy does not enforce a desired trajectory, but uses feedback more intelligently, correcting only those deviations that interfere with task goals. From this framework, task-constrained variability, goal-directed corrections, motor synergies, controlled parameters, simplifying rules and discrete coordination modes emerge naturally. We present experimental results from a range of motor tasks to support this theory.},
author = {Todorov, Emanuel and Jordan, Michael I.},
doi = {10.1038/nn963},
file = {::},
isbn = {1097-6256 (Print)\r1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {11},
pages = {1226--1235},
pmid = {12404008},
title = {{Optimal feedback control as a theory of motor coordination}},
volume = {5},
year = {2002}
}
@article{Zong2017,
abstract = {Monocular ORB-SLAM has been proved to be one of the best open-source SLAM method. However, it is still unsatisfying especially in low illumination indoor environment, which is caused by scale recovery and wrong feature matching. In this paper, we proposed a vehicle model based monocular ORB- SLAM method supplemented by April-Tag to improve the per- formance of original algorithm. This approach is practical when autonomous driving in low-light and less-feature environment like garages and tunnels. We achieve this by proposing a vehicle model based initialization method fusing April-Tag measurement to recover scale. During tracking procedure, the outliers ORB feature points will be removed by checking reprojection error calculated from April-Tag. In addition, considering vehicle model can only obtain 2D motion, the vertical transition is estimated from camera model. Afterwards, a local Bundle Adjustment(BA) is applied to optimize camera pose both from frame to frame and frame to keyframe which will reduce accumulative error of the vehicle model. Finally, a convincing result is obtained from the testing drive in a garage.},
author = {Zong, Wenhao and Chen, Longquan and Zhang, Changzhu and Wang, Zhuping and Cheny, Qijun},
doi = {10.1109/SMC.2017.8122816},
file = {::},
isbn = {9781538616451},
journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017},
pages = {1441--1446},
title = {{Vehicle model based visual-tag monocular ORB-SLAM}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Weiss2013,
abstract = {In this paper, we describe a novel approach in fusing optical flow with inertial cues (3D acceleration and 3D angular velocities) in order to navigate a Micro Aerial Vehicle (MAV) drift free in 4DoF and metric velocity. Our approach only requires two consecutive images with a minimum of three feature matches. It does not require any (point) map nor any type of feature history. Thus it is an inherently failsafe approach that is immune to map and feature-track failures. With these minimal requirements we show in real experiments that the system is able to navigate drift free in all angles including yaw, in one metric position axis, and in 3D metric velocity. Furthermore, it is a power-on-and-go system able to online self-calibrate the inertial biases, the visual scale and the full 6DoF extrinsic transformation parameters between camera and IMU. {\textcopyright} 2013 IEEE.},
author = {Weiss, Stephan and Brockers, Roland and Matthies, Larry},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2013.6696955},
isbn = {9781467363587},
issn = {21530858},
month = {nov},
pages = {4180--4186},
publisher = {IEEE},
title = {{4DoF drift free navigation using inertial cues and optical flow}},
url = {http://ieeexplore.ieee.org/document/6696955/},
year = {2013}
}
@article{Babu2014,
abstract = {—A tight coupling between perception and manipu-lation is required for dynamic robots to react in a timely and appropriate manner to changes in the world. In conventional robotics, perception transforms visual information into internal models which are used by planning algorithms to generate trajec-tories for motion. Under this paradigm, it is possible for a plan to become stale if the robot or environment changes configuration before the robot can replan. Perception and actuation are only loosely coupled through planning; there is no rapid feedback or interplay between them. For a statically stable robot in a slowly changing environment, this is an appropriate strategy for manipulating the world. A tightly coupled system, by contrast, connects perception directly to actuation, allowing for rapid feedback. This tight coupling is important for a dynamically unstable robot which engages in active manipulation. In such robots, planning does not fall between perception and manipula-tion; rather planning creates the connection between perception and manipulation. We show that Simultaneous Localization and Mapping (SLAM) can be used as a tool to perform the tight coupling for a humanoid robot with numerous proprioceptive and exteroceptive sensors. Three different approaches to generate a motion plan for grabbing a piece of debris is evaluated using for Atlas humanoid robot. Results indicate higher success rate and accuracy for motion plans that implement tight coupling between perception and manipulation using SLAM.},
author = {Babu, Benzun Pious Wisely and Bove, Christopher and Gennert, Michael},
file = {::},
journal = {2014 IROS Workshop},
title = {{Tight Coupling between Manipulation and Perception using SLAM}},
url = {http://users.wpi.edu/$\sim$bpwiselybabu/iros_workshop_bpwiselybabu.pdf},
year = {2014}
}
@article{Peretroukhin2017,
abstract = {We present a novel method to fuse the power of deep networks with the computational efficiency of geometric and probabilistic localization algorithms. In contrast to other methods that completely replace a classical visual estimator with a deep network, we propose an approach that uses a convolutional neural network to learn difficult-to-model corrections to the estimator from ground-truth training data. To this end, we derive a novel loss function for learning SE(3) corrections based on a matrix Lie groups approach, with a natural formulation for balancing translation and rotation errors. We use this loss to train a deep pose correction network (DPC-Net) that predicts corrections for a particular estimator, sensor and environment. Using the KITTI odometry dataset, we demonstrate significant improvements to the accuracy of a computationally-efficient sparse stereo visual odometry pipeline, that render it as accurate as a modern computationally-intensive dense estimator. Further, we show how DPC-Net can be used to mitigate the effect of poorly calibrated lens distortion parameters.},
archivePrefix = {arXiv},
arxivId = {1709.03128},
author = {Peretroukhin, Valentin and Kelly, Jonathan},
doi = {10.1109/LRA.2017.2778765},
eprint = {1709.03128},
file = {::},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep Learning in Robotics and Automation,localization},
number = {3},
pages = {2424--2431},
title = {{DPC-Net: Deep Pose Correction for Visual Localization}},
url = {http://arxiv.org/abs/1709.03128%0Ahttp://dx.doi.org/10.1109/LRA.2017.2778765},
volume = {3},
year = {2018}
}
@article{Posner,
abstract = {Recent progress in AI technology has been breath-taking. However, many of the advances have played to the strengths of virtual environments: infinite training data is available , risk-free exploration is possible, acting is essentially free. In contrast, we require our robots to robustly operate in real-time, to learn from a limited amount of data, take mission-and sometimes safety-critical decisions and increasingly even display a knack for creative problem solving. To bridge this gap, here we offer an alternative view of recent advances in AI. In particular, we posit that, for the first time, roboticists can draw meaningful functional parallels between AI technology and components identified in the cognitive sciences as pivotal to robust operation in the real world: Dual Process Theory and metacognition. Revisiting recent work in robot learning, we establish the building blocks of a Dual Process Theory for robots and highlight potentially fruitful future research directions towards delivering robust, versatile and safe embodied AI.},
author = {Posner, Ingmar},
file = {:home/matias/Documents/Mendeley Desktop/Posner/Unknown/Posner - Unknown - Robots Thinking Fast and Slow On Dual Process Theory and Metacognition in Embodied AI.pdf:pdf},
title = {{Robots Thinking Fast and Slow: On Dual Process Theory and Metacognition in Embodied AI}}
}
@article{Kappler2018,
abstract = {We address the challenging problem of robotic grasping and manipulation in the presence of uncertainty. This uncertainty is due to noisy sensing, inaccurate models, and hard-to-predict environment dynamics. We quantify the importance of continuous, real-time perception and its tight integration with reactive motion generation methods in dynamic manipulation scenarios. We compare three different systems that are instantiations of the most common architectures in the field: 1) a traditional sense-plan-act approach that is still widely used; 2) a myopic controller that only reacts to local environment dynamics; and 3) a reactive planner that integrates feedback control and motion optimization. All architectures rely on the same components for real-time perception and reactive motion generation to allow a quantitative evaluation. We extensively evaluate the systems on a real robotic platform in four scenarios that exhibit either a challenging workspace geometry or a dynamic environment. We quantify the robustness and accuracy that is due to integrating real-time feedback at different time scales in a reactive motion generation system. We also report on the lessons learned for system building.},
archivePrefix = {arXiv},
arxivId = {1703.03512},
author = {Kappler, Daniel and Meier, Franziska and Issac, Jan and Mainprice, Jim and Cifuentes, Cristina Garcia and Wuthrich, Manuel and Berenz, Vincent and Schaal, Stefan and Ratliff, Nathan and Bohg, Jeannette},
doi = {10.1109/LRA.2018.2795645},
eprint = {1703.03512},
file = {:home/matias/Documents/Mendeley Desktop/Kappler et al/2018/Kappler et al. - 2018 - Real-time perception meets reactive motion generation.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Reactive and sensor-based planning,perception for grasping and manipulation,sensor-based control},
number = {3},
pages = {1864--1871},
publisher = {IEEE},
title = {{Real-time perception meets reactive motion generation}},
volume = {3},
year = {2018}
}
@article{Chancan2020,
abstract = {State-of-The-Art algorithms for visual place recognition, and related visual navigation systems, can be broadly split into two categories: computer-science-oriented models including deep learning or image retrieval-based techniques with minimal biological plausibility, and neuroscience-oriented dynamical networks that model temporal properties underlying spatial navigation in the brain. In this letter, we propose a new compact and high-performing place recognition model that bridges this divide for the first time. Our approach comprises two key neural models of these categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by brain architectures of fruit flies, Drosophila melanogaster, and (2) a one-dimensional continuous attractor neural network (CANN). The resulting FlyNet+CANN network incorporates the compact pattern recognition capabilities of our FlyNet model with the powerful temporal filtering capabilities of an equally compact CANN, replicating entirely in a hybrid neural implementation the functionality that yields high performance in algorithmic localization approaches like SeqSLAM. We evaluate our model, and compare it to three state-of-The-Art methods, on two benchmark real-world datasets with small viewpoint variations and extreme environmental changes-achieving 87% AUC results under day to night transitions compared to 60% for Multi-Process Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times faster, respectively.},
archivePrefix = {arXiv},
arxivId = {1910.06840},
author = {Chancan, Marvin and Hernandez-Nunez, Luis and Narendra, Ajay and Barron, Andrew B. and Milford, Michael},
doi = {10.1109/LRA.2020.2967324},
eprint = {1910.06840},
file = {:home/matias/Documents/Mendeley Desktop/Chancan et al/2020/Chancan et al. - 2020 - A Hybrid compact neural architecture for visual place recognition.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Biomimetics,localization,visual-based navigation},
number = {2},
pages = {993--1000},
title = {{A Hybrid compact neural architecture for visual place recognition}},
volume = {5},
year = {2020}
}
@article{Kaneko2004,
abstract = {A development of humanoid robot HRP-2 is presented in this paper. HRP-2 is a humanoid robotics platform, which we developed in phase two of HRP. HRP was a humanoid robotics project, which had run by the Ministry of Economy, Trade and Industry (METI) of Japan from 1998FY to 2002FY for five years. The ability of the biped locomotion of HRP-2 is improved so that HRP-2 can cope with uneven surface, can walk at two third level of human speed, and can walk on a narrow path. The ability of whole body motion of HRP-2 is also improved so that HRP-2 can get up by a humanoid robot's own self if HRP-2 tips over safely. In this paper, the appearance design, the mechanisms, the electrical systems, specifications, and features upgraded from its prototype are also introduced.},
author = {Kaneko, Kenji and Kanehiro, Fumio and Kajita, Shuuji and Hirukaka, Hirohisa and Kawasaki, Toshikazu and Hirata, Masaru and Akachi, Kazuhiko and Isozumi, Takakatsu},
doi = {10.1109/robot.2004.1307969},
file = {::},
isbn = {0-7803-8232-3},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {2},
pages = {1083--1090},
title = {{Humanoid robot HRP-2}},
url = {http://ieeexplore.ieee.org/document/1307969/},
volume = {2004},
year = {2004}
}
@article{Crassidis2006,
abstract = {A sigma-point Kalman filter is derived for integrating GPS measurements with inertial measurements from gyros and accelerometers to determine both the position and the attitude of a moving vehicle. Sigma-point filters use a carefully selected set of sample points to more accurately map the probability distribution than the linearization of the standard extended Kalman filter (EKF), leading to faster convergence from inaccurate initial conditions in position/attitude estimation problems. The filter formulation is based on standard inertial navigation equations. The global attitude parameterization is given by a quaternion, while a generalized three-dimensional attitude representation is used to define the local attitude error. A multiplicative quaternion-error approach is used to guarantee that quaternion normalization is maintained in the filter. Simulation and experimental results are shown to compare the performance of the sigma-point filter with a standard EKF approach. {\textcopyright} 2006 IEEE.},
author = {Crassidis, John L.},
doi = {10.1109/TAES.2006.1642588},
file = {::},
isbn = {978-1-62410-056-7},
issn = {00189251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {2},
pages = {750--756},
title = {{Sigma-point Kalman filtering for integrated GPS and inertial navigation}},
volume = {42},
year = {2006}
}
@article{Kitano1998,
abstract = {RoboCup is an attempt to foster intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined. The First Robot World Cup Soccer Games and Conferences (RoboCup-97) was held during IJCAI-97, Nagoya, with over 40 teams participating from throughout the world. RoboCup soccer is a task for a team of fast-moving robots in a dynamic, noisy environment. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multi-agent collaboration, strategy acquisition, real-time reasoning, robotics, and sensor-fusion. This article describes technical challenges involved in RoboCup, its official rules, a report of RoboCup-97, and future perspectives.},
author = {Kitano, Hiroaki and Asada, Minoru and Noda, Itsuki and Matsubara, Hitoshi},
doi = {10.1109/100.728221},
isbn = {1070-9932 VO - 5},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Landmark project,RoboCup,Robot World Cup,Robot soccer},
number = {3},
pages = {30--36},
title = {{RoboCup: Robot World Cup}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=728221},
volume = {5},
year = {1998}
}
@article{Younes2016,
abstract = {Extensive research in the field of monocular SLAM for the past fifteen years has yielded workable systems that found their way into various applications in robotics and augmented reality. Although filter-based monocular SLAM systems were common at some time, the more efficient keyframe-based solutions are becoming the de facto methodology for building a monocular SLAM system. The objective of this paper is threefold: first, the paper serves as a guideline for people seeking to design their own monocular SLAM according to specific environmental constraints. Second, it presents a survey that covers the various keyframe-based monocular SLAM systems in the literature, detailing the components of their implementation, and critically assessing the specific strategies made in each proposed solution. Third, the paper provides insight into the direction of future research in this field, to address the major limitations still facing monocular SLAM; namely, in the issues of illumination changes, initialization, highly dynamic motion, poorly textured scenes, repetitive textures, map maintenance, and failure recovery.},
archivePrefix = {arXiv},
arxivId = {1607.00470},
author = {Younes, Georges and Asmar, Daniel and Shammas, Elie and Zelek, John},
doi = {10.1016/j.robot.2017.09.010},
eprint = {1607.00470},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Keyframe based,Monocular,Visual SLAM},
number = {2012},
pages = {67--88},
title = {{Keyframe-based monocular SLAM: design, survey, and future directions}},
volume = {98},
year = {2017}
}
@article{Scaramuzza2009,
abstract = {In structure-from-motion with a single camera it is well known that the scene can be only recovered up to a scale. In order to compute the absolute scale, one needs to know the baseline of the camera motion or the dimension of at least one element in the scene. In this paper, we show that there exists a class of structure-from-motion problems where it is possible to compute the absolute scale completely automatically without using this knowledge, that is, when the camera is mounted on wheeled vehicles (e.g. cars, bikes, or mobile robots). The construction of these vehicles puts interesting constraints on the camera motion, which are known as "nonholonomic constraints". The interesting case is when the camera has an offset to the vehicle's center of motion. We show that by just knowing this offset, the absolute scale can be computed with a good accuracy when the vehicle turns. We give a mathematical derivation and provide experimental results on both simulated and real data over a large image dataset collected during a 3 Km path. To our knowledge this is the first time nonholonomic constraints of wheeled vehicles are used to estimate the absolute scale. We believe that the proposed method can be useful in those research areas involving visual odometry and mapping with vehicle mounted cameras. {\textcopyright}2009 IEEE.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich and Pollefeys, Marc and Siegwart, Roland},
doi = {10.1109/ICCV.2009.5459294},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {absolute scale,camera calibration,computer vision,omnidirectional vision,ransac,slam,structure from motion,visual odometry},
pages = {1413--1419},
title = {{Absolute scale in structure from motion from a single vehicle mounted camera by exploiting nonholonomic constraints}},
year = {2009}
}
@article{Campusano,
author = {Campusano, Miguel},
keywords = {live programming,m,robot,s},
number = {Dcc},
title = {{Live Robot Programming: Ayudando al Desarrollo de Robots}}
}
@article{Attilio1998,
abstract = {Este trabajo tiene por objetivo el contribuir al mejoramiento de las t{\'{e}}cnicas basadas en estereoscop{\'{i}}a para la reconstrucci{\'{o}}n tridimensional de escenas reales. En la estereoscop{\'{i}}a tradicional, las escenas son reconstruidas tridimensionalmente a partir de dos o m{\'{a}}s im{\'{a}}genes tomadas desde perspectivas distintas. El proceso de reconstrucci{\'{o}}n tridimensional se basa en el an{\'{a}}lisis del conjunto de im{\'{a}}genes para encontrar la posici{\'{o}}n de la proyecci{\'{o}}n de un punto de la escena sobre cada plano {\'{o}}ptico (plano sobre el cual se proyecta la imagen). Una vez determinadas las posiciones relativas de los puntos correspondientes (disparidades), mediante el proceso conocido como b{\'{u}}squeda de correspondencias (matching) y dada cierta configuraci{\'{o}}n geom{\'{e}}trica del sistema de c{\'{a}}maras, es posible determinar la posici{\'{o}}n del punto en el espacio tridimensional mediante triangulaci{\'{o}}n. El aspecto de mayor complejidad en el proceso de reconstrucci{\'{o}}n es el de b{\'{u}}squeda de correspondencias debido a que est{\'{a}} sujeto a varias fuentes de error y ambig{\"{u}}edades, que a su vez originan estimaciones err{\'{o}}neas de la posici{\'{o}}n del punto en el espacio. Por est{\'{a}} raz{\'{o}}n es necesario desarrollar m{\'{e}}todos que permitan reducir la ocurrencia de errores y un aumento de la exactitud y la confiabilidad de las mediciones de profundidad. Se proponen dos aportes novedosos para contribuir al mejoramiento de los algoritmos estereosc{\'{o}}picos existentes: • La incorporaci{\'{o}}n de los factores de confiabilidad de las mediciones, obtenidos a partir del an{\'{a}}lisis de las curvas de correlaci{\'{o}}n. • La introducci{\'{o}}n de un m{\'{e}}todo autom{\'{a}}tico para el ajuste del rango de b{\'{u}}squeda de disparidades a partir de una estimaci{\'{o}}n inicial de las disparidades de menor precisi{\'{o}}n, pero mayor confiabilidad. Tambi{\'{e}}n se analiza el uso de secuencias de pares de im{\'{a}}genes tomadas desde distintas posiciones como m{\'{e}}todo para reducir el error de estimaci{\'{o}}n de la profundidad. El perfeccionar las t{\'{e}}cnicas de reconstrucci{\'{o}}n tridimensional es esencial para cualquier aplicaci{\'{o}}n futura que pretenda confiar su automatismo a un sistema de visi{\'{o}}n artificial.},
author = {Attilio, Miguel and Torriti, Torres},
keywords = {3D Reconstruction,3D Scene Analysis,Adaptive Disparity Range,Computer Vision,Machine Vision,Robot Vision,Stereo Matching,Stereo Vision,Stereoscopic Sequence Analysis},
pages = {225},
title = {{Reconstruccion confiable de superficies usando rango de disparidad adaptivo}},
year = {1998}
}
@article{Oliensis2005,
abstract = {We analyze the least-squares error for structure from motion with a single infinitesimal motion ("structure from optical flow"). We present asymptotic approximations to the noiseless error over two, complementary regions of motion estimates: roughly forward and non-forward translations. Our approximations are powerful tools for understanding the error. Experiments show that they capture its detailed behavior over the entire range of motions. We illustrate the use of our approximations by deriving new properties of the least-squares error. We generalize the earlier results of Jepson/Heeger/Maybank on the bas-relief ambiguity and of Oliensis on the reflected minimum. We explain the error's complexity and its multiple local minima for roughly forward translation estimates (epipoles within the field of view) and identify the factors that make this complexity likely. For planar scenes, we clarify the effects of the two-fold ambiguity, show the existence of a new, double bas-relief ambiguity, and analyze the error's local minima. For nonplanar scenes, we derive simplified error approximations for reasonable assumptions on the image and scene. For example, we show that the error tends to have a simpler form when many points are tracked. We show experimentally that our analysis for zero image noise gives a good model of the error for large noise. We show theoretically and experimentally that the error for projective structure from motion is simpler but flatter than the error for calibrated images.},
author = {Oliensis, John},
doi = {10.1023/B:VISI.0000045326.88734.8b},
isbn = {3-540-21981-1},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {ambiguity,error analysis,least-squares error,local minima,motion estimation,noise sensitivity,optical flow,optimization,shape from x,structure from motion},
number = {3},
pages = {48--99},
title = {{The least-squares error for structure from infinitesimal motion}},
volume = {61},
year = {2005}
}
@inproceedings{Ma2012,
abstract = {We present a real-time system that enables a highly capable dynamic quadruped robot to maintain an accurate 6-DOF pose estimate (better than 0.5m over every 50m traveled) over long distances traversed through complex, dynamic outdoor terrain, during day and night, in the presence of camera occlusion and saturation, and occasional large external disturbances, such as slips or falls. The system fuses a stereo-camera sensor, inertial measurement units (IMU), and leg odometry with an Extended Kalman Filter (EKF) to ensure robust, low-latency performance. Extensive experimental results obtained from multiple field tests are presented to illustrate the performance and robustness of the system over hours of continuous runs over hundreds of meters of distance traveled in a wide variety of terrains and conditions. {\textcopyright} 2012 IEEE.},
author = {Ma, Jeremy and Susca, Sara and Bajracharya, Max and Matthies, Larry and Malchano, Matt and Wooden, Dave},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225132},
isbn = {9781467314039},
issn = {10504729},
month = {may},
pages = {619--626},
publisher = {IEEE},
title = {{Robust multi-sensor, day/night 6-DOF pose estimation for a dynamic legged vehicle in GPS-denied environments}},
url = {http://ieeexplore.ieee.org/document/6225132/},
year = {2012}
}
@article{Scaramuzza2009a,
abstract = {This paper presents a system capable of recovering the trajectory of a vehicle from the video input of a single camera at a very high frame-rate. The overall frame-rate is limited only by the feature extraction process, as the outlier removal and the motion estimation steps take less than 1 millisecond with a normal laptop computer. The algorithm relies on a novel way of removing the outliers of the feature matching process.We show that by exploiting the nonholonomic constraints of wheeled vehicles it is possible to use a restrictive motion model which allows us to parameterize the motion with only 1 feature correspondence. Using a single feature correspondence for motion estimation is the lowest model parameterization possible and results in the most efficient algorithms for removing outliers. Here we present two methods for outlier removal. One based on RANSAC and the other one based on histogram voting. We demonstrate the approach using an omnidirectional camera placed on a vehicle during a peak time tour in the city of Zurich. We show that the proposed algorithm is able to cope with the large amount of clutter of the city (other moving cars, buses, trams, pedestrians, sudden stops of the vehicle, etc.). Using the proposed approach, we cover one of the longest trajectories ever reported in real-time from a single omnidirectional camera and in cluttered urban scenes, up to 3 kilometers.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich and Siegwart, Roland},
doi = {10.1109/robot.2009.5152255},
file = {::},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {2009 IEEE International Conference on Robotics and Automation},
keywords = {camera calibration,computer vision,omnidirectional vision,ransac,slam,structure from motion,visual odometry},
pages = {4293--4299},
title = {{Real-time monocular visual odometry for on-road vehicles with 1-point RANSAC}},
year = {2009}
}
@book{Silberschatz2008,
author = {Sookoor, Tamim},
booktitle = {Science},
edition = {8th},
isbn = {0470128720},
pages = {1--20},
publisher = {Wiley Publishing},
title = {{Operating System Concepts - Chapters}},
year = {2007}
}
@article{Moro2013,
abstract = {This manuscript proposes a method to directly transfer the features of horse walking, trotting, and galloping to a quadruped robot, with the aim of creating a much more natural (horse-like) locomotion profile. A principal component analysis on horse joint trajectories shows that walk, trot, and gallop can be described by a set of four kinematic Motion Primitives (kMPs). These kMPs are used to generate valid, stable gaits that are tested on a compliant quadruped robot. Tests on the effects of gait frequency scaling as follows: results indicate a speed optimal walking frequency around 3.4 Hz, and an optimal trotting frequency around 4 Hz. Following, a criterion to synthesize gait transitions is proposed, and the walk/trot transitions are successfully tested on the robot. The performance of the robot when the transitions are scaled in frequency is evaluated by means of roll and pitch angle phase plots. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Moro, Federico L. and Spr{\"{o}}witz, Alexander and Tuleu, Alexandre and Vespignani, Massimo and Tsagarakis, Nikos G. and Ijspeert, Auke J. and Caldwell, Darwin G.},
doi = {10.1007/s00422-013-0551-9},
file = {::},
issn = {03401200},
journal = {Biological Cybernetics},
keywords = {Compliant quadruped robot,Gait transitions,Horse-like gaits,Kinematic Motion Primitives (kMPs),Quadrupedal locomotion},
number = {3},
pages = {309--320},
pmid = {23463501},
title = {{Horse-like walking, trotting, and galloping derived from kinematic Motion Primitives (kMPs) and their application to walk/trot transitions in a compliant quadruped robot}},
volume = {107},
year = {2013}
}
@article{Censi2016,
abstract = {This paper describes a theory of co-design. The objects of investigation are design problems, defined as tuples of function space, implementation space, and resources space, along with the relations that relates implementation to function and implementation to resource usage. Co-design problems are defined as the composition of design problems by three operations, roughly equivalent to the concepts of series, parallel, and feedback. Monotone design problems are those for which functions are partially ordered and there is an order-preserving map between functions and resources. The main results is that any composition of monotone co-design problem is monotone, and that there exists a systematic procedure to obtain the solution to the composed problem explicitly from the solution of the primitive problems.},
author = {Censi, Andrea},
doi = {10.1109/ACC.2016.7525085},
isbn = {9781467386821},
issn = {07431619},
journal = {Proceedings of the American Control Conference},
pages = {1227--1234},
title = {{Monotone co-design problems; Or, everything is the same}},
volume = {2016-July},
year = {2016}
}
@article{Xiao2020,
abstract = {Moving in complex environments is an essential capability of intelligent mobile robots. Decades of research and engineering have been dedicated to developing sophisticated navigation systems to move mobile robots from one point to another. Despite their overall success, a recently emerging research thrust is devoted to developing machine learning techniques to address the same problem, based in large part on the success of deep learning. However, to date, there has not been much direct comparison between the classical and emerging paradigms to this problem. In this article, we survey recent works that apply machine learning for motion control in mobile robot navigation, within the context of classical navigation systems. The surveyed works are classified into different categories, which delineate the relationship of the learning approaches to classical methods. Based on this classification, we identify common challenges and promising future directions.},
archivePrefix = {arXiv},
arxivId = {2011.13112},
author = {Xiao, Xuesu and Liu, Bo and Warnell, Garrett and Stone, Peter},
doi = {10.1177/ToBeAssigned},
eprint = {2011.13112},
file = {:home/matias/Documents/Mendeley Desktop/Xiao, Batou, Ritto, Desceliers/2019/Xiao, Batou, Ritto, Desceliers - 2019 - Stochastic modeling for hysteretic bit–rock interaction of a drill string under torsional vibr.pdf:pdf},
journal = {The International Journal of Robotics Research},
month = {nov},
pages = {107754631982824},
title = {{Motion Control for Mobile Robot Navigation Using Machine Learning: a Survey}},
url = {http://arxiv.org/abs/2011.13112},
year = {2020}
}
@article{Abbeel2005,
abstract = {Kalman filters are a workhorse of robotics and are routinely used in state-estimation problems. However, their performance critically depends on a large number of modeling parameters which can be very difficult to obtain, and are often set via significant manual tweaking and at a great cost of engineering time. In this paper, we propose a method for automatically learning the noise parameters of a Kalman filter. We also demonstrate on a commercial wheeled rover that our Kalman filter's learned noise covariance parameters-obtained quickly and fully automatically-significantly outperform an earlier, carefully and laboriously hand-designed one.},
author = {Abbeel, Pieter and Coates, Adam and Montemerlo, Michael and Ng, Andrew Y. and Thrun, Sebastian},
doi = {10.15607/rss.2005.i.038},
isbn = {9780262701143},
issn = {2330765X},
journal = {Robotics: Science and Systems},
pages = {289--296},
title = {{Discriminative training of kalman filters}},
volume = {1},
year = {2005}
}
@article{Albani2015,
abstract = {To recognize facial expression from candid, non-posed images, we propose a deep-learning based approach using convolutional neural networks (CNNs). In order to evaluate the performance in real-time candid facial expression recognition, we have created a candid image facial expression (CIFE) dataset, with seven types of expression in more than 10,000 images gathered from the Web. As baselines, two feature-based approaches (LBP+SVM, SIFT+SVM) are tested on the dataset. The structure of our proposed CNN-based approach is described, and a data augmentation technique is provided in order to generate sufficient number of training samples. The performance using the feature-based approaches is close to the state of the art when tested with standard datasets, but fails to function well when dealing with candid images. Our experiments show that the CNN-based approach is very effective in candid image expression recognition, significantly outperforming the baseline approaches, by a 20% margin.},
author = {Li, Wei and Li, Min and Su, Zhong and Zhu, Zhigang},
doi = {10.1109/MVA.2015.7153185},
file = {::},
isbn = {9784901122153},
journal = {Proceedings of the 14th IAPR International Conference on Machine Vision Applications, MVA 2015},
keywords = {deep learning,nao robots,robocup spl,robot vision},
number = {July},
pages = {279--282},
title = {{A deep-learning approach to facial expression recognition with candid images}},
year = {2015}
}
@article{Li2014a,
abstract = {When fusing visual and inertial measurements for motion estimation, each measurement's sampling time must be precisely known. This requires knowledge of the time offset that inevitably exists between the two sensors' data streams. The first contribution of this work is an online approach for estimating this time offset, by treating it as an additional state variable to be estimated along with all other variables of interest (inertial measurement unit (IMU) pose and velocity, biases, camera-to-IMU transformation, feature positions). We show that this approach can be employed in pose-tracking with mapped features, in simultaneous localization and mapping, and in visual-inertial odometry. The second main contribution of this paper is an analysis of the identifiability of the time offset between the visual and inertial sensors. We show that the offset is locally identifiable, except in a small number of degenerate motion cases, which we characterize in detail. These degenerate cases are either (i) cases known to cause loss of observability even when no time offset exists, or (ii) cases that are unlikely to occur in practice. Our simulation and experimental results validate these theoretical findings, and demonstrate that the proposed approach yields high-precision, consistent estimates, in scenarios involving either known or unknown features, with both constant and time-varying offsets. {\textcopyright} The Author(s) 2014.},
author = {Li, Mingyang and Mourikis, Anastasios I.},
doi = {10.1177/0278364913515286},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Vision-aided inertial navigation,identifiability analysis,online temporal calibration,time-offset estimation},
number = {7},
pages = {947--964},
title = {{Online temporal calibration for camera-IMU systems: Theory and algorithms}},
url = {http://ijr.sagepub.com/content/33/7/947.short},
volume = {33},
year = {2014}
}
@phdthesis{Eade2008,
abstract = {Simultaneous localisation and mapping is the task of estimating from sensor observations both motion and structure in an unknown environment. Performing SLAMwith a single video camera, while an attractive prospect, adds its own particular difficulties to the already considerable general challenges of the problem. This thesis advances the state of the art in monocular SLAM in terms of efficiency, richness of scene description, statistical correctness, and robustness. First, a SLAM algorithm from the robotics literature, designed to permit efficient operation with complex maps, is adapted to the monocular setting. A method for efficiently and correctly adding landmarks to the map is presented. The implemented SLAM system accurately maps thousands of landmarks in real time, giving an orderof- magnitude performance improvement over previous methods. Next, the system is extended to allow incorporation of edge landmarks as well as points. Edgelet landmarks and their representation are defined, and a method is described for reliably tracking edgelets, even in the presence of measurement ambiguity. An efficient selection algorithm for acquiring new edgelets from video allows the system to quickly extend the map. The working system produces geometrically accurate and meaningful edge maps at frame rate. With a focus on preserving statistical consistency during estimation, a novel monocular SLAM algorithm is presented. Estimation proceeds on a graph of local maps, partitioning and coalescing the observations taken from video. Careful parameterisation keeps local maps consistent,while optimisation of the connecting graph structure aids global convergence. The system can handle thousands of landmarks at frame rate, while delivering statistical performance superior to existing methods. Finally, this thesis mitigates the problems of tracking failure and large-scale localisation with a unified framework for loop closing and recovery. A hierarchical method is presented for finding correspondences between new video images and the existing map, using local and global appearance models and structure estimates. The framework is instantiated within the graph-based monocular SLAM system. The extended implementation continues mapping despite repeated tracking failures, successfully joining maps and closing loops in real time.},
author = {Eade, Ethan},
booktitle = {Philosophy},
number = {September},
pages = {217},
school = {Universiy of Cambridge},
title = {{Monocular Simultaneous Localisation and Mapping}},
url = {http://mi.eng.cam.ac.uk/$\sim$ee231/thesis_revised.pdf},
year = {2008}
}
@article{Frost2016,
abstract = {Without knowledge of the absolute baseline between images, the scale of a map from single-camera simultaneous localization and mapping system is subject to calamitous drift over time. We describe a monocular approach that in addition to point measurements also considers object detections to resolve this scale ambiguity and drift. By placing a prior on the size of the objects, the scale estimation can be seamlessly integrated into a bundle adjustment. When object observations are available, the local scale of the map is then determined jointly with the camera pose in local adjustments. Unlike many previous visual odometry methods, our approach does not impose restrictions such as approximately constant camera height or planar roadways, and is therefore applicable to a much wider range of applications. We evaluate our approach on the KITTI dataset and show that it reduces scale drift over long-range outdoor sequences with a total length of 40 km. Qualitative evaluation is also performed on video footage from a hand-held camera.},
author = {Frost, Duncan P. and Kahler, Olaf and Murray, David W.},
doi = {10.1109/ICRA.2016.7487680},
file = {::},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4770--4776},
title = {{Object-aware bundle adjustment for correcting monocular scale drift}},
volume = {2016-June},
year = {2016}
}
@techreport{Jaekel,
abstract = {In this paper we present a novel multi-stereo visual-inertial odometry (VIO) framework which aims to improve the robustness of a robot's state estimate during aggressive motion and in visually challenging environments. Our system uses a fixed-lag smoother which jointly optimizes for poses and landmarks across all stereo pairs. We also makes use of nonlinear factor recovery (NFR) in order to enforce a sparse information matrix after marginalization while also maintaining the information contained in the dense priors. We propose a novel 1-point RANdom SAmple Consensus (RANSAC) algorithm which is able to jointly perform outlier rejection across features from all stereo pairs. The result is a VIO system which is able to maintain an accurate state estimate under conditions which have typically proven to be challenging for traditional state-of-the-art VIO systems. We demonstrate the benefits of our proposed multi-stereo algorithm by evaluating it against VINS-Mono on three challenging simulated micro-aerial vehicle (MAV) flights. We show that our proposed algorithm is able to achieve a significantly lower average trajectory error on all three flights.},
author = {Jaekel, Joshua and Kaess, Michael},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
file = {:home/matias/Documents/Mendeley Desktop/Jaekel, Kaess/2019/Jaekel, Kaess - 2019 - Robust Multi-Stereo Visual-Inertial Odometry.pdf:pdf},
title = {{Robust Multi-Stereo Visual-Inertial Odometry}},
year = {2019}
}
@inproceedings{Stasse2006,
abstract = {Humanoid robotics and SLAM (Simultaneous Localisation and Mapping) are certainly two of the most significant themes of the current worldwide robotics research effort, but the two fields have up until now largely run independent parallel paths, despite the obvious benefit to be gained in joining the two. The next major step forward in humanoid robotics will be increased autonomy, and the ability of a robot to create its own world map on the fly will be a significant enabling technology. Meanwhile, SLAM techniques have found most success with robot platforms and sensor configurations which are outside of the humanoid domain. Humanoid robots move with high linear and angular accelerations in full 3D, and normally only vision is available as an outward-looking sensor. Building on recently published work on monocular SLAM using vision, and on pattern generation, we show that real-time SLAM for a humanoid can indeed be achieved. Using HRP-2, we present results in which a sparse 3D map of visual landmarks is acquired on the fly using a single camera and demonstrated loop closing and drift-free 3D motion estimation within a typical cluttered indoor environment. This is achieved by tightly coupling the pattern generator, the robot odometry and inertial sensing to aid visual mapping within a standard EKF framework. To our knowledge this is the first implementation of real-time 3D SLAM for a humanoid robot able to demonstrate loop closing. {\textcopyright} 2006 IEEE.},
author = {Stasse, Olivier and Davison, Andrew J. and Sellaouti, Ramzi and Yokoi, Kazuhito},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2006.281645},
isbn = {142440259X},
month = {oct},
pages = {348--355},
publisher = {IEEE},
title = {{Real-time 3D SLAM for humanoid robot considering pattern generator information}},
url = {http://ieeexplore.ieee.org/document/4058956/},
year = {2006}
}
@article{Zeestraten2017,
abstract = {In imitation learning, multivariate Gaussians are widely used to encode robot behaviors. Such approaches do not provide the ability to properly represent end-effector orientation, as the distance metric in the space of orientations is not Euclidean. In this paper, we present an extension of common imitation learning techniques to Riemannian manifolds. This generalization enables the encoding of joint distributions that include the robot pose. We show that Gaussian conditioning, Gaussian product, and nonlinear regression can be achieved with this representation. The proposed approach is illustrated with examples on a two-dimensional sphere, with an example of regression between two robot end-effector poses, as well as an extension of task-parameterized Gaussian mixture model and Gaussian mixture regression to Riemannian manifolds.},
author = {Zeestraten, Martijn J.A. and Havoutis, Ioannis and Silverio, Joao and Calinon, Sylvain and Caldwell, Darwin G.},
doi = {10.1109/LRA.2017.2657001},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Learning and adaptive systems,Probability and statistical methods},
number = {3},
pages = {1240--1247},
title = {{An Approach for Imitation Learning on Riemannian Manifolds}},
url = {http://ieeexplore.ieee.org/document/7829369/},
volume = {2},
year = {2017}
}
@article{Oßwald2010,
abstract = {Reliable and efficient navigation with a humanoid robot is a difficult task. First, the motion commands are executed rather inaccurately due to backlash in the joints or foot slippage. Second, the observations are typically highly affected by noise due to the shaking behavior of the robot. Thus, the localization performance is typically reduced while the robot moves and the uncertainty about its pose increases. As a result, the reliable and efficient execution of a navigation task cannot be ensured anymore since the robot's pose estimate might not correspond to the true location. In this paper, we present a reinforcement learning approach to select appropriate navigation actions for a humanoid robot equipped with a camera for localization. The robot learns to reach the destination reliably and as fast as possible, thereby choosing actions to account for motion drift and trading off velocity in terms of fast walking movements against accuracy in localization.We present extensive simulated and practical experiments with a humanoid robot and demonstrate that our learned policy significantly outperforms a hand-optimized navigation strategy. {\textcopyright}2010 IEEE.},
author = {O{\ss}wald, Stefan and Hornung, Armin and Bennewitz, Maren},
doi = {10.1109/ROBOT.2010.5509420},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2375--2380},
title = {{Learning reliable and efficient navigation with a humanoid}},
year = {2010}
}
@article{Leutenegger2015,
abstract = {Combining visual and inertial measurements has become popular in mobile robotics, since the two sensing modalities offer complementary characteristics that make them the ideal choice for accurate visual-inertial odometry or simultaneous localization and mapping (SLAM). While historically the problem has been addressed with filtering, advancements in visual estimation suggest that nonlinear optimization offers superior accuracy, while still tractable in complexity thanks to the sparsity of the underlying problem. Taking inspiration from these findings, we formulate a rigorously probabilistic cost function that combines reprojection errors of landmarks and inertial terms. The problem is kept tractable and thus ensuring real-time operation by limiting the optimization to a bounded window of keyframes through marginalization. Keyframes may be spaced in time by arbitrary intervals, while still related by linearized inertial terms. We present evaluation results on complementary datasets recorded with our custom-built stereo visual-inertial hardware that accurately synchronizes accelerometer and gyroscope measurements with imagery. A comparison of both a stereo and monocular version of our algorithm with and without online extrinsics estimation is shown with respect to ground truth. Furthermore, we compare the performance to an implementation of a state-of-the-art stochastic cloning sliding-window filter. This competitive reference implementation performs tightly coupled filtering-based visual-inertial odometry. While our approach declaredly demands more computation, we show its superior performance in terms of accuracy.},
author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
doi = {10.1177/0278364914554813},
isbn = {0278-3649},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Visual-inertial odometry,bundle adjustment,inertial measurement unit (IMU),keyframes,robotics,sensor fusion,simultaneous localization and mapping (SLAM),stereo camera},
number = {3},
pages = {314--334},
title = {{Keyframe-based visual-inertial odometry using nonlinear optimization}},
volume = {34},
year = {2015}
}
@article{Gupta2015,
abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.},
archivePrefix = {arXiv},
arxivId = {1502.02551},
author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
eprint = {1502.02551},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {1737--1746},
title = {{Deep learning with limited numerical precision}},
url = {http://arxiv.org/abs/1502.02551},
volume = {3},
year = {2015}
}
@inproceedings{Pretto2009,
abstract = {Motion blur is a severe problem in images grabbed by legged robots and, in particular, by small humanoid robots. Standard feature extraction and tracking approaches typically fail when applied to sequences of images strongly affected by motion blur. In this paper, we propose a new feature detection and tracking scheme that is robust even to nonuniform motion blur. Furthermore, we developed a framework for visual odometry based on features extracted out of and matched in monocular image sequences. To reliably extract and track the features, we estimate the point spread function (PSF) of the motion blur individually for image patches obtained via a clustering technique and only consider highly distinctive features during matching. We present experiments performed on standard datasets corrupted with motion blur and on images taken by a camera mounted on walking small humanoid robots to show the effectiveness of our approach. The experiments demonstrate that our technique is able to reliably extract and match features and that it is furthermore able to generate a correct visual odometry, even in presence of strong motion blur effects and without the aid of any inertial measurement sensor. {\textcopyright} 2009 IEEE.},
author = {Pretto, Alberto and Menegatti, Emanuele and Bennewitz, Maren and Burgard, Wolfram and Pagello, Enrico},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152447},
isbn = {9781424427895},
issn = {10504729},
month = {may},
pages = {2250--2257},
publisher = {IEEE},
title = {{A visual odometry framework robust to motion blur}},
url = {http://ieeexplore.ieee.org/document/5152447/},
year = {2009}
}
@article{Warren2019,
abstract = {Redundant navigation systems are critical for safe operation of UAVs in high-risk environments. Since most commercial UAVs almost wholly rely on GPS, jamming, interference, and multi-pathing are real concerns that usually limit their operations to low-risk environments and visual line-of-sight. This letter presents a vision-based route-following system for the autonomous, safe return of UAVs under primary navigation failure such as GPS jamming. Using a Visual Teach and Repeat framework to build a visual map of the environment during an outbound flight, we show the autonomous return of the UAV by visually localizing the live view to this map when a simulated GPS failure occurs, controlling the vehicle to follow the safe outbound path back to the launch point. Using gimbal-stabilized stereo vision and inertial sensing alone, without reliance on external infrastructure, Visual Odometry and localization are achieved at altitudes of 5-25 m and flight speeds up to 55 km/h. We examine the performance of the visual localization algorithm under a variety of conditions and also demonstrate closed-loop autonomy along a complicated 450 m path.},
archivePrefix = {arXiv},
arxivId = {1809.05757},
author = {Warren, Michael and Greeff, Melissa and Patel, Bhavit and Collier, Jack and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/LRA.2018.2883408},
eprint = {1809.05757},
file = {:home/matias/Documents/Mendeley Desktop/Warren et al/2019/Warren et al. - 2019 - There ' s No Place Like Home Visual Teach and Repeat for Emergency Return of Multirotor UAVs During GPS Failur.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial systems: Perception and autonomy,sensor-based control,visual-based navigation},
number = {1},
pages = {161--168},
title = {{There's no place like home: Visual teach and repeat for emergency return of multirotor UAVs during GPS failure}},
volume = {4},
year = {2019}
}
@inproceedings{Houben2016,
abstract = {Visual SLAM is an area of vivid research and bears countless applications for moving robots. In particular, micro aerial vehicles benefit from visual sensors due to their low weight. Their motion is, however, often faster and more complex than that of ground-based robots which is why systems with multiple cameras are currently evaluated and deployed. This, in turn, drives the computational demand for visual SLAM algorithms. We present an extension of the recently introduced monocular ORB-SLAM for multiple cameras alongside an inertial measurement unit (IMU). Our main contributions are: Embedding the multi-camera setup into the underlying graph SLAM approach that defines the upcoming sparse optimization problems on several adjusted subgraphs, integration of an IMU filter that supports visual tracking, and enhancements of the original algorithm in local map estimation and keyframe creation. The SLAM system is evaluated on a public stereo SLAM dataset for flying robots and on a new dataset with three mounted cameras. The main advantages of the proposed method are its restricted computational load, high positional accuracy, and low number of parameters.},
author = {Houben, Sebastian and Quenzel, Jan and Krombach, Nicola and Behnke, Sven},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759261},
file = {::},
isbn = {9781509037629},
issn = {21530866},
keywords = {SLAM,Unmanned Aerial Systems,Visual Tracking},
month = {oct},
number = {October},
pages = {1616--1622},
pmid = {7759261},
publisher = {IEEE},
title = {{Efficient multi-camera visual-inertial SLAM for micro aerial vehicles}},
url = {http://ieeexplore.ieee.org/document/7759261/},
volume = {2016-Novem},
year = {2016}
}
@article{Klingensmith2016,
abstract = {A robot with a hand-mounted depth sensor scans a scene. When the robot's joint angles are not known with certainty, how can it best reconstruct the scene? In this work, we simultaneously estimate the joint angles of the robot and reconstruct a dense volumetric model of the scene. In this way, we perform simultaneous localization and mapping in the configuration space of the robot, rather than in the pose space of the camera. We show using simulations and robot experiments that our approach greatly reduces both 3-D reconstruction error and joint angle error over simply using the forward kinematics. Unlike other approaches, ours directly reasons about robot joint angles, and can use these to constrain the pose of the sensor. Because of this, it is more robust to missing or ambiguous depth data than approaches that are unconstrained by the robot's kinematics.},
author = {Klingensmith, Matthew and Sirinivasa, Siddartha S. and Kaess, Michael},
doi = {10.1109/LRA.2016.2518242},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Kinematics,Mapping,RGBD Perception,SLAM,Visual-Based navigation},
month = {jul},
number = {2},
pages = {1156--1163},
title = {{Articulated Robot Motion for Simultaneous Localization and Mapping (ARM-SLAM)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7383249},
volume = {1},
year = {2016}
}
@article{Bajcsy2018,
abstract = {Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.},
archivePrefix = {arXiv},
arxivId = {1603.02729},
author = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.},
doi = {10.1007/s10514-017-9615-3},
eprint = {1603.02729},
file = {:home/matias/Documents/Mendeley Desktop/Bajcsy, Aloimonos, Tsotsos/2018/Bajcsy, Aloimonos, Tsotsos - 2018 - Revisiting active perception.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Attention,Control,Perception,Sensing},
number = {2},
pages = {177--196},
title = {{Revisiting active perception}},
volume = {42},
year = {2018}
}
@article{Carlone2015,
abstract = {Pose graph optimization is the non-convex optimization problem underlying pose-based Simultaneous Localization and Mapping (SLAM). If robot orientations were known, pose graph optimization would be a linear least-squares problem, whose solution can be computed efficiently and reliably. Since rotations are the actual reason why SLAM is a difficult problem, in this work we survey techniques for 3D rotation estimation. Rotation estimation has a rich history in three scientific communities: robotics, computer vision, and control theory. We review relevant contributions across these communities, assess their practical use in the SLAM domain, and benchmark their performance on representative SLAM problems (Fig. 1). We show that the use of rotation estimation to bootstrap iterative pose graph solvers entails significant boost in convergence speed and robustness.},
author = {Carlone, Luca and Tron, Roberto and Daniilidis, Kostas and Dellaert, Frank},
doi = {10.1109/ICRA.2015.7139836},
file = {::},
isbn = {9781479969227},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {4597--4604},
title = {{Initialization techniques for 3D SLAM: A survey on rotation estimation and its use in pose graph optimization}},
volume = {2015-June},
year = {2015}
}
@article{Ma2017,
abstract = {We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59 % to 92 % on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs. Software22https://github.com/fangchangma/sparse-to-dense and video demonstration33https://www.youtube.com/watch?v=vNIIT-M7×7Y are publicly available.},
archivePrefix = {arXiv},
arxivId = {1709.07492},
author = {Mal, Fangchang and Karaman, Sertac},
doi = {10.1109/ICRA.2018.8460184},
eprint = {1709.07492},
file = {::},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4796--4803},
title = {{Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image}},
url = {http://arxiv.org/abs/1709.07492},
year = {2018}
}
@article{Galvez-Lopez2014,
abstract = {We present a real-time object-based SLAM system that leverages the largest object database to date. Our approach comprises two main components: (1) a monocular SLAM algorithm that exploits object rigidity constraints to improve the map and find its real scale, and (2) a novel object recognition algorithm based on bags of binary words, which provides live detections with a database of 500 3D objects. The two components work together and benefit each other: the SLAM algorithm accumulates information from the observations of the objects, anchors object features to especial map landmarks and sets constrains on the optimization. At the same time, objects partially or fully located within the map are used as a prior to guide the recognition algorithm, achieving higher recall. We evaluate our proposal on five real environments showing improvements on the accuracy of the map and efficiency with respect to other state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1504.02398},
author = {G{\'{a}}lvez-L{\'{o}}pez, Dorian and Salas, Marta and Tard{\'{o}}s, Juan D. and Montiel, J. M.M.},
doi = {10.1016/j.robot.2015.08.009},
eprint = {1504.02398},
file = {::},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Object recognition,Object slam},
pages = {435--449},
title = {{Real-time monocular object SLAM}},
volume = {75},
year = {2016}
}
@inproceedings{Xinjilefu2014b,
abstract = {We propose a framework to use full-body dynamics for humanoid state estimation. The main idea is to decouple the full body state vector into several independent state vectors. Some decoupled state vectors can be estimated very efficiently with a steady state Kalman Filter. In a steady state Kalman Filter, state covariance is computed only once during initialization. Furthermore, decoupling speeds up numerical linearization of the dynamic model. We demonstrate that these state estimators are capable of handling walking on flat ground and on rough terrain.},
author = {Xinjilefu, X. and Feng, Siyuan and Huang, Weiwei and Atkeson, Christopher G.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6906609},
isbn = {978-1-4799-3685-4},
issn = {10504729},
pages = {195--201},
title = {{Decoupled state estimation for humanoids using full-body dynamics}},
year = {2014}
}
@inproceedings{Long2012,
abstract = {Distributions in position and orientation are central to many problems in robot localization. To increase efficiency, a majority of algorithms for planar mobile robots use Gaussians defined on positional Cartesian coordinates and heading. However, the distribution of poses for a noisy two-wheeled robot moving in the plane has been observed by many to be a "bananashaped" distribution, which is clearly not Gaussian/normal in these coordinates. As uncertainty increases, many localization algorithms therefore become "inconsistent" due to the normality assumption breaking down. We observe that this is because the combination of Cartesian coordinates and heading is not the most appropriate set of coordinates to use, and that the banana distribution can be described in closed form as a Gaussian in an alternative set of coordinates via the so-called exponential map. With this formulation, we can derive closed-form expressions for propagating the mean and covariance of the Gaussian in these exponential coordinates for a differential-drive car moving along a trajectory constructed from sections of straight segments and arcs of constant curvature. In addition, we detail how to fuse two or more Gaussians in exponential coordinates together with given relative pose measurements between robots moving in formation. These propagation and fusion formulas utilized here reduce uncertainty in localization better than when using traditional methods. We demonstrate with numerical examples dramatic improvements in the estimated pose of three robots moving in formation when compared to classical Cartesiancoordinate-based Gaussian fusion methods.},
author = {Long, Andrew W. and Wolfe, C. Kevin and Mashner, Michael J. and Chirikjian, Gregory S.},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/rss.2012.viii.034},
file = {::},
isbn = {9780262519687},
issn = {2330765X},
pages = {265--272},
title = {{The banana distribution is Gaussian: A localization study with exponential coordinates}},
url = {http://roboticsproceedings.org/rss08/p34.pdf},
volume = {8},
year = {2013}
}
@article{Zhou,
abstract = {We describe an approach for simultaneous localization and calibration of a stream of range images. Our approach jointly optimizes the camera trajectory and a calibration function that corrects the camera's unknown nonlinear distortion. Experiments with real-world benchmark data and synthetic data show that our approach increases the accuracy of camera trajectories and geometric models estimated from range video produced by consumer-grade cameras.},
author = {Zhou, Qian Yi and Koltun, Vladlen},
doi = {10.1109/CVPR.2014.65},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {454--460},
title = {{Simultaneous localization and calibration: Self-calibration of consumer depth cameras}},
year = {2014}
}
@article{Ostafew2016,
abstract = {This paper presents a Learning-based Nonlinear Model Predictive Control (LB-NMPC) algorithm to achieve high-performance path tracking in challenging off-road terrain through learning. The LB-NMPC algorithm uses a simple a priori vehicle model and a learned disturbance model. Disturbances are modeled as a Gaussian process (GP) as a function of system state, input, and other relevant variables. The GP is updated based on experience collected during previous trials. Localization for the controller is provided by an onboard, vision-based mapping and navigation system enabling operation in large-scale, GPS-denied environments. The paper presents experimental results including over 3 km of travel by three significantly different robot platforms with masses ranging from 50 to 600 kg and at speeds ranging from 0.35 to 1.2 m/s (associated video at http://tiny.cc/RoverLearnsDisturbances). Planned speeds are generated by a novel experience-based speed scheduler that balances overall travel time, path-tracking errors, and localization reliability. The results show that the controller can start from a generic a priori vehicle model and subsequently learn to reduce vehicle- and trajectory-specific path-tracking errors based on experience.},
author = {Ostafew, Chris J. and Schoellig, Angela P. and Barfoot, Timothy D. and Collier, Jack},
doi = {10.1002/rob.21587},
file = {:home/matias/Documents/Mendeley Desktop/Ostafew et al/2016/Ostafew et al. - 2016 - Learning-based Nonlinear Model Predictive Control to Improve Vision-based Mobile Robot Path Tracking.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {jan},
number = {1},
pages = {133--152},
title = {{Learning-based Nonlinear Model Predictive Control to Improve Vision-based Mobile Robot Path Tracking}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/rob.21514/abstract http://doi.wiley.com/10.1002/rob.21587},
volume = {33},
year = {2016}
}
@article{VanDIjk2019,
abstract = {Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles - even those not appearing in the training set - but may depend on features that are not universally present.},
archivePrefix = {arXiv},
arxivId = {1905.07005},
author = {{Van DIjk}, Tom and {De Croon}, Guido},
doi = {10.1109/ICCV.2019.00227},
eprint = {1905.07005},
file = {:home/matias/Documents/Mendeley Desktop/Van DIjk, De Croon/2019/Van DIjk, De Croon - 2019 - How do neural networks see depth in single images.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2183--2191},
title = {{How do neural networks see depth in single images?}},
volume = {2019-Octob},
year = {2019}
}
@article{Epstein2007,
abstract = {A key component of spatial navigation is the ability to use visual information to ascertain where one is located and how one is oriented in the world. We used functional magnetic resonance imaging to examine the neural correlates of this phenomenon in humans. Subjects were scanned while retrieving different kinds of topographical and nontopographical information in response to visual scenes. In the three critical conditions, they viewed images of a familiar college campus, and reported either the location of the place depicted in the image (location task), the compass direction that the camera was facing when the image was taken (orientation task), or whether the location was on campus or not (familiarity task). Our analyses focused on the retrosplenial cortex (RSC)/parietal-occipital sulcus region and the parahippocampal place area (PPA), which previous studies indicate play a critical role in place recognition. RSC activity depended on the type of information retrieved, with the strongest response in the location task. In contrast, PPA activity did not depend on the retrieval task. Additional analyses revealed a strong effect of familiarity in RSC but not in the PPA, with the former region responding much more strongly to images of the familiar campus than to images of an unfamiliar campus. These results suggest that the PPA and RSC play distinct but complementary roles in place recognition. In particular, the PPA may primarily support perception of the immediate scene, whereas RSC may support memory retrieval mechanisms that allow the scene to be localized within the broader spatial environment. Copyright {\textcopyright} 2007 Society for Neuroscience.},
author = {Epstein, Russell A. and Parker, Whitney E. and Feiler, Alana M.},
doi = {10.1523/JNEUROSCI.0799-07.2007},
file = {:home/matias/Documents/Mendeley Desktop/Epstein, Parker, Feiler/2007/Epstein, Parker, Feiler - 2007 - Where Am I Now Distinct Roles for Parahippocampal and Retrosplenial Cortices in Place Recognition.pdf:pdf},
issn = {02706474},
journal = {Journal of Neuroscience},
keywords = {Navigation,Parahippocampal cortex,Place recognition,Retrosplenial cortex,Scene analysis,fMRI},
number = {23},
pages = {6141--6149},
pmid = {17553986},
title = {{Where am i now? Distinct roles for parahippocampal and retrosplenial cortices in place recognition}},
volume = {27},
year = {2007}
}
@article{Courbariaux2015,
abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g.-1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
archivePrefix = {arXiv},
arxivId = {1511.00363},
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean Pierre},
eprint = {1511.00363},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {3123--3131},
title = {{Binaryconnect: Training deep neural networks with binary weights during propagations}},
url = {http://arxiv.org/abs/1511.00363},
volume = {2015-Janua},
year = {2015}
}
@article{Radford2014,
abstract = {In December 2013, 16 teams from around the world gathered at Homestead Speedway near Miami, FL to participate in the DARPA Robotics Challenge (DRC) Trials, an aggressive robotics competition partly inspired by the aftermath of the Fukushima Daiichi reactor incident. While the focus of the DRC Trials is to advance robotics for use in austere and inhospitable environments, the objectives of the DRC are to progress the areas of supervised autonomy and mobile manipulation for everyday robotics. NASA's Johnson Space Center led a team comprised of numerous partners to develop Valkyrie, NASA's first bipedal humanoid robot. Valkyrie is a 44 degree-of-freedom, series elastic actuator-based robot that draws upon over 18 years of humanoid robotics design heritage. Valkyrie's application intent is aimed at not only responding to events like Fukushima, but also advancing human spaceflight endeavors in extraterrestrial planetary settings. This paper presents a brief system overview, detailing Valkyrie's mechatronic subsystems, followed by a summarization of the inverse kinematics-based walking algorithm employed at the Trials. Next, the software and control architectures are highlighted along with a description of the operator interface tools. Finally, some closing remarks are given about the competition, and a vision of future work is provided.},
author = {Radford, Nicolaus A. and Strawser, Philip and Hambuchen, Kimberly and Mehling, Joshua S. and Verdeyen, William K. and Donnan, A. Stuart and Holley, James and Sanchez, Jairo and Nguyen, Vienny and Bridgwater, Lyndon and Berka, Reginald and Ambrose, Robert and {Myles Markee}, Mason and Fraser-Chanpong, N. J. and McQuin, Christopher and Yamokoski, John D. and Hart, Stephen and Guo, Raymond and Parsons, Adam and Wightman, Brian and Dinh, Paul and Ames, Barrett and Blakely, Charles and Edmondson, Courtney and Sommers, Brett and Rea, Rochelle and Tobler, Chad and Bibby, Heather and Howard, Brice and Niu, Lei and Lee, Andrew and Conover, Michael and Truong, Lily and Reed, Ryan and Chesney, David and Platt, Robert and Johnson, Gwendolyn and Fok, Chien Liang and Paine, Nicholas and Sentis, Luis and Cousineau, Eric and Sinnet, Ryan and Lack, Jordan and Powell, Matthew and Morris, Benjamin and Ames, Aaron and Akinyode, Jide},
doi = {10.1002/rob.21560},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {3},
pages = {397--419},
title = {{Valkyrie: NASA's First Bipedal Humanoid Robot}},
volume = {32},
year = {2015}
}
@article{Barron2020,
abstract = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
archivePrefix = {arXiv},
arxivId = {1701.03077},
author = {Barron, Jonathan T.},
doi = {10.1109/CVPR.2019.00446},
eprint = {1701.03077},
file = {:home/matias/Documents/Mendeley Desktop/Barron/2019/Barron - 2019 - A general and adaptive robust loss function.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {3D from Single Image,Computer Vision Theory,Deep Learning,Image and Video Synthesis,Low-level Vision,Statistic},
pages = {4326--4334},
title = {{A general and adaptive robust loss function}},
volume = {2019-June},
year = {2019}
}
@article{Toussaint2016,
author = {Toussaint, Marc},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint/2016/Toussaint - 2016 - Maths for Intelligent Systems.pdf:pdf},
number = {20},
title = {{Maths for Intelligent Systems}},
year = {2016}
}
@article{Zhang2002,
abstract = {Instantaneous camera motion estimation is an important research topic in computer vision. Although in the theory more than five points uniquely determine the solution in an ideal situation, in practice one can usually obtain better estimates by using more image velocity measurements because of the noise present in the velocity measurements. However, the usefulness of using a large number of observations has never been analyzed in detail. In this paper, we formulate this problem in the statistical estimation framework. We show that under certain noise models, consistency of motion estimation can be established: that is, arbitrarily accurate estimates of motion parameters are possible with more and more observations. This claim does not simply follow from the general consistency result for maximum likelihood estimates. Some experiments will be provided to verify our theory. Our analysis and experiments also indicate that many previously proposed algorithms are inconsistent under even very simple noise models.},
author = {Zhang, Tong and Tomasi, Carlo},
doi = {10.1023/A:1013248231976},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Camera motion analysis,Consistency,Egomotion,Stochastic estimation},
number = {1},
pages = {51--79},
title = {{On the consistency of instantaneous rigid motion estimation}},
volume = {46},
year = {2002}
}
@article{Abuzaid2014,
abstract = {We hypothesize and study various systems optimiza-tions to speed up the performance of convolutional neu-ral networks on CPUs. Currently, large-scale CNN experi-ments require specialized hardware, such as NVidia GPUs, and specialized APIs, such as NVidia's CuDNN library, to achieve adequate training performance. This provides a significant barrier to research, as the availability of high-performance GPUs is rather scarce. To address this prob-lem, we examine the possibility of optimizing CNN perfor-mance for CPUs by borrowing techniques that have pre-viously been used for GPUs and studying the various per-formance trade-offs therein. With these improvements, we are able to train on CPUs up to 4X faster than state-of-the-art systems, such as Caffe, while still maintaining statistical correctness.},
author = {Abuzaid, Firas},
file = {::},
pages = {1--5},
title = {{Optimizing CPU Performance for Convolutional Neural Networks}},
year = {2014}
}
@article{Rittberger2017,
abstract = {No Abstract},
author = {Rittberger, Berthold and Richardson, Jeremy},
doi = {10.1080/13501763.2017.1316946},
file = {::},
issn = {14664429},
journal = {Journal of European Public Policy},
keywords = {#IstandwithCEU},
number = {3},
pages = {324},
publisher = {Taylor & Francis},
title = {{What happens when we do not defend academic freedom}},
url = {https://www.tandfonline.com/doi/full/10.1080/13501763.2017.1316946},
volume = {26},
year = {2019}
}
@article{Toussaint2016a,
author = {Toussaint, Marc},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint/2016/Toussaint - 2016 - Introduction to Robotics_Path Planning_RLearning.pdf:pdf},
number = {April},
title = {{Introduction to Robotics_Path Planning_RLearning}},
year = {2016}
}
@article{Oriolo2015,
abstract = {We present a method for odometric localization of humanoid robots using standard sensing equipment, i.e., a monocular camera, an inertial measurement unit (IMU), joint encoders and foot pressure sensors. Data from all these sources are integrated using the prediction-correction paradigm of the Extended Kalman Filter. Position and orientation of the torso, defined as the representative body of the robot, are predicted through kinematic computations based on joint encoder readings; an asynchronous mechanism triggered by the pressure sensors is used to update the placement of the support foot. The correction step of the filter uses as measurements the torso orientation, provided by the IMU, and the head pose, reconstructed by a VSLAM algorithm. The proposed method is validated on the humanoid NAO through two sets of experiments: open-loop motions aimed at assessing the accuracy of localization with respect to a ground truth, and closed-loop motions where the humanoid pose estimates are used in real-time as feedback signals for trajectory control.},
author = {Oriolo, Giuseppe and Paolillo, Antonio and Rosa, Lorenzo and Vendittelli, Marilena},
doi = {10.1007/s10514-015-9498-0},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {EKF,Humanoid robots,Localization,Odometry,Visual SLAM},
month = {jun},
number = {5},
pages = {867--879},
publisher = {Springer US},
title = {{Humanoid odometric localization integrating kinematic, inertial and visual information}},
url = {http://link.springer.com/10.1007/s10514-015-9498-0},
volume = {40},
year = {2016}
}
@article{Patarot2013,
abstract = {The approach described here attempts to overcome foot-mounted limitations, contrary to a majority of current implementations of inertial navigation systems (INS). The aim of our development is to maintain repeatable performance, especially without step counting, while carefully dealing with the mobility requirements and the computation cost. The inertial measurement unit (IMU) is belt mounted to facilitate the equipment of the user. The pedestrian trajectory is computed in real time. The resulting position is transmitted and displayed to the user on a smartphone where no specific application is installed. The description of our indoor experiments reveals the potential of this approach, in terms of positioning performance, with more than 75% of our experiments when the relative start-end error remains below 5% of the total traveled distance. {\textcopyright} 2013 IEEE.},
author = {Patarot, Alexandre and Boukallel, Mehdi and Lamy-Perbal, Sylvie and Vervisch-Picois, Alexandre},
doi = {10.1109/IPIN.2013.6817842},
file = {::},
isbn = {9781479940431},
journal = {2013 International Conference on Indoor Positioning and Indoor Navigation, IPIN 2013},
keywords = {Belt,IMU,INS,Indoor,Inertial Measurement Unit,Inertial Navigation System,Jerk,MEMS,MicroElectroMechanical Systems,Navigation,Pedestrian,Positioning,Quaternions},
number = {October},
pages = {28--31},
title = {{Belt mounted IMU with enhanced distance estimation for pedestrian indoor positioning}},
year = {2013}
}
@book{Thrun2005a,
abstract = {Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations. This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, www.probabilistic-robotics.org, has additional material. The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data.},
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
doi = {10.5555/1121596},
file = {:home/matias/Documents/Mendeley Desktop/Parsons/2006/Parsons - 2006 - Probabilistic Robotics by Sebastian Thrun, Wolfram Burgard and Dieter Fox, MIT Press, 647 pp., $55.00, ISBN 0-262-20162.pdf:pdf},
isbn = {9780262201629},
issn = {0269-8889},
pages = {668},
publisher = {MIT Press},
title = {{Probabilistic Robotics}},
year = {2005}
}
@article{Gao2019,
abstract = {In this paper, we propose a complete and robust motion planning system for the aggressive flight of autonomous quadrotors. The proposed method is built upon on a classical teach-and-repeat framework, which is widely adopted in infrastructure inspection, aerial transportation, and search-and-rescue. For these applications, human's intention is essential to decide the topological structure of the flight trajectory of the drone. However, poor teaching trajectories and changing environments prevent a simple teach-and-repeat system from being applied flexibly and robustly. In this paper, instead of commanding the drone to precisely follow a teaching trajectory, we propose a method to automatically convert a human-piloted trajectory, which can be arbitrarily jerky, to a topologically equivalent one. The generated trajectory is guaranteed to be smooth, safe, and kinodynamically feasible, with a human preferable aggressiveness. Also, to avoid unmapped or dynamic obstacles during flights, a sliding-windowed local perception and re-planning method are introduced to our system, to generate safe local trajectories onboard. We name our system as teach-repeat-replan. It can capture users' intention of a flight mission, convert an arbitrarily jerky teaching path to a smooth repeating trajectory, and generate safe local re-plans to avoid unmapped or moving obstacles. The proposed planning system is integrated into a complete autonomous quadrotor with global and local perception and localization sub-modules. Our system is validated by performing aggressive flights in challenging indoor/outdoor environments. We release all components in our quadrotor system as open-source ros-packages.},
archivePrefix = {arXiv},
arxivId = {1907.00520},
author = {Gao, Fei and Wang, Luqi and Zhou, Boyu and Zhou, Xin and Pan, Jie and Shen, Shaojie},
doi = {10.1109/tro.2020.2993215},
eprint = {1907.00520},
file = {:home/matias/Documents/Mendeley Desktop/Gao et al/2020/Gao et al. - 2020 - Teach-Repeat-Replan A Complete and Robust System for Aggressive Flight in Complex Environments.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
pages = {1--20},
title = {{Teach-Repeat-Replan: A Complete and Robust System for Aggressive Flight in Complex Environments}},
url = {http://arxiv.org/abs/1907.00520},
year = {2020}
}
@article{Hutter2017,
abstract = {This paper provides a system overview about ANYmal, a quadrupedal robot developed for operation in harsh environments. The 30 kg, 0.5m tall robotic dog was built in a modular way for simple maintenance and user-friendly handling, while focusing on high mobility and dynamic motion capability. The system is tightly sealed to reach IP67 standard and protected to survive falls. Rotating lidar sensors in the front and back are used for localization and terrain mapping and compact force sensors in the feet provide accurate measurements about the contact situations. The variable payload, such as a modular pan-tilt head with a variety of inspection sensors, can be exchanged depending on the application. Thanks to novel, compliant joint modules with integrated electronics, ANYmal is precisely torque controllable and very robust against impulsive loads during running or jumping. In a series of experiments we demonstrate that ANYmal can execute various climbing maneuvers, walking gaits, as well as a dynamic trot and jump. As special feature, the joints can be fully rotated to switch between X- and O-type kinematic configurations. Detailed measurements unveil a low energy consumption of 280W during locomotion, which results in an autonomy of more than 2h.},
author = {Hutter, M. and Gehring, C. and Lauber, A. and Gunther, F. and Bellicoso, C. D. and Tsounis, V. and Fankhauser, P. and Diethelm, R. and Bachmann, S. and Bloesch, M. and Kolvenbach, H. and Bjelonic, M. and Isler, L. and Meyer, K.},
doi = {10.1080/01691864.2017.1378591},
file = {::},
issn = {15685535},
journal = {Advanced Robotics},
keywords = {Legged robot,autonomous navigation,field robotics,quadruped robot,series elastic actuation},
number = {17},
pages = {918--931},
publisher = {Taylor & Francis},
title = {{ANYmal - toward legged robots for harsh environments}},
url = {https://doi.org/10.1080/01691864.2017.1378591},
volume = {31},
year = {2017}
}
@article{Bouvrie2010,
abstract = {We propose a natural image representation, the neural response, motivated by the neuroscience of the visual cortex. The inner product defined by the neural response leads to a similarity measure between functions which we call the derived kernel. Based on a hierarchical architecture, we give a recursive definition of the neural response and associated derived kernel. The derived kernel can be used in a variety of application domains such as classification of images, strings of text and genomics data. {\textcopyright} SFoCM 2009.},
author = {Smale, S. and Rosasco, L. and Bouvrie, J. and Caponnetto, A. and Poggio, T.},
doi = {10.1007/s10208-009-9049-1},
file = {::},
issn = {16153383},
journal = {Foundations of Computational Mathematics},
keywords = {Computer vision,Kernels,Unsupervised learning},
number = {1},
pages = {67--91},
title = {{Mathematics of the neural response}},
volume = {10},
year = {2010}
}
@article{VallejosSanchez2011,
author = {{Vallejos Sanchez}, Paul Albert},
title = {{Metodolog{\'{i}}a de Dise{\~{n}}o de Robots Semi-Pasivos}},
url = {http://www.tesis.uchile.cl/tesis/uchile/2011/cf-vallejos_ps/html/index-frames.html %5Cnhttp://www.repositorio.uchile.cl/handle/2250/102566},
year = {2011}
}
@book{Lynch2017,
author = {Lynch, Kevin M. and Park, Frank C.},
doi = {10.1017/9781316661239},
file = {:home/matias/Documents/Mendeley Desktop/Lynch, Park/2017/Lynch, Park - 2017 - Modern Robotics Mechanics, Planning, and Control.pdf:pdf},
isbn = {0816057451},
issn = {1941-000X},
keywords = {Artificial intelligence,Control engineering,Educational robotics,Engineering,Motion planning,Networked control system,Nonlinear control,Robot,Robot kinematics,Robotics,Teleoperation},
pages = {532},
publisher = {Cambridge University Press},
title = {{Modern Robotics: Mechanics, Planning, and Control}},
year = {2017}
}
@article{Murphy2011,
abstract = {LittleDog is a small four-legged robot designed for research on legged locomotion. The LittleDog platform was designed by Boston Dynamics with funding from DARPA to enable rapid advances in the state of the art of rough-terrain locomotion algorithms. In addition to providing a fleet of 12 robots with baseline software and development tools, LittleDog served as a cross-team common platform that allowed direct comparison of results across multiple research teams. Here we report the details of this robotic system. {\textcopyright} 2011 The Author(s).},
author = {Murphy, Michael P. and Saunders, Aaron and Moreira, Cassie and Rizzi, Alfred A. and Raibert, Marc},
doi = {10.1177/0278364910387457},
file = {::},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Quadruped,hardware,legged locomotion,platform,rough terrain},
number = {2},
pages = {145--149},
title = {{The LittleDog robot}},
volume = {30},
year = {2011}
}
@book{Absil2007,
abstract = {Determining human exposure to suspended particulate concentrations requires measurements that quantify different particle properties in microenvironments where people live, work, and play. Particle mass, size, and chemical composition are important exposure variables, and these are typically measured with time-integrated samples on filters that are later submitted to laboratory analyses. This requires substantial sample handling, quality assurance, and data reduction. Newer technologies are being developed that allow in-situ, time-resolved measurements for mass, carbon, sulfate, nitrate, particle size, and other variables. These are large measurement systems that are more suitable for fixed monitoring sites than for personal applications. Human exposure studies need to be designed to accomplish specific objectives rather than to serve too many purposes. Resources need to be divided among study design, field sampling, laboratory analysis, quality assurance, data management, and data analysis phases. Many exposure projects allocated too little to the non-measurement activities. {\textcopyright} 2002 Elsevier Science Ltd. All rights reserved.},
author = {Chow, Judith C. and Engelbrecht, Johann P. and Freeman, Natalie C.G. and {Hisham Hashim}, Jamal and Jantunen, Matti and Michaud, Jon Pierre and {De Tejada}, Sandra Saenz and Watson, John G. and Wei, Fusheng and Wilson, William E. and Yasuno, Mayayuki and Zhu, Tan},
booktitle = {Chemosphere},
doi = {10.1016/S0045-6535(02)00233-3},
file = {::},
isbn = {0045-6535},
issn = {00456535},
keywords = {Air quality,Human exposure,Network design,PM10,PM2.5},
number = {9},
pages = {873--901},
pmid = {12492156},
publisher = {Princeton University Press},
title = {{Chapter one: Exposure measurements}},
url = {http://press.princeton.edu/books/absil/},
volume = {49},
year = {2002}
}
@article{Martins2015,
abstract = {This paper presents a novel compliant spring system designed to be attached to a conventional robotics servo motor, turning it into a series elastic actuator (SEA). The system is composed by only two mechanical parts: a torsional polyurethane spring and a round aluminum support for link attachment. The polyurethane spring, had its design derived from a iterative FEM-based optimization process. A magnetometer based circuit is used to measure angular displacement and communicate it through a RS485 bus protocol.},
author = {Martins, Leandro Tom{\'{e}} and {Arend Tatsch}, Christopher A. and Maciel, Eduardo Henrique and Gerndt, Reinhard and {Da Silva Guerra}, Rodrigo},
doi = {10.1016/j.ifacol.2015.12.019},
file = {::},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {modular design,passive compliance,robotics,series elastic actuator},
number = {19},
pages = {112--117},
publisher = {Elsevier B.V.},
title = {{A Polyurethane-based Compliant Element for Upgrading Conventional Servos into Series Elastic Actuators}},
url = {http://dx.doi.org/10.1016/j.ifacol.2015.12.019},
volume = {48},
year = {2015}
}
@inproceedings{Kajita1991,
abstract = {This paper introduces a new control mthod tor biped locomotion on rugged terrain. We assume the ideal robot model which has massless legs. When a particular constraint control is applied to the ideal biped, the dynamics of the robot becoms completely linear. We call such motion the Linear Inverted Pendulum Mode and use it to develop the control scheme of the biped walking on rugged terrain. In the linear inverted pendulum mode, walkiig on a particular rugged ground is shown to be equivalent to walking on a level ground. It is ako shown that the additional use of the ankle torque makes our control scheme robust and applicable to a real biped robot with mass legs. To ascertain our theory, we built an experimental biped robot. As the basic experi- mnt, our robot achieved stable walking on a level ground.},
author = {Kajita, S. and Tani, K.},
booktitle = {Fifth International Conference on Advanced Robotics 'Robots in Unstructured Environments},
doi = {10.1109/icar.1991.240688},
isbn = {0-7803-0078-5},
keywords = {ankle torque,constraint control,dynamic biped locomotion,dynamics,linear dynamics,linear inverted pendulum mode,mobile robots,position control ankle torque,position controlangular velocity,robot,rugged terrain},
pages = {741--746 vol.1},
publisher = {IEEE},
title = {{Study of dynamic biped locomotion on rugged terrain-theory and basic experiment}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=240688 http://ieeexplore.ieee.org/document/240688/},
year = {2002}
}
@article{Naseer2014,
abstract = {Image-based localization is an important problem in robotics and an integral part of visual mapping and navigation systems. An approach to robustly match images to previously recorded ones must be able to cope with seasonal changes especially when it is supposed to work reliably over long periods of time. In this paper, we present a novel approach to visual localization of mobile robots in outdoor environments, which is able to deal with substantial seasonal changes. We formulate image matching as a minimum cost flow problem in a data association graph to effectively exploit sequence information. This allows us to deal with non-matching image sequences that result from temporal occlusions or from visiting new places. We present extensive experimental evaluations under substantial seasonal changes. Our approach achieves accurate matching across seasons and outperforms existing state-of-the-art methods such as FABMAP2 and SeqSLAM.},
author = {Naseer, Tayyab and Spinello, Luciano and Burgard, Wolfram and Stachniss, Cyrill},
file = {:home/matias/Documents/Mendeley Desktop/Naseer et al/2014/Naseer et al. - 2014 - Robust visual robot localization across seasons using network flows.pdf:pdf},
isbn = {9781577356806},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Robotics},
pages = {2564--2570},
title = {{Robust visual robot localization across seasons using network flows}},
volume = {4},
year = {2014}
}
@article{Remy2012,
abstract = {This paper introduces StarlETH, a compliant quadrupedal robot that is de- signed to study fast, efficient, and versatile locomotion. The platform is fully actuated with high compliant series elastic actuation, making the system torque controllable and at the same time well suited for highly dynamic maneuvers. We additionally emphasize key elements of a powerful real time control and simulation environment. The work is concluded with a number of experiments that demonstrate the performance of the presented hardware and controllers. Copyright {\textcopyright} 2012 by World Scientific Publishing Co. Pte. Ltd.},
author = {Hutter, Marco and Gehring, Christian and Bloesch, Michael and Hoepflinger, Mark A. and Remy, C. David and Siegwart, Roland},
doi = {10.1142/9789814415958_0062},
file = {::},
isbn = {9789814415941},
journal = {Adaptive Mobile Robotics - Proceedings of the 15th International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines, CLAWAR 2012},
keywords = {Compliant quadruped,Legged locomotion,SEA,Torque control},
pages = {483--490},
title = {{Starleth: A compliant quadrupedal robot for fast, efficient, and versatile locomotion}},
url = {http://infoscience.epfl.ch/record/181042},
year = {2012}
}
@inproceedings{Davison2003,
abstract = {Ego-motion estimation for an agile single camera moving through general, unknown scenes becomes a much more challenging problem when real-time performance is required rather than under the off-line processing conditions under which most successful structure from motion work has been achieved. This task of estimating camera motion from measurements of a continuously expanding set of self-mapped visual features is one of a class of problems known as Simultaneous Localisation and Mapping (SLAM) in the robotics community, and we argue that such real-time mapping research, despite rarely being camera-based, is more relevant here than off-line structure from motion methods due to the more fundamental emphasis placed on propagation of uncertainty. We present a top-down Bayesian framework for single-camera localisation via mapping of a sparse set of natural features using motion modelling and an information-guided active measurement strategy, in particular addressing the difficult issue of real-time feature initialisation via a factored sampling approach. Real-time handling of uncertainty permits robust localisation via the creating and active measurement of a sparse map of landmarks such that regions can be re-visited after periods of neglect and localisation can continue through periods when few features are visible. Results are presented of real-time localisation for a hand-waved camera with very sparse prior scene knowledge and all processing carried out on a desktop PC.},
author = {Davison, Andrew J.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/iccv.2003.1238654},
isbn = {0-7695-1950-4},
issn = {15505499},
keywords = {Localizacion,SLAM,vision},
pages = {1403--1410},
publisher = {IEEE},
title = {{Real-time simultaneous localisation and mapping with a single camera}},
url = {http://ieeexplore.ieee.org/search/selected.jsp http://ieeexplore.ieee.org/document/1238654/},
volume = {2},
year = {2003}
}
@article{Zefran1999,
abstract = {The set of rigid-body motions forms a Lie group called SE(3), the special Euclidean group in three dimensions. In this paper, we investigate possilble choices of Riemannian metrics and affine connections on SE(3) for applications to kinematic analysis and robot-trajectory planning. In the first part of the paper, we study metrics whose geodesics are screw motions. We prove that no Riemannian metrics call have such geodesics, and we show that the metrics whose geodesics are screw motions form a two-parameter family of semi-Riemannan metrics. In the second part of the paper, we investigate affine connections which through the covariant derivative give the correct expression for the acceleration of a rigid body. We prove that there is a unique symmetric connection with this property. Furthermore, we show that there is a family of Riemannian metrics that are compatible with such a connection. These metrics are products of the bi-invariant metric on the group of rotations and a positive-definite constant metric on the group of translations. {\textcopyright} 1999, SAGE Publications. All rights reserved.},
author = {{\v{Z}}efran, Milo{\v{s}} and Kumar, Vijay and Croke, Christopher},
doi = {10.1177/027836499901800208},
file = {::},
issn = {17413176},
journal = {The International Journal of Robotics Research},
month = {feb},
number = {2},
pages = {1--16},
title = {{Metrics and Connections for Rigid-Body Kinematics}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/027836499901800208},
volume = {18},
year = {1999}
}
@article{Crick2011,
abstract = {We present rosbridge, a middleware abstraction layer which provides robotics technology with a standard, minimalist applications development framework accessible to applications programmers who are not themselves roboticists. Rosbridge provides a simple, socket-based programmatic access to robot interfaces and algorithms provided (for now) by ROS, the open-source “Robot Operating System”, the current state-of-the-art in robot middleware. In particular, it facilitates the use of web technologies such as Javascript for the purpose of broadening the use and usefulness of robotic technology. We demonstrate potential applications in the interface design, education, human-robot interaction and remote laboratory environments.},
author = {Crick, Christopher and Jay, Graylin and Osentoski, Sarah},
file = {::},
journal = {Proceedings of the 15th {\ldots}},
pages = {1--12},
title = {{Rosbridge: Ros for Non-Ros Users}},
url = {http://cs.okstate.edu/$\sim$chriscrick/Crick11b.pdf},
year = {2011}
}
@book{Nilsson2002,
abstract = {Artificial intelligence (AI) is a field within computer science that is attempting to build enhanced intelligence into computer systems. This book traces the history of the subject, from the early dreams of eighteenth-century (and earlier) pioneers to the more successful work of today's AI engineers. AI is becoming more and more a part of everyone's life. The technology is already embedded in face-recognizing cameras, speech-recognition software, Internet search engines, and health-care robots, among other applications. The book's many diagrams and easy-to-understand descriptions of AI programs will help the casual reader gain an understanding of how these and other AI systems actually work. Its thorough (but unobtrusive) end-of-chapter notes containing citations to important source materials will be of great use to AI scholars and researchers. This book promises to be the definitive history of a field that has captivated the imaginations of scientists, philosophers, and writers for centuries.},
author = {Nilsson, Nils J.},
booktitle = {The Quest for Artificial Intelligence: A History of Ideas and Achievements},
doi = {10.1017/CBO9780511819346},
file = {::},
isbn = {9780511819346},
issn = {00289604},
number = {25},
pages = {1--562},
pmid = {12096616},
title = {{The quest for artificial intelligence: A history of ideas and achievements}},
volume = {139},
year = {2011}
}
@article{Hornung2014,
abstract = {Accurate and reliable localization is a prerequisite for autonomously performing high-level tasks with humanoid robots. In this paper, we present a probabilistic localization method for humanoid robots navigating in arbitrary complex indoor environments using only onboard sensing, which is a challenging task. Inaccurate motion execution of biped robots leads to an uncertain estimate of odometry, and their limited payload constrains perception to observations from lightweight and typically noisy sensors. Additionally, humanoids do not walk on flat ground only and perform a swaying motion while walking, which requires estimating a full 6D torso pose. We apply Monte Carlo localization to globally determine and track a humanoid's 6D pose in a given 3D world model, which may contain multiple levels and staircases. We present an observation model to integrate range measurements from a laser scanner or a depth camera as well as attitude data and information from the joint encoders. To increase the localization accuracy, e.g., while climbing stairs, we propose a further observation model and additionally use monocular vision data in an improved proposal distribution. We demonstrate the effectiveness of our methods in extensive real-world experiments with a Nao humanoid. As the experiments illustrate, the robot is able to globally localize itself and accurately track its 6D pose while walking and climbing stairs. {\textcopyright} 2014 World Scientific Publishing Company.},
author = {Hornung, Armin and O{\ss}wald, Stefan and Maier, Daniel and Bennewitz, Maren},
doi = {10.1142/S0219843614410023},
issn = {02198436},
journal = {International Journal of Humanoid Robotics},
keywords = {3D,RGB-D,laser,localization,navigation,range sensing,vision},
month = {jun},
number = {2},
pages = {1441002},
title = {{Monte carlo localization for humanoid robot navigation in complex indoor environments}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0219843614410023},
volume = {11},
year = {2014}
}
@article{Mazuran2016,
abstract = {For long-term operations, graph-based simultaneous localization and mapping (SLAM) approaches require nodes to be marginalized in order to control the computational cost. In this paper, we present a method to recover a set of nonlinear factors that best represents the marginal distribution in terms of Kullback-Leibler divergence. The proposed method, which we call nonlinear factor recovery (NFR), estimates both the mean and the information matrix of the set of nonlinear factors, where the recovery of the latter is equivalent to solving a convex optimization problem. NFR is able to provide either the dense distribution or a sparse approximation of it. In contrast to previous algorithms, our method does not necessarily require a global linearization point and can be used with any nonlinear measurement function. Moreover, we are not restricted to only using tree-based sparse approximations and binary factors, but we can include any topology and correlations between measurements. Experiments performed on several publicly available datasets demonstrate that our method outperforms the state of the art with respect to the Kullback-Leibler divergence and the sparsity of the solution.},
author = {Mazuran, Mladen and Burgard, Wolfram and Tipaldi, Gian Diego},
doi = {10.1177/0278364915581629},
file = {:home/matias/Documents/Mendeley Desktop/Mazuran, Burgard, Tipaldi/2016/Mazuran, Burgard, Tipaldi - 2016 - Nonlinear factor recovery for long-term SLAM.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Mobile robotics,SLAM,graphical models,localization,mapping,nonlinear optimization},
number = {1-3},
pages = {50--72},
title = {{Nonlinear factor recovery for long-term SLAM}},
url = {http://journals.sagepub.com/doi/10.1177/0278364915581629},
volume = {35},
year = {2016}
}
@article{Wong2020a,
abstract = {Simultaneous trajectory estimation and mapping (STEAM) is a method for continuous-time trajectory estimation in which the trajectory is represented as a Gaussian Process (GP). Previous formulations of STEAM used a GP prior that assumed either white-noise-on-acceleration (WNOA) or white-noise-on-jerk (WNOJ). However, previous work did not provide a principled way to choose the continuous-time motion prior or its parameters on a real robotic system. This letter derives a novel data-driven motion prior where ground truth trajectories of a moving robot are used to train a motion prior that better represents the robot's motion. In this approach, we use a prior where latent accelerations are represented as a GP with a Mat{\'{e}}rn covariance function and draw a connection to the Singer acceleration model. We then formulate a variation of STEAM using this new prior. We train the WNOA, WNOJ, and our new latent-force prior and evaluate their performance in the context of both lidar localization and lidar odometry of a car driving along a 20 km route, where we show improved state estimates compared to the two previous formulations.},
author = {Wong, Jeremy N. and Yoon, David J. and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/LRA.2020.2969153},
file = {:home/matias/Documents/Mendeley Desktop/Wong et al/2020/Wong et al. - 2020 - A Data-Driven Motion Prior for Continuous-Time Trajectory Estimation on SE(3).pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,localization},
number = {2},
pages = {1429--1436},
title = {{A Data-Driven Motion Prior for Continuous-Time Trajectory Estimation on SE(3)}},
volume = {5},
year = {2020}
}
@book{Barfoot2017,
abstract = {A key aspect of robotics today is estimating the state, such as position and orientation, of a robot as it moves through the world. Most robots and autonomous vehicles depend on noisy data from sensors such as cameras or laser rangefinders to navigate in a three-dimensional world. This book presents common sensor models and practical advice on how to carry out state estimation for rotations and other state variables. It covers both classical state estimation methods such as the Kalman filter, as well as important modern topics such as batch estimation, the Bayes filter, sigmapoint and particle filters, robust estimation for outlier rejection, and continuous-time trajectory estimation and its connection to Gaussian-process regression. The methods are demonstrated in the context of important applications such as point-cloud alignment, pose-graph relaxation, bundle adjustment, and simultaneous localization and mapping. Students and practitioners of robotics alike will find this a valuable resource.},
author = {Barfoot, Timothy D.},
booktitle = {State Estimation for Robotics},
doi = {10.1017/9781316671528},
file = {:home/matias/Documents/Mendeley Desktop/Barfoot/2017/Barfoot - 2017 - State estimation for robotics.pdf:pdf},
isbn = {9781316671528},
pages = {1--368},
publisher = {Cambridge University Press},
title = {{State estimation for robotics}},
year = {2017}
}
@incollection{Bloesch2017b,
abstract = {Control algorithms for legged robots rely on accurate and fail-safe ego-motion estimation in order to keep balance and perform desired tasks. To this end, the robot must integrate the measurements from different sensor modalities into a single consistent state estimation. In particular, the estimation process must provide estimates of the gravity direction and the local velocities of the robot since those quantities are essential for stabilizing the system and to counteract external disturbances. In comparison to other types of robots, legged robots interact through intermittent contacts with the surrounding. This provides the system with an additional source of information which can be leveraged in order to improve the state estimation. Since there is no one-size-fits-all solution, the following chapter will provide an insight into the different concepts and algorithms by discussing state-of-the-art approaches and examples. This should enable the reader to design a tailored state estimation solution to his or her specific robot and environment.},
address = {Dordrecht},
author = {Bloesch, Michael and Hutter, Marco},
booktitle = {Humanoid Robotics: A Reference},
doi = {10.1007/978-94-007-7194-9_69-1},
file = {::},
isbn = {978-94-007-6045-5},
keywords = {contact constraints,kalman,odometry,sensor fusion,state estimation},
pages = {1--29},
publisher = {Springer Netherlands},
title = {{Technical Implementations of the Sense of Balance}},
url = {http://link.springer.com/10.1007/978-94-007-7194-9_69-1},
year = {2017}
}
@article{Jaeger2010,
abstract = {German National Research Center for Information Technology, 2001},
author = {Holland, Karen},
doi = {10.1054/nepr.2001.0035},
file = {::},
isbn = {0-7803-9048-2},
issn = {14715953},
journal = {Nurse Education in Practice},
number = {4},
pages = {221--223},
pmid = {19036266},
title = {{Report from Nurse Education tomorrow 2001: 12th Annual International Participative Conference - For education in health Care, Grey College, University of Durham, UK. 7-9 September 2001}},
volume = {1},
year = {2001}
}
@inproceedings{Zbontar2015,
abstract = {We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61% on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.},
archivePrefix = {arXiv},
arxivId = {1409.4326},
author = {{\v{Z}}bontar, Jure and {Le Cun}, Yann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298767},
eprint = {1409.4326},
file = {::},
isbn = {9781467369640},
issn = {10636919},
number = {1},
pages = {1592--1599},
title = {{Computing the stereo matching cost with a convolutional neural network}},
url = {http://arxiv.org/abs/1409.4326},
volume = {07-12-June},
year = {2015}
}
@article{Zhang1997,
abstract = {Almost all problems in computer vision are related in one form or another to the problem of estimating parameters from noisy data. In this tutorial, we present what is probably the most commonly used techniques for parameter estimation. These include linear least-squares (pseudo-inverse and eigen analysis); orthogonal least-squares; gradient-weighted least-squares; bias-corrected renormalization; Kalman filtering; and robust techniques (clustering, regression diagnostics, M-estimators, least median of squares). Particular attention has been devoted to discussions about the choice of appropriate minimization criteria and the robustness of the different techniques. Their application to conic fitting is described.},
author = {Zhang, Zhengyou},
doi = {10.1016/S0262-8856(96)01112-2},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Bias correction,Kalman filtering,Least-squares,Parameter estimation,Robust regression},
number = {1},
pages = {59--76},
title = {{Parameter estimation techniques: A tutorial with application to conic fitting}},
volume = {15},
year = {1997}
}
@article{Topping2016,
abstract = {{\textcopyright} 2016 SPIE. This paper explores the applicability of a Linear Quadratic Regulator (LQR) controller design to the problem of bipedal stance on the Minitaur [1] quadrupedal robot. Restricted to the sagittal plane, this behavior exposes a three degree of freedom (DOF) double inverted pendulum with extensible length that can be projected onto the familiar underactuated revolute-revolute "Acrobot" model by assuming a locked prismatic DOF, and a pinned toe. While previous work has documented the successful use of local LQR control to stabilize a physical Acrobot, simulations reveal that a design very similar to those discussed in the past literature cannot achieve an empirically viable controller for our physical plant. Experiments with a series of increasingly close physical facsimiles leading to the actual Minitaur platform itself corroborate and underscore the physical Minitaur platform corroborate and underscore the implications of the simulation study. We conclude that local LQR-based linearized controller designs are too fragile to stabilize the physical Minitaur platform around its vertically erect equilibrium and end with a brief assessment of a variety of more sophisticated nonlinear control approaches whose pursuit is now in progress.},
author = {Topping, T. Turner and Vasilopoulos, Vasileios and De, Avik and Koditschek, Daniel E.},
doi = {10.1117/12.2231103},
file = {::},
isbn = {9781510600782},
issn = {1996756X},
journal = {Unmanned Systems Technology XVIII},
number = {May},
pages = {98370H},
title = {{Towards bipedal behavior on a quadrupedal platform using optimal control}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2231103},
volume = {9837},
year = {2016}
}
@misc{Botvinick2012,
abstract = {Recent developments in decision-making research are bringing the topic of planning back to center stage in cognitive science. This renewed interest reopens an old, but still unanswered question: how exactly does planning happen? What are the underlying information processing operations and how are they implemented in the brain? Although a range of interesting possibilities exists, recent work has introduced a potentially transformative new idea, according to which planning is accomplished through probabilistic inference. {\textcopyright} 2012.},
author = {Botvinick, Matthew and Toussaint, Marc},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/j.tics.2012.08.006},
file = {:home/matias/Documents/Mendeley Desktop/Botvinick, Toussaint/2012/Botvinick, Toussaint - 2012 - Planning as inference(2).pdf:pdf},
issn = {13646613},
number = {10},
pages = {485--488},
pmid = {22940577},
title = {{Planning as inference}},
volume = {16},
year = {2012}
}
@inproceedings{Wu2015,
abstract = {In this paper, we present a square-root inverse sliding window filter (SR-ISWF) for vision-aided inertial navigation systems (VINS). While regular inverse filters suffer from numerical issues, employing their square-root equivalent enables the usage of single-precision number representations, thus achieving considerable speed ups as compared to double-precision alternatives on resource-constrained mobile platforms. Besides a detailed description of the SR-ISWF for VINS, which focuses on the numerical procedures that enable exploiting the problem's structure for gaining in efficiency, this paper presents a thorough validation of the algorithm's processing requirements and achieved accuracy. In particular, experiments are conducted using a commercial-grade cell phone, where the proposed algorithm is shown to achieve the same level of estimation accuracy, when compared to state-of-the-art VINS algorithms, with significantly higher speed.},
author = {Wu, Kejian J. and Ahmed, Ahmed M. and Georgiou, Georgios A. and Roumeliotis, Stergios I.},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.008},
isbn = {9780992374716},
issn = {2330765X},
title = {{A square root inverse filter for efficient vision-aided inertial navigation on mobile devices}},
volume = {11},
year = {2015}
}
@inproceedings{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1710.09829},
file = {::},
issn = {10495258},
pages = {3857--3867},
title = {{Dynamic routing between capsules}},
url = {https://research.google.com/pubs/pub46351.html},
volume = {2017-Decem},
year = {2017}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
number = {2},
pages = {91--110},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@article{Gan2020,
abstract = {This article develops a Bayesian continuous 3D semantic occupancy map from noisy point clouds by generalizing the Bayesian kernel inference model for building occupancy maps, a binary problem, to semantic maps, a multi-class problem. The proposed method provides a unified probabilistic model for both occupancy and semantic probabilities and nicely reverts to the original occupancy mapping framework when only one occupied class exists in obtained measurements. The Bayesian spatial kernel inference relaxes the independent grid assumption and brings smoothness and continuity to the map inference, enabling to exploit local correlations present in the environment and increasing the performance. The accompanying software uses multi-threading and vectorization, and runs at about 2 $\mathrm{Hz}$ on a laptop CPU. Evaluations using multiple sequences of stereo camera and LiDAR datasets show that the proposed method consistently outperforms current baselines. We also present a qualitative evaluation using data collected with a bipedal robot platform on the University of Michigan - North Campus.},
archivePrefix = {arXiv},
arxivId = {1909.04631},
author = {Gan, Lu and Zhang, Ray and Grizzle, Jessy W. and Eustice, Ryan M. and Ghaffari, Maani},
doi = {10.1109/LRA.2020.2965390},
eprint = {1909.04631},
file = {:home/matias/Documents/Mendeley Desktop/Gan et al/2020/Gan et al. - 2020 - Bayesian Spatial Kernel Smoothing for Scalable Dense Semantic Mapping.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mapping,RGB-D perception,range sensing,semantic scene understanding},
number = {2},
pages = {790--797},
title = {{Bayesian Spatial Kernel Smoothing for Scalable Dense Semantic Mapping}},
volume = {5},
year = {2020}
}
@article{Maass2002,
abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
author = {Maass, Wolfgang and Natschl{\"{a}}ger, Thomas and Markram, Henry},
doi = {10.1162/089976602760407955},
file = {::},
isbn = {0899-7667 (Print)\r0899-7667 (Linking)},
issn = {08997667},
journal = {Neural Computation},
keywords = {*Models, Neurological,*Neural Networks (Computer),Action Potentials/physiology,Computer Simulation,Computer Systems,Computers,Neurons/*physiology,Support, Non-U.S. Gov't},
number = {11},
pages = {2531--2560},
pmid = {12433288},
title = {{Real-time computing without stable states: A new framework for neural computation based on perturbations}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=12433288},
volume = {14},
year = {2002}
}
@article{Daniilidis1990,
abstract = {Fundamental instabilities have been observed in the performance of the majority of the algorithms for three dimensional motion estimation from two views. Many geometric and intuitive interpretations have been offered to explain the error sensitivity of the estimated parameters. In this contribution, the importance of the form of the error norm to be minimized with respect to the motion parameters is addressed. The error norms used by the existing algorithms are described in a unifying notation, and a geometric interpretation of them is given. Then, for the continuous case of pure translational motion it is proved that the minimization of the objective function leading to an eigenvector solution suffers from a crucial instability. The analyticity of the results allows the examination of error sensitivity in terms of the translation direction, the viewing angle and the distance of the moving object from the camera. A norm possessing a reasonable geometric interpretation in the image plane is proposed to eliminate the effects of the instability mentioned above. Due to the high nonlinearity of this norm it has not been possible to prove explicitly its stabilizing role. It is shown by analytical means that a simplification of this norm - leading to a closed form solution - has undesirable properties. {\textcopyright} 1990 Butterworth-Heinemann Ltd.},
author = {Daniilidis, Konstantinos and Nagel, Hans Hellmut},
doi = {10.1016/0262-8856(90)80006-F},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {error sensitivity,motion estimation,motion parameters},
number = {4},
pages = {297--303},
title = {{Analytical results on error sensitivity of motion estimation from two views}},
volume = {8},
year = {1990}
}
@article{Camara2020,
author = {Camara, Luis G and Pivoˇ, Tom{\'{a}}{\v{s}}},
file = {:home/matias/Documents/Mendeley Desktop/Camara, Pivoˇ/2020/Camara, Pivoˇ - 2020 - Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition A Accurate and Robust Teach and Rep.pdf:pdf},
number = {March},
title = {{Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition : A Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition : A CNN Approach}},
year = {2020}
}
@article{Li2020,
abstract = {The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.},
archivePrefix = {arXiv},
arxivId = {2003.02327},
author = {Li, Yimeng and Kosecka, Jana},
eprint = {2003.02327},
file = {:home/matias/Documents/Mendeley Desktop/Li, Kosecka/2020/Li, Kosecka - 2020 - Learning View and Target Invariant Visual Servoing for Navigation.pdf:pdf},
isbn = {9781728173955},
title = {{Learning View and Target Invariant Visual Servoing for Navigation}},
url = {http://arxiv.org/abs/2003.02327},
year = {2020}
}
@incollection{Martins2015a,
abstract = {In this article we present a compact and modular device designed to allow a conventional stiff servo actuator to be easily upgraded into a series elastic actuator (SEA). This is a low cost, open source and open hardware solution including mechanical CAD drawings, circuit schematics, board designs and firmware code. We present a complete overview of the project as well as a case study where we show the device being employed as an upgrade to add compliance to the knee joints of an existing humanoid robot design.},
author = {Martins, Leandro Tome and {De Mendonca Pretto}, Roberta and Gerndt, Reinhard and {Da Silva Guerra}, Rodrigo},
booktitle = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
doi = {10.1007/978-3-319-18615-3_57},
file = {::},
isbn = {9783319186146},
issn = {03029743},
keywords = {Passive compliance,Series elastic actuator},
pages = {701--708},
title = {{Design of a modular series elastic upgrade to a robotics actuator}},
url = {http://link.springer.com/10.1007/978-3-319-18615-3 http://link.springer.com/10.1007/978-3-319-18615-3_57},
volume = {8992},
year = {2015}
}
@article{Inc.2013,
abstract = {This product specification provides advanced information regarding the electrical specification and design related information for the MPU-6000™ and MPU-6050™ MotionTracking™ devices, collectively called the MPU-60X0™ or MPU™. Electrical characteristics are based upon design analysis and simulation results only. Specifications are subject to change without notice. Final specifications will be updated based upon characterization of production silicon. For references to register map and descriptions of individual registers, please refer to the MPU-6000/MPU-6050 Register Map and Register Descriptions document.},
author = {Inc., InvenSense},
file = {::},
journal = {InvenSense Inc.},
number = {408},
pages = {1--57},
title = {{MPU-6000 and MPU-6050 Product Specification}},
url = {https://www.cdiweb.com/datasheets/invensense/MPU-6050_DataSheet_V3 4.pdf},
volume = {1},
year = {2013}
}
@article{Olson2010,
abstract = {Knowing the time at which sensors acquired data is critical to the proper processing and interpretation of that data, particularly for mobile robots attempting to project sensor data into a consistent coordinate frame. Unfortunately, many popular commercial sensors provide no support for synchronization, rendering conventional synchronization algorithms useless. In this paper, we describe a passive synchronization algorithm that can significantly reduce timing error versus naively time-stamping sensor data when it arrives at the host. It is passive in the sense that the algorithm requires no special cooperation from the sensor. Our method estimates the timing jitter induced by hosts, and thus does not require a real-time operating system. We rigorously derive and characterize the method, proving that it can only improve upon the synchronization accuracy of the standard approach. {\textcopyright}2010 IEEE.},
author = {Olson, Edwin},
doi = {10.1109/IROS.2010.5650579},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
pages = {1059--1064},
title = {{A passive solution to the sensor synchronization problem}},
year = {2010}
}
@misc{Xie2017,
abstract = {Obstacle avoidance is a fundamental requirement for autonomous robots which operate in, and interact with, the real world. When perception is limited to monocular vision avoiding collision becomes significantly more challenging due to the lack of 3D information. Conventional path planners for obstacle avoidance require tuning a number of parameters and do not have the ability to directly benefit from large datasets and continuous use. In this paper, a dueling architecture based deep double-Q network (D3QN) is proposed for obstacle avoidance, using only monocular RGB vision. Based on the dueling and double-Q mechanisms, D3QN can efficiently learn how to avoid obstacles in a simulator even with very noisy depth information predicted from RGB image. Extensive experiments show that D3QN enables twofold acceleration on learning compared with a normal deep Q network and the models trained solely in virtual environments can be directly transferred to real robots, generalizing well to various new environments with previously unseen dynamic objects.},
archivePrefix = {arXiv},
arxivId = {1706.09829},
author = {Xie, Linhai and Wang, Sen and Markham, Andrew and Trigoni, Niki},
booktitle = {arXiv},
eprint = {1706.09829},
file = {:home/matias/Documents/Mendeley Desktop/Xie et al/2017/Xie et al. - 2017 - Towards monocular vision based obstacle avoidance through deep reinforcement learning.pdf:pdf},
title = {{Towards monocular vision based obstacle avoidance through deep reinforcement learning}},
year = {2017}
}
@article{Martinez-Carranza2012,
abstract = {We describe a method for visual odometry using a single camera based on an EKF framework. Previous work has shown that filtering based approaches can achieve accuracy performance comparable to that of optimisation methods providing that large numbers of features are used. However, computational requirements are signicantly increased and frame rates are low. We address this by employing higher level structure - in the form of planes - to efficiently parameterise features and so reduce the filter state size and computational load. Moreover, we extend a 1-point RANSAC outlier rejection method to the case of features lying on planes. Results of experiments with both simulated and real-world data demonstrate that the method is effective, achieving comparable accuracy whilst running at significantly higher frame rates. {\textcopyright} 2012 IEEE.},
author = {Mart{\'{i}}nez-Carranza, Jos{\'{e}} and Calway, Andrew},
doi = {10.1109/ICRA.2012.6225100},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5210--5215},
title = {{Efficient visual odometry using a structure-driven temporal map}},
year = {2012}
}
@inproceedings{Burri2016,
abstract = {In this paper we present a new estimation algorithm that allows for the combination of information from any number of process and measurement models. This adds more flexibility to the design of the estimator and in our case avoids the need for state augmentation. We achieve this by adapting the maximum likelihood formulation of the Kalman Filter, and thereby represent all measurement models as residuals. Posing the problem in this form allows for the straightforward integration of any number of (nonlinear) constraints between two subsequent states. To solve the optimization we present a closed form recursive set of equations that directly marginalizes out information that is not required, this leads to an efficient and generic implementation. The new algorithm is applied to parameter estimation on MAVs which have two dynamic models, the MAV dynamic model and the IMU-driven model. We show the benefits and limitations of the new filtering approach on a simplified simulation example and on a real MAV system.},
author = {Burri, Michael and Bloesch, Michael and Schindler, Dominik and Gilitschenski, Igor and Taylor, Zachary and Siegwart, Roland},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759483},
isbn = {9781509037629},
issn = {21530866},
month = {oct},
number = {October},
pages = {3124--3130},
publisher = {IEEE},
title = {{Generalized information filtering for MAV parameter estimation}},
url = {http://ieeexplore.ieee.org/document/7759483/},
volume = {2016-Novem},
year = {2016}
}
@article{Mazuran2018a,
abstract = {Mobile robot localization is a mature field that over the years has demonstrated its effectiveness and robust-ness. The majority of the approaches, however, rely on a globally consistent map, and localize on it in an absolute coordinate frame. This global consistency cannot be guaranteed when the map is estimated by the robot itself, and an error in the map will likely result in the failure of the localization subsystem. In this paper we introduce a novel paradigm for localization, namely relative topometric localization, by which we forgo the need for a globally consistent map. We adopt a graph-based representation of the environment, and estimate both the topological location on the graph and the relative metrical position with respect to it. We extensively evaluated our approach and tested it against Monte Carlo localization on both simulated and real data. The results show significant improvements in scenarios where there is no globally consistent map.},
author = {Mazuran, Mladen and Boniardi, Federico and Burgard, Wolfram and Tipaldi, Gian Diego},
doi = {10.1007/978-3-319-60916-4_25},
file = {:home/matias/Documents/Mendeley Desktop/Mazuran et al/2018/Mazuran et al. - 2018 - Relative Topometric Localization in Globally Inconsistent Maps.pdf:pdf},
pages = {435--451},
title = {{Relative Topometric Localization in Globally Inconsistent Maps}},
year = {2018}
}
@book{Murray1994,
abstract = {A Mathematical Introduction to Robotic Manipulation presents a mathematical formulation of the kinematics, dynamics, and control of robot manipulators. It uses an elegant set of mathematical tools that emphasizes the geometry of robot motion and allows a large class of robotic manipulation problems to be analyzed within a unified framework.The foundation of the book is a derivation of robot kinematics using the product of the exponentials formula. The authors explore the kinematics of open-chain manipulators and multifingered robot hands, present an analysis of the dynamics and control of robot systems, discuss the specification and control of internal forces and internal motions, and address the implications of the nonholonomic nature of rolling contact are addressed, as well.The wealth of information, numerous examples, and exercises make A Mathematical Introduction to Robotic Manipulation valuable as both a reference for robotics researchers and a text for students in advanced robotics courses.},
author = {Ellis-Pegler, R. B.},
booktitle = {New Zealand Medical Journal},
doi = {10.1.1.169.3957},
file = {::},
isbn = {9780849379819},
issn = {00288446},
number = {788},
pages = {847--849},
pmid = {2997678},
title = {{Aids 1985}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:A+Mathematical+Introduction+to+Robotic+Manipulation#0},
volume = {98},
year = {1985}
}
@inproceedings{Julier2001,
abstract = {This paper analyzes the properties of the full covariance simultaneous map building problem (SLAM). We prove that, even for the special case of a stationary vehicle (with no process noise) which uses a range-bearing sensor and has non-zero angular uncertainty, the full-covariance SLAM algorithm always yields an inconsistent map. We also show, through simulations, that these conclusions appear to extend to a moving vehicle with process noise. However, these inconsistencies only become apparent after several hundred beacon updates.},
author = {Julier, S. J. and Uhlmann, J. K.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2001.933280},
isbn = {0780365763},
issn = {10504729},
number = {Ic},
pages = {4238--4243},
publisher = {IEEE},
title = {{A counter example to the theory of simultaneous localization and map building}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=933280},
volume = {4},
year = {2001}
}
@article{Ibrahim2016,
abstract = {Localization and tracking of resources on construction jobsites are an emerging area where the location of materials, labor, and equipment is used to estimate productivity, measure project's progress and/or enhance jobsite safety. GPS has been widely used for outdoor tracking of construction operations. However, GPS is not suitable for indoor applications due to the lack of signal coverage; particularly inside tunnels or buildings. Several indoor localization research studies had been attempted, however such developments rely heavily on extensive external communication network infrastructures. These developments also are susceptible to electromagnetic interference in noisy construction jobsites. This paper presents indoor localization system using a microcontroller equipped with an inertial measurement unit (IMU). The IMU contains a cluster of sensors: accelerometer, gyroscope and magnetometer. The microcontroller uses a direct cosine matrix algorithm to fuse sensors data and calculate non-gravitational acceleration using nine-degrees-of-freedom motion equations. Current position is calculated based on measured acceleration and heading, while accounting for growing error in speed estimation utilizing jerk integration algorithm. Experimental results are presented to illustrate the relative effectiveness of the developed system, which is able to operate independently of any external aids and visibility conditions.},
author = {Ibrahim, Magdy and Moselhi, Osama},
doi = {10.1016/j.autcon.2016.05.006},
file = {::},
isbn = {0926-5805},
issn = {09265805},
journal = {Automation in Construction},
keywords = {Automated progress reporting,DCM,IMU,Indoor localization,Inertial navigation,Kalman filter},
pages = {13--20},
publisher = {Elsevier B.V.},
title = {{Inertial measurement unit based indoor localization for construction applications}},
url = {http://dx.doi.org/10.1016/j.autcon.2016.05.006},
volume = {71},
year = {2016}
}
@phdthesis{Zucchelli2002,
abstract = {Reconstructing the {3D} shape of a scene from its {2D} images is a problem that has attracted
a great deal of research. {3D} models are nowadays widely used for scientific
visualization, entertainment and engineering tasks. Most of the approaches developed
by the computer vision community can be roughly classified as feature based or flow
based, according to if the data they use is a set of features matches or an optical flow
field. While a dense optical flow field, due to its noisy nature, is not extremely suitable
for tracking, finding corresponding features between different views of large baseline is
still an open problem.
The system we develop in this thesis is of a hybrid type. We track sparse features
over sequences acquired at {25Hz} from an hand held camera. During the tracking good
features can be selected as those laying in high textured areas: this guarantees higher
precision in the estimation of features displacements. Such displacements are used to
approximate optical flow. We demonstrate that this approximation is a good one for
our working conditions. Using this approach we bypass the matching problem of stereo
and the complexity and time integration problems of the optical flow based reconstruction.
Time integration is obtained by an optimal predict-update procedure that merges
measurements by re-weighting by the respective covariance measurements.
Most of the research effort of this thesis is focused on the robust estimation of structure
and motion from a pair of images and the related optical flow field. We test first
a linear solution that has the appealing property of being of closed form but the problem
of returning biased estimates. We propose an non-linear refinement to the linear
estimator showing convergence properties and improvements in bias and variance. We
further extend the non-linear estimator to incorporate the optical flow covariance matrix
(maximum-likelihood) and, moreover, we show that, in the case of dense sequences, it
is possible to locally time integrate the reconstruction process for increased robustness.
We experimentally investigate the possibility of introducing geometrical constraints in
the structure and motion estimation. Such constraints are of bilinear type, i.e. planes,
lines and incidence of these primitives are used. For this purpose we present a new motion
based segmentation algorithm able to automatically detect and reconstruct planar
regions.
To asses the efficacy of our solution the algorithms were tested on a variety of real
and simulated sequences.},
author = {Zucchelli, M.},
booktitle = {Thesis},
doi = {10.1002/ardp.18581460304},
isbn = {9172833084},
issn = {15214184},
pages = {142},
school = {Royal Institute of Technology},
title = {{Optical flow based structure from motion optical flow based structure from motion}},
year = {2002}
}
@article{Chen2018,
abstract = {Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.},
author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
file = {:home/matias/Documents/Mendeley Desktop/Chen et al/2018/Chen et al. - 2018 - Learning to see in the dark.pdf:pdf},
journal = {arXiv},
title = {{Learning to see in the dark}},
year = {2018}
}
@inproceedings{Gouda2013,
abstract = {This paper is a survey work for designing a Vision based Simultaneous Localization and Mapping (VSLAM) humanoid robot to generate a map of an unknown environment. A lot of factors have to be considered while designing a VSLAM robot. Vision Sensors are very attractive for application in SLAM because of their rich sensory output and cost effectiveness. Different issues are involved in the problem of vision based SLAM and many different approaches exist in order to solve these issues. Similarly the type of environment determines the suitable feature extraction method. The main objective of this survey is to conduct a comparative study among the current vision sensing methods in terms of imaging systems used for performing VSLAM, feature extraction algorithms used in some recently published papers, and initialization of landmarks, and to figure out the best for our work. {\textcopyright} 2013 IEEE.},
author = {Gouda, Walaa and Gomaa, Walid and Ogawa, Tetsuji},
booktitle = {Proceedings of the 2013 2nd International Japan-Egypt Conference on Electronics, Communications and Computers, JEC-ECC 2013},
doi = {10.1109/JEC-ECC.2013.6766407},
isbn = {978-1-4799-3160-6},
keywords = {SLAM,VSLAM,feature extraction,landmarks},
month = {dec},
pages = {170--175},
publisher = {IEEE},
title = {{Vision based SLAM for humanoid robots: A survey}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6766407 http://ieeexplore.ieee.org/document/6766407/},
year = {2013}
}
@article{Sola2018,
abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
archivePrefix = {arXiv},
arxivId = {1812.01537},
author = {Sol{\`{a}}, Joan and Deray, Jeremie and Atchuthan, Dinesh},
eprint = {1812.01537},
file = {:home/matias/Documents/Mendeley Desktop/Sol{\`{a}}, Deray, Atchuthan/2018/Sol{\`{a}}, Deray, Atchuthan - 2018 - A micro Lie theory for state estimation in robotics.pdf:pdf},
pages = {1--17},
title = {{A micro Lie theory for state estimation in robotics}},
url = {http://arxiv.org/abs/1812.01537},
year = {2018}
}
@article{Estefo2014,
abstract = {In traditional robot behavior programming, the edit-compile-simulate-deploy-run cycle creates a large mental disconnect between program creation and eventual robot behavior. This significantly slows down behavior development because there is no immediate mental connection between the program and the resulting behavior. With live programming the development cycle is made extremely tight, realizing such an immediate connection. In our work on programming of ROS robots in a more dynamic fashion through PhaROS, we have experimented with the use of the Live Robot Programming language. This has given rise to a number of requirements for such live programming of robots. In this text we introduce these requirements and illustrate them using an example robot behavior.},
archivePrefix = {arXiv},
arxivId = {1412.4629},
author = {Estef{\'{o}}, Pablo and Campusano, Miguel and Fabresse, Luc and Fabry, Johan and Laval, Jannik and Bouraqad, Noury},
eprint = {1412.4629},
journal = {Workshop on Domain-Specific Languages and models for Robotic systems},
number = {Dcc},
pages = {1--5},
title = {{Towards Live Programming in ROS with PhaROS and LRP}},
url = {http://arxiv.org/abs/1412.4629},
year = {2014}
}
@article{Kenneally2016,
abstract = {This letter introduces Minitaur, a dynamically running and leaping quadruped, which represents a novel class of direct-drive (DD) legged robots. We present a methodology that achieves the well-known benefits of DD robot design (transparency, mechanical robustness/efficiency, high-actuation bandwidth, and increased specific power), affording highly energetic behaviors across our family of machines despite severe limitations in specific force. We quantify DD drivetrain benefits using a variety of metrics, compare our machines' performance to previously reported legged platforms, and speculate on the potential broad-reaching value of 'transparency' for legged locomotion.},
author = {Kenneally, Gavin and De, Avik and Koditschek, D. E.},
doi = {10.1109/LRA.2016.2528294},
file = {::},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mechanism Design of Mobile Robots,Multilegged Robots,Novel Actuators for Natural Machine Motion},
number = {2},
pages = {900--907},
pmid = {110590},
title = {{Design Principles for a Family of Direct-Drive Legged Robots}},
url = {http://ieeexplore.ieee.org/document/7403902/},
volume = {1},
year = {2016}
}
@book{Macka,
abstract = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a rst- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single eld, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
author = {Yang, Yuhong},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/jasa.2005.s54},
file = {::},
isbn = {9780521642989},
issn = {0162-1459},
number = {472},
pages = {1461--1462},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference, and Learning Algorithms}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2005.s54%5Cnhttp://www.cambridge.org/0521642981 http://www.tandfonline.com/doi/abs/10.1198/jasa.2005.s54},
volume = {100},
year = {2005}
}
@article{Sodhi2020,
abstract = {Actively exploring and mapping an unknown environment requires integration of both simultaneous localization and mapping (SLAM) and path planning methods. Path planning relies on a map that contains free and occupied space information and is efficient to query, while the role of SLAM is to keep the map consistent as new measurements are continuously added. A key challenge, however, lies in ensuring a map representation compatible with both these objectives: that is, a map that maintains free space information for planning but can also adapt efficiently to dynamically changing pose estimates from a graph-based SLAM system.In this paper, we propose an online global occupancy map that can be corrected for accumulated drift efficiently based on incremental solutions from a sparse graph-based SLAM optimization. Our map maintains free space information for real-time path planning while undergoing a bounded number of updates in each loop closure iteration. We evaluate performance for both simulated and real-world datasets for an application involving underwater exploration and mapping.},
author = {Sodhi, Paloma and Ho, Bing Jui and Kaess, Michael},
doi = {10.1109/IROS40897.2019.8967991},
file = {:home/matias/Documents/Mendeley Desktop/Sodhi, Ho, Kaess/2019/Sodhi, Ho, Kaess - 2019 - Online and Consistent Occupancy Grid Mapping for Planning in Unknown Environments.pdf:pdf},
isbn = {9781728140049},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {7879--7886},
title = {{Online and Consistent Occupancy Grid Mapping for Planning in Unknown Environments}},
year = {2019}
}
@article{Jones2010,
abstract = {We describe a model to estimate motion from monocular visual and inertial measurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is 'rich enough', a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30 km. We also describe an integrated approach to 'loop-closure', that is the recognition of previously seen locations and the topological re-adjustment of the traveled path. It represents visual features relative to the global orientation reference provided by the gravity vector estimated by the filter, and relative to the scale provided by their known position within the map; these features are organized into 'locations' defined by visibility constraints, represented in a topological graph, where loop-closure can be performed without the need to re-compute past trajectories or perform bundle adjustment. The software infrastructure as well as the embedded platform is described in detail in a previous technical report. {\textcopyright} 2011 The Author(s).},
author = {Jones, Eagle S. and Soatto, Stefano},
doi = {10.1177/0278364910388963},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Visual-inertial navigation,assisted driving,autonomous robotics,localization,location recognition,loop closure,simultaneous localization and mapping (SLAM),structure from motion,vision-aided navigation,vision-based navigation},
month = {apr},
number = {4},
pages = {407--430},
title = {{Visual-inertial navigation, mapping and localization: A scalable real-time causal approach}},
url = {http://journals.sagepub.com/doi/10.1177/0278364910388963},
volume = {30},
year = {2011}
}
@article{Tai2016,
abstract = {Deep learning techniques have been widely applied, achieving state-of-the-art results in various fields of study. This survey focuses on deep learning solutions that target learning control policies for robotics applications. We carry out our discussions on the two main paradigms for learning control with deep networks: deep reinforcement learning and imitation learning. For deep reinforcement learning (DRL), we begin from traditional reinforcement learning algorithms, showing how they are extended to the deep context and effective mechanisms that could be added on top of the DRL algorithms. We then introduce representative works that utilize DRL to solve navigation and manipulation tasks in robotics. We continue our discussion on methods addressing the challenge of the reality gap for transferring DRL policies trained in simulation to real-world scenarios, and summarize robotics simulation platforms for conducting DRL research. For imitation leaning, we go through its three main categories, behavior cloning, inverse reinforcement learning and generative adversarial imitation learning, by introducing their formulations and their corresponding robotics applications. Finally, we discuss the open challenges and research frontiers.},
archivePrefix = {arXiv},
arxivId = {1612.07139},
author = {Tai, Lei and Zhang, Jingwei and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
eprint = {1612.07139},
month = {dec},
title = {{A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation}},
url = {http://arxiv.org/abs/1612.07139},
year = {2016}
}
@inproceedings{Sunderhauf2015,
abstract = {After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.},
archivePrefix = {arXiv},
arxivId = {1501.04158},
author = {S{\"{u}}nderhauf, Niko and Shirazi, Sareh and Dayoub, Feras and Upcroft, Ben and Milford, Michael},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353986},
eprint = {1501.04158},
file = {:home/matias/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/2015/S{\"{u}}nderhauf et al. - 2015 - On the performance of ConvNet features for place recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
keywords = {Computer vision,Feature extraction,Real-time systems,Robustness,Semantics,Visualization},
month = {sep},
pages = {4297--4304},
publisher = {IEEE},
title = {{On the performance of ConvNet features for place recognition}},
url = {http://ieeexplore.ieee.org/document/7353986/},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Concha2016,
abstract = {The so-called direct visual SLAM methods have shown a great potential in estimating a semidense or fully dense reconstruction of the scene, in contrast to the sparse reconstructions of the traditional feature-based algorithms. In this paper, we propose for the first time a direct, tightly-coupled formulation for the combination of visual and inertial data. Our algorithm runs in real-time on a standard CPU. The processing is split in three threads. The first thread runs at frame rate and estimates the camera motion by a joint non-linear optimization from visual and inertial data given a semidense map. The second one creates a semidense map of high-gradient areas only for camera tracking purposes. Finally, the third thread estimates a fully dense reconstruction of the scene at a lower frame rate. We have evaluated our algorithm in several real sequences with ground truth trajectory data, showing a state-of-the-art performance.},
author = {Concha, Alejo and Loianno, Giuseppe and Kumar, Vijay and Civera, Javier},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487266},
isbn = {9781467380263},
issn = {10504729},
month = {may},
pages = {1331--1338},
publisher = {IEEE},
title = {{Visual-inertial direct SLAM}},
url = {http://webdiis.unizar.es/$\sim$jcivera/papers/concha_etal_icra16.pdf http://ieeexplore.ieee.org/document/7487266/},
volume = {2016-June},
year = {2016}
}
@article{Baltes2002,
author = {Baltes, Jacky and Sadeghnejad, Soroush and Seifert, Daniel and Behnke, Sven and Gerndt, Reinhard},
file = {::},
number = {June 2015},
title = {{RoboCup Humanoid Robotic Soccer Competitions 2002 – 2050}},
year = {2002}
}
@article{Stachenfeld2017,
abstract = {A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.},
author = {Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},
doi = {10.1038/nn.4650},
file = {:home/matias/Documents/Mendeley Desktop/Stachenfeld, Botvinick, Gershman/2017/Stachenfeld, Botvinick, Gershman - 2017 - The hippocampus as a predictive map.pdf:pdf},
issn = {15461726},
journal = {Nature Neuroscience},
number = {11},
pages = {1643--1653},
pmid = {28967910},
title = {{The hippocampus as a predictive map}},
volume = {20},
year = {2017}
}
@article{Romay2017,
abstract = {Team ViGIR and Team Hector participated in the DARPA Robotics Challenge (DRC) Finals, held June 2015 in Pomona, California, along with 21 other teams from around the world. Both teams competed using the same high-level software, in conjunction with independently developed low-level software specific to their humanoid robots. On the basis of previous work on operator-centric manipulation control at the level of affordances, we developed an approach that allows one or more human operators to share control authority with a high-level behavior controller. This collaborative autonomy decreases the completion time of manipulation tasks, increases the reliability of the human-robot team, and allows the operators to adjust the robotic system's autonomy on-the-fly. This article discusses the technical challenges we faced and overcame during our efforts to allow the human operators to interact with the robotic system at a higher level of abstraction and share control authority with it. We introduce and evaluate the proposed approach in the context of our two teams' participation in the DRC Finals. We also present additional, systematic experiments conducted in the lab afterward. Finally, we present a discussion about the lessons learned while transitioning between operator-centered manipulation control and behavior-centered manipulation control during competition.},
author = {Romay, Alberto and Kohlbrecher, Stefan and Stumpf, Alexander and von Stryk, Oskar and Maniatopoulos, Spyros and Kress-Gazit, Hadas and Schillinger, Philipp and Conner, David C.},
doi = {10.1002/rob.21671},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {333--358},
title = {{Collaborative Autonomy between High-level Behaviors and Human Operators for Remote Manipulation Tasks using Different Humanoid Robots}},
url = {http://doi.wiley.com/10.1002/rob.21671},
volume = {34},
year = {2017}
}
@article{Oskiper2007,
abstract = {Over the past decade, tremendous amount of research activity has focused around the problem of localization in GPS denied environments. Challenges with localization are highlighted in human wearable systems where the operator can freely move through both indoors and outdoors. In this paper, we present a robust method that addresses these challenges using a human wearable system with two pairs of backward and forward looking stereo cameras together with an inertial measurement unit (IMU). This algorithm can run in real-time with 15Hz update rate on a dual-core 2GHz laptop PC and it is designed to be a highly accurate local (relative) pose estimation mechanism acting as the front-end to a Simultaneous Localization and Mapping (SLAM) type method capable of global corrections through landmark matching. Extensive tests of our prototype system so far, reveal that without any global landmark matching, we achieve between 0.5% and 1% accuracy in localizing a person over a 500 meter travel indoors and outdoors. To our knowledge, such performance results with a real time system have not been reported before. {\textcopyright} 2007 IEEE.},
author = {Oskiper, Taragay and Zhu, Zhiwei and Samarasekera, Supun and Kumar, Rakesh},
doi = {10.1109/CVPR.2007.383087},
file = {:home/matias/Documents/Mendeley Desktop/Oskiper, Zhu/2007/Oskiper, Zhu - 2007 - Visual Odometry System Using Multiple Stereo Cameras and Inertial Measurement Unit Visual Odometry System Using(2).pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {June},
title = {{Visual odometry system using multiple stereo cameras and inertial measurement unit}},
year = {2007}
}
@article{Loquercio2020a,
abstract = {Motivated by the astonishing capabilities of natural intelligent agents and inspired by theories from psychology, this paper explores the idea that perception gets coupled to 3D properties of the world via interaction with the environment. Existing works for depth estimation require either massive amounts of annotated training data or some form of hard-coded geometrical constraint. This paper explores a new approach to learning depth perception requiring neither of those. Specifically, we train a specialized global-local network architecture with what would be available to a robot interacting with the environment: from extremely sparse depth measurements down to even a single pixel per image. From a pair of consecutive images, our proposed network outputs a latent representation of the observer's motion between the images and a dense depth map. Experiments on several datasets show that, when ground truth is available even for just one of the image pixels, the proposed network can learn monocular dense depth estimation up to 22.5% more accurately than state-of-the-art approaches. We believe that this work, despite its scientific interest, lays the foundations to learn depth from extremely sparse supervision, which can be valuable to all robotic systems acting under severe bandwidth or sensing constraints.},
archivePrefix = {arXiv},
arxivId = {2003.00752},
author = {Loquercio, Antonio and Dosovitskiy, Alexey and Scaramuzza, Davide},
doi = {10.1109/lra.2020.3009067},
eprint = {2003.00752},
file = {:home/matias/Documents/Mendeley Desktop/Loquercio, Dosovitskiy, Scaramuzza/2020/Loquercio, Dosovitskiy, Scaramuzza - 2020 - Learning Depth With Very Sparse Supervision.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
number = {4},
pages = {5542--5549},
title = {{Learning Depth With Very Sparse Supervision}},
volume = {5},
year = {2020}
}
@article{Xie2019,
abstract = {This paper describes a unified method solving for inverse, forward, and hybrid dynamics problems for robotic manipulators with either open kinematic chains or closed kinematic loops based on factor graphs. Manipulator dynamics is considered to be a well studied problem, and various different algorithms have been developed to solve each type of dynamics problem. However, they are not easily explained in a unified and intuitive way. In this paper, we introduce factor graphs as a unifying graphical language in which not only to solve all types of dynamics problems, but also explain the classical dynamics algorithms in a unified framework.},
archivePrefix = {arXiv},
arxivId = {1911.10065},
author = {Xie, Mandy and Dellaert, Frank},
eprint = {1911.10065},
file = {:home/matias/Documents/Mendeley Desktop/Xie, Dellaert/Unknown/Xie, Dellaert - Unknown - A Unified Method for Solving Inverse , Forward , and Hybrid Manipulator Dynamics using Factor Graphs.pdf:pdf},
title = {{A Unified Method for Solving Inverse, Forward, and Hybrid Manipulator Dynamics using Factor Graphs}},
url = {http://arxiv.org/abs/1911.10065},
year = {2019}
}
@inproceedings{Klingensmith2015,
abstract = {We describe CHISEL: a system for real-time house-scale (300 square meter or more) dense 3D reconstruction onboard a Google Tango [1] mobile device by using a dynamic spatially-hashed truncated signed distance field[2] for mapping, and visual-inertial odometry for localization. By aggressively culling parts of the scene that do not contain surfaces, we avoid needless computation and wasted memory. Even under very noisy conditions, we produce high-quality reconstructions through the use of space carving. We are able to reconstruct and render very large scenes at a resolution of 2-3 cm in real time on a mobile device without the use of GPU computing. The user is able to view and interact with the reconstruction in real-time through an intuitive interface. We provide both qualitative and quantitative results on publicly available RGB-D datasets [3], and on datasets collected in real-time from two devices.},
author = {Klingensmith, Matthew and Dryanovski, Ivan and Srinivasa, Siddhartha S. and Xiao, Jizhong},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.040},
isbn = {9780992374716},
issn = {2330765X},
title = {{CHISEL: Real time large scale 3D reconstruction onboard a mobile device using spatially-hashed signed distance fields}},
url = {http://www.roboticsproceedings.org/rss11/p40.html},
volume = {11},
year = {2015}
}
@phdthesis{Fourie2017,
abstract = {This thesis presents a sum-product inference algorithm for platform navigation called Multi-modal iSAM(incrementalsmoothingandmapping). CommonGaussian- only likelihoods are restrictive and require a complex front-end processes to deal with non-Gaussian measurements. Instead, our approach allows the front-end to defer ambiguities with non-Gaussian measurement models. We retain the acyclic Bayes tree (and incremental update strategy) from the predecessor iSAM2 max- product algorithm [Kaess et al., IJRR 2012]. The approach propagates continuous beliefs on the Bayes (Junction) tree, which is an efficient symbolic refactorization of the nonparametric factor graph, and asymptotically approximates the under- lying Chapman-Kolmogorov equations. Our method tracks dominant modes in the marginal posteriors of all variables with minimal approximation error, while suppressing almost all lowlikelihood modes (in a non-permanent manner). Keep- ing with existing inertial navigation,we present a novel, continuous-time, retroac- tively calibrating inertial odometry residual function, using preintegration to seam- lessly incorporate pure inertial sensor measurements into a factor graph. We cen- tralize around a factor graph (with starved graph databases) to separate elements of the navigation into an ecosystem of processes. Practical examples are included, such as how to infer multi-modal marginal posterior belief estimates for ambigu- ous loop closures; rawbeam-formedacoustic measurements; or conventional para- metric likelihoods, and others. Thesis},
author = {Fourie, Dehann},
booktitle = {Multi-modal and inertial sensor solutions for navigation-type factor graphs},
doi = {10.1575/1912/9305},
school = {Massachussets Institute of Technology},
title = {{Multi-modal and inertial sensor solutions for navigation-type factor graphs}},
year = {2017}
}
@article{Wang2017d,
abstract = {This paper studies visual odometry (VO) from the perspective of deep learning. After tremendous efforts in the robotics and computer vision communities over the past few decades, state-of-the-art VO algorithms have demonstrated incredible performance. However, since the VO problem is typically formulated as a pure geometric problem, one of the key features still missing from current VO systems is the capability to automatically gain knowledge and improve performance through learning. In this paper, we investigate whether deep neural networks can be effective and beneficial to the VO problem. An end-to-end, sequence-to-sequence probabilistic visual odometry (ESP-VO) framework is proposed for the monocular VO based on deep recurrent convolutional neural networks. It is trained and deployed in an end-to-end manner, that is, directly inferring poses and uncertainties from a sequence of raw images (video) without adopting any modules from the conventional VO pipeline. It can not only automatically learn effective feature representation encapsulating geometric information through convolutional neural networks, but also implicitly model sequential dynamics and relation for VO using deep recurrent neural networks. Uncertainty is also derived along with the VO estimation without introducing much extra computation. Extensive experiments on several datasets representing driving, flying and walking scenarios show competitive performance of the proposed ESP-VO to the state-of-the-art methods, demonstrating a promising potential of the deep learning technique for VO and verifying that it can be a viable complement to current VO systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.08429v1},
author = {Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
doi = {10.1177/0278364917734298},
eprint = {arXiv:1709.08429v1},
file = {::},
isbn = {0037549716666},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Visual odometry,deep learning,pose estimation,recurrent convolutional neural networks,uncertainty},
number = {4-5},
pages = {513--542},
title = {{End-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks}},
volume = {37},
year = {2018}
}
@book{IanGoodfellowYoshuaBengio2017,
abstract = {Deep Learning book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Ian Goodfellow, Yoshua Bengio}, Aaron Courville},
booktitle = {Linear Algebra},
doi = {10.1016/b978-0-12-391420-0.09987-x},
eprint = {arXiv:1011.1669v3},
isbn = {3540620583, 9783540620587},
issn = {1432122X},
number = {7553},
pages = {i--ii},
pmid = {21728107},
title = {{Front Matter}},
volume = {521},
year = {2014}
}
@phdthesis{Niedfeldt2014,
abstract = {Multiple target tracking (MTT) is the process of identifying the number of targets present in a surveillance region and the state estimates, or track, of each target. MTT remains a challenging problem due to the NP-hard data association step, where unlabeled measurements are identified as either a measurement of an existing target, a new target, or a spurious measurement called clutter. Existing techniques suffer from at least one of the following drawbacks: divergence in clutter, underlying assumptions on the number of targets, high computational complexity, time-consuming implementation, poor performance at low detection rates, and/or poor track continuity. Our goal is to develop an efficient MTT algorithm that is simple yet effective and that maintains track continuity enabling persistent tracking of an unknown number of targets. A related field to tracking is regression analysis, where the parameters of static signals are estimated from a batch or a sequence of data. The random sample consensus (RANSAC) algorithm was developed to mitigate the effects of spurious measurements, and has since found wide application within the computer vision community due to its robustness and effi- ciency. The main concept of RANSAC is to form numerous simple hypotheses from a batch of data and identify the hypothesis with the most supporting measurements. Unfortunately, RANSAC is not designed to track multiple targets using sequential measurements. To this end, we have developed the recursive-RANSAC (R-RANSAC) algorithm, which tracks multiple signals in clutter without requiring prior knowledge of the number of existing signals. The basic premise of the R-RANSAC algorithm is to store a set of RANSAC hypotheses between time steps. New measurements are used to either update existing hypotheses or generate new hypotheses using RANSAC. Storing multiple hypotheses enables R-RANSAC to track multiple targets. Good tracks are identified when a sufficient number of measurements support a hypothesis track. The complexity of R-RANSAC is shown to be squared in the number of measurements and stored tracks, and under moderate assumptions R-RANSAC converges in mean to the true states. We apply R-RANSAC to a variety of simulation, camera, and radar tracking examples. Keywords:},
author = {Niedfeldt, Peter C.},
booktitle = {All Theses and Dissertations},
file = {::},
keywords = {IMM,JPDA,Kalman filter,MCMC data association,MHT,PHD,RANSAC,SAR,data association,multiple target tracking,video surveillance},
number = {July},
pages = {Paper 4195},
title = {{Recursive-RANSAC: A Novel Algorithm for Tracking Multiple Targets in Clutter}},
url = {http://scholarsarchive.byu.edu/etd/4195},
year = {2014}
}
@book{Heuel2004,
abstract = {El prop{\'{o}}sito de este libro es explicar de forma clara cu{\'{a}}les son la estructura y las caracter{\'{i}}sticas de lo que se viene denominando el sector de la informaci{\'{o}}n digital, un {\'{a}}mbito que surge de la confluencia de los avances registrados en la tecnolog{\'{i}}a inform{\'{a}}tica, las telecomunicaciones y el almacenaje de la informaci{\'{o}}n, y que nos es directamente perceptible en un sinf{\'{i}}n de productos.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {{Alonso Ar{\'{e}}valo}, Julio},
booktitle = {El Profesional de la Informacion},
doi = {10.1076/epri.10.10.36.6816},
eprint = {9780201398298},
isbn = {9789526052946},
issn = {0022541X},
keywords = {Gesti{\'{o}}n de la informaci{\'{o}}n Gesti{\'{o}}n del Conocimiento,Machine learning,abstract,de recuperaci{\'{o}}n de informaci{\'{o}}n,developed par-,digital,eduardo pablo giordanino,evaluaci{\'{o}}n,implicit feedback,information retrieval,instituto de formaci{\'{o}}n t{\'{e}}cnica,lic,medidas de evaluaci{\'{o}}n,necessity,of information,recuperaci{\'{o}}n de informaci{\'{o}}n,relevance inference,retrieval systems,retrieval systems has been,superior n{\textordmasculine} 13,tecnicatura superior en bibliotecolog{\'{i}}a,the evaluation of information,thoughts about the evaluation,title,ulas for information retrieval,utility and viability,y servicios de informaci{\'{o}}n},
number = {1},
pages = {15},
pmid = {4520227},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Gesti{\'{o}}n de la Informaci{\'{o}}n, gesti{\'{o}}n de contenidos y conocimiento.}},
url = {http://eprints.rclis.org/12992/},
volume = {9},
year = {2013}
}
@article{Scaramuzza2011b,
abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper [1]. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap. {\textcopyright} 2011 IEEE.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
doi = {10.1109/MRA.2011.943233},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
month = {dec},
number = {4},
pages = {80--92},
pmid = {6153423},
title = {{Tutorial: Visual odometry}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6096039},
volume = {18},
year = {2011}
}
@inproceedings{Yamane2011,
abstract = {This paper presents methods and experimental results regarding the identification of kinematic and dynamic parameters of force-controlled biped humanoid robots. We first describe a kinematic calibration method to estimate joint angle sensor offsets. The method is practical in the sense that it only uses joint angle and link orientation sensors, which most humanoid robots are equipped with. The basic idea is to solve an optimization problem that represents a kinematic constraint that can be easily enforced, such as placing both feet flat on floor. We then present two methods to identify physically consistent mass and local center of mass parameters even when obtaining enough excitation is difficult, as is always the case in humanoid robots. We demonstrate by experiment that these methods give good identification results even when the regressor has a large condition number. Moreover, we show that gradient-based optimization performs better than the leasts-quare method in many cases. {\textcopyright} 2011 IEEE.},
author = {Yamane, Katsu},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/Humanoids.2011.6100803},
isbn = {9781612848679},
issn = {21640572},
month = {oct},
pages = {269--275},
publisher = {IEEE},
title = {{Practical kinematic and dynamic calibration methods for force-controlled humanoid robots}},
url = {http://ieeexplore.ieee.org/document/6100803/},
year = {2011}
}
@phdthesis{Zarco2013,
author = {Zarco, Mario},
isbn = {3110075113},
school = {Universidad Nacional Aut{\'{o}}noma de M{\'{e}}xico},
title = {{Evaluaci{\'{o}}n de t{\'{e}}cnicas de control visual monocular para la locomoci{\'{o}}n de robots humanoides NAO}},
year = {2015}
}
@article{Carlone2014,
abstract = {Pose graph optimization from relative measurements is challenging because of the angular component of the poses: the variables live on a manifold product with nontrivial topology and the likelihood function is nonconvex and has many local minima. Because of these issues, iterative solvers are not robust to large amounts of noise. This paper describes a global estimation method, called multi-hypothesis orientation-from-lattice estimation in 2-D (MOLE2D), for the estimation of the nodes' orientation in a pose graph. We demonstrate that the original nonlinear optimization problem on the manifold product is equivalent to an unconstrained quadratic optimization problem on the integer lattice. Exploiting this insight, we show that, in general, the maximum likelihood estimate alone cannot be considered a reliable estimator. Therefore, MOLE2D returns a set of point estimates, for which we can derive precise probabilistic guarantees. Experiments show that the method is able to tolerate extreme amounts of noise, far above all noise levels of sensors used in applications. Using MOLE2D's output to bootstrap the initial guess of iterative pose graph optimization methods improves their robustness and makes them avoid local minima even for high levels of noise. {\textcopyright} 2004-2012 IEEE.},
author = {Carlone, Luca and Censi, Andrea},
doi = {10.1109/TRO.2013.2291626},
file = {::},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Integer quadratic programming,SO(2) manifold,mobile robots,multi-hypothesis estimation,orientation estimation,pose graph optimization,simultaneous localization and mapping (SLAM)},
number = {2},
pages = {475--492},
title = {{From angular manifolds to the integer lattice: Guaranteed orientation estimation with application to pose graph optimization}},
volume = {30},
year = {2014}
}
@article{Jaderberg2015,
abstract = {Predicting the affinity profiles of nucleic acid-binding proteins directly from the protein sequence is a challenging problem. We present a statistical approach for learning the recognition code of a family of transcription factors or RNA-binding proteins (RBPs) from high-throughput binding data. Our method, called affinity regression, trains on protein binding microarray (PBM) or RNAcompete data to learn an interaction model between proteins and nucleic acids using only protein domain and probe sequences as inputs. When trained on mouse homeodomain PBM profiles, our model correctly identifies residues that confer DNA-binding specificity and accurately predicts binding motifs for an independent set of divergent homeodomains. Similarly, when trained on RNAcompete profiles for diverse RBPs, our model correctly predicts the binding affinities of held-out proteins and identifies key RNA-binding residues, despite the high level of sequence divergence across RBPs. We expect that the method will be broadly applicable to modeling and predicting paired macromolecular interactions in settings where high-throughput affinity data are available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02025v1},
author = {Pelossof, Raphael and Singh, Irtisha and Yang, Julie L. and Weirauch, Matthew T. and Hughes, Timothy R. and Leslie, Christina S.},
doi = {10.1038/nbt.3343},
eprint = {arXiv:1506.02025v1},
file = {::},
isbn = {9781627480031},
issn = {15461696},
journal = {Nature Biotechnology},
month = {nov},
number = {12},
pages = {1242--1249},
pmid = {26571099},
title = {{Affinity regression predicts the recognition code of nucleic acid-binding proteins}},
url = {http://www.nature.com/doifinder/10.1038/nbt.3343},
volume = {33},
year = {2015}
}
@article{Aldana-Iuit2016,
abstract = {A novel similarity-covariant feature detector that extracts points whose neighborhoods, when treated as a 3D intensity surface, have a saddle-like intensity profile. The saddle condition is verified efficiently by intensity comparisons on two concentric rings that must have exactly two dark-to-bright and two bright-to-dark transitions satisfying certain geometric constraints. Experiments show that the Saddle features are general, evenly spread and appearing in high density in a range of images. The Saddle detector is among the fastest proposed. In comparison with detector with similar speed, the Saddle features show superior matching performance on number of challenging datasets.},
archivePrefix = {arXiv},
arxivId = {1608.06800},
author = {Aldana-Iuit, Javier and Mishkin, Dmytro and Chum, Ondřej and Matas, Jiř{\'{i}}},
doi = {10.1109/ICPR.2016.7899712},
eprint = {1608.06800},
file = {:home/matias/Documents/Mendeley Desktop/Aldana-Iuit et al/2016/Aldana-Iuit et al. - 2016 - In the Saddle Chasing fast and repeatable features.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {675--680},
title = {{In the Saddle: Chasing fast and repeatable features}},
volume = {0},
year = {2016}
}
@inproceedings{Salas2015,
abstract = {Under the assumption of identical covariance at each pose, Horn's method finds an aligning transform that is quite similar to the one found using an optimization on the manifold.},
author = {Salas, Marta and Latif, Yasir and Reid, Ian D and Montiel, J. M. M.},
booktitle = {RSS Workshop: The Problem of Mobile Sensors},
number = {6},
pages = {2--4},
title = {{Trajectory Alignment and Evaluation in SLAM : Horn's Method vs Alignment on the Manifold}},
volume = {1},
year = {2015}
}
@article{Engel2016,
abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-And camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-The-Art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {1607.02565},
author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
doi = {10.1109/TPAMI.2017.2658577},
eprint = {1607.02565},
file = {:home/matias/Documents/Mendeley Desktop/Engel, Koltun, Cremers/2018/Engel, Koltun, Cremers - 2018 - Direct Sparse Odometry.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D reconstruction,SLAM,Visual odometry,structure from motion},
month = {jul},
number = {3},
pages = {611--625},
pmid = {28422651},
title = {{Direct Sparse Odometry}},
url = {http://arxiv.org/abs/1607.02565 http://ieeexplore.ieee.org/document/7898369/},
volume = {40},
year = {2018}
}
@article{Kim2017,
abstract = {This paper presents the technical approaches used and experimental results obtained by Team SNU (Seoul National University) at the 2015 DARPA Robotics Challenge (DRC) Finals. Team SNU is one of the newly qualified teams, unlike 12 teams who previously participated in the December 2013 DRC Trials. The hardware platform THORMANG, which we used, has been developed by ROBOTIS. THORMANG is one of the smallest robots at the DRC Finals. Based on this platform, we focused on developing software architecture and controllers in order to perform complex tasks in disaster response situations and modifying hardware modules to maximize manipulability. Ensuring stability and modularization are two main keywords in the technical approaches of the architecture. We designed our interface and controllers to achieve a higher robustness level against disaster situations. Moreover, we concentrated on developing our software architecture by integrating a number of modules to eliminate software system complexity and programming errors. With these efforts on the hardware and software, we successfully finished the competition without falling, and we ranked 12th out of 23 teams. This paper is concluded with a number of lessons learned by analyzing the 2015 DRC Finals.},
author = {Kim, Sanghyun and Kim, Mingon and Lee, Jimin and Hwang, Soonwook and Chae, Joonbo and Park, Beomyeong and Cho, Hyunbum and Sim, Jaehoon and Jung, Jaesug and Lee, Hosang and Shin, Seho and Kim, Minsung and Choi, Wonje and Lee, Yisoo and Park, Sumin and Oh, Jiyong and Lee, Yongjin and Lee, Sangkuk and Lee, Myunggi and Yi, Sangyup and Chang, Kyong Sok K.C. and Kwak, Nojun and Park, Jaeheung},
doi = {10.1002/rob.21678},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {359--380},
title = {{Team SNU's Control Strategies for Enhancing a Robot's Capability: Lessons from the 2015 DARPA Robotics Challenge Finals}},
url = {http://doi.wiley.com/10.1002/rob.21678},
volume = {34},
year = {2017}
}
@article{Fabry,
author = {Campusano, Miguel},
keywords = {live programming,m,robot,s},
number = {Dcc},
title = {{Live Robot Programming: Ayudando al Desarrollo de Robots}}
}
@article{Barfoot2019a,
abstract = {We present a Gaussian variational inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit both the mean and (inverse) covariance of a Gaussian to the posterior efficiently, by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. This is different than maximum a posteriori (MAP) estimation, which seeks the point estimate for the state that maximizes the posterior (i.e., the mode). The proposed exactly sparse Gaussian variational inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch–Tung–Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the ‘extended' RTS smoother in the nonlinear case because it finds the best-fit Gaussian (mean and covariance), not the MAP point estimate. We demonstrate the technique on controlled simulation problems and a batch nonlinear simultaneous localization and mapping problem with an experimental dataset.},
archivePrefix = {arXiv},
arxivId = {1911.08333},
author = {Barfoot, Timothy D. and Forbes, James R. and Yoon, David J.},
doi = {10.1177/0278364920937608},
eprint = {1911.08333},
file = {:home/matias/Documents/Mendeley Desktop/Barfoot, Forbes/2019/Barfoot, Forbes - 2019 - Exactly Sparse Gaussian Variational Inference with Application to Derivative-Free Batch Nonlinear State Estimat.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Gaussian variational inference,derivative-free state estimation,exact sparsity},
number = {1},
pages = {1--31},
title = {{Exactly sparse Gaussian variational inference with application to derivative-free batch nonlinear state estimation}},
url = {http://arxiv.org/abs/1911.08333},
volume = {1},
year = {2020}
}
@article{Paton2017,
abstract = {Vision-based, autonomous, route-following algorithms enable robots to autonomously repeat manually driven routes over long distances. Through the use of inexpensive, commercial vision sensors, these algorithms have the potential to enable robotic applications across multiple industries. However, in order to extend these algorithms to long-term autonomy, they must be able to operate over long periods of time. This poses a difficult challenge for vision-based systems in unstructured and outdoor environments, where appearance is highly variable. While many techniques have been developed to perform localization across extreme appearance change, most are not suitable or untested for vision-in-the-loop systems such as autonomous route following, which requires continuous metric localization to keep the robot driving. In this paper, we present a vision-based, autonomous, route-following algorithm that combines multiple channels of information during localization to increase robustness against daily appearance change such as lighting. We explore this multichannel visual teach and repeat framework by adding the following channels of information to the basic single-camera, gray-scale, localization pipeline: images that are resistant to lighting change and images from additional stereo cameras to increase the algorithm's field of view. Using these methods, we demonstrate robustness against appearance change through extensive field deployments spanning over 26 km with an autonomy rate greater than 99.9%. We furthermore discuss the limits of this system when subjected to harsh environmental conditions by investigating keypoint match degradation through time.},
author = {Paton, Michael and Pomerleau, Fran{\c{c}}ois and MacTavish, Kirk and Ostafew, Chris J. and Barfoot, Timothy D.},
doi = {10.1002/rob.21669},
file = {:home/matias/Documents/Mendeley Desktop/Paton et al/2017/Paton et al. - 2017 - Expanding the Limits of Vision-based Localization for Long-term Route-following Autonomy.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {98--122},
title = {{Expanding the Limits of Vision-based Localization for Long-term Route-following Autonomy}},
volume = {34},
year = {2017}
}
@article{Du2020,
abstract = {For relocalization in large-scale point clouds, we propose the first approach that unifies global place recognition and local 6DoF pose refinement. To this end, we design a Siamese network that jointly learns 3D local feature detection and description directly from raw 3D points. It integrates FlexConv and Squeeze-and-Excitation (SE) to assure that the learned local descriptor captures multi-level geometric information and channel-wise relations. For detecting 3D keypoints we predict the discriminativeness of the local descriptors in an unsupervised manner. We generate the global descriptor by directly aggregating the learned local descriptors with an effective attention mechanism. In this way, local and global 3D descriptors are inferred in one single forward pass. Experiments on various benchmarks demonstrate that our method achieves competitive results for both global point cloud retrieval and local point cloud registration in comparison to state-of-the-art approaches. To validate the generalizability and robustness of our 3D keypoints, we demonstrate that our method also performs favorably without fine-tuning on the registration of point clouds that were generated by a visual SLAM system. Code and related materials are available at https://vision.in.tum.de/research/vslam/dh3d.},
archivePrefix = {arXiv},
arxivId = {2007.09217},
author = {Du, Juan and Wang, Rui and Cremers, Daniel},
eprint = {2007.09217},
file = {:home/matias/Documents/Mendeley Desktop/Du, Wang, Cremers/2020/Du, Wang, Cremers - 2020 - DH3D Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF Relocalization.pdf:pdf},
keywords = {3d deep learning,point clouds,relocalization},
title = {{DH3D: Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF Relocalization}},
url = {http://arxiv.org/abs/2007.09217},
year = {2020}
}
@book{Chirikjian2012,
abstract = {The subjects of stochastic processes, information theory, and Lie groups are usually treated separately from each other. This unique two-volume set presents these topics in a unified setting, thereby building bridges between fields that are rarely studied by the same people. Unlike the many excellent formal treatments available for each of these subjects individually, the emphasis in both of these volumes is on the use of stochastic, geometric, and group-theoretic concepts in the modeling of physical phenomena. Volume 2 builds on the fundamentals presented in Volume 1, delving deeper into relationships among stochastic geometry, geometric aspects of the theory of communications and coding, multivariate statistical analysis, and error propagation on Lie groups. Extensive exercises, motivating examples, and real-world applications make the work suitable as a textbook for use in courses that emphasize applied stochastic processes or differential geometry. Stochastic Models, Information Theory, and Lie Groups will be of interest to advanced undergraduate and graduate students, researchers, and practitioners working in applied mathematics, the physical sciences, and engineering.},
address = {Boston},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chirikjian, Gregory S.},
booktitle = {Stochastic Models, Information Theory, and Lie Groups},
doi = {10.1007/978-0-8176-4944-9},
eprint = {arXiv:1011.1669v3},
isbn = {9780817649449},
issn = {1098-6596},
pages = {1--433},
pmid = {25246403},
publisher = {Birkh{\"{a}}user Boston},
title = {{Stochastic models, information theory, and Lie groups}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107415324A009 http://link.springer.com/10.1007/978-0-8176-4944-9 http://link.springer.com/10.1007/978-0-8176-4803-9},
volume = {2},
year = {2012}
}
@article{Wang2017e,
abstract = {This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-of-the-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.},
archivePrefix = {arXiv},
arxivId = {1709.08429},
author = {Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
doi = {10.1109/ICRA.2017.7989236},
eprint = {1709.08429},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Localization,Visual-Based Navigation},
pages = {2043--2050},
title = {{DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks}},
url = {https://www.cs.ox.ac.uk/files/9026/DeepVO.pdf},
year = {2017}
}
@article{Kuindersma2016,
abstract = {This paper describes a collection of optimization algorithms for achieving dynamic planning, control, and state estimation for a bipedal robot designed to operate reliably in complex environments. To make challenging locomotion tasks tractable, we describe several novel applications of convex, mixed-integer, and sparse nonlinear optimization to problems ranging from footstep placement to whole-body planning and control. We also present a state estimator formulation that, when combined with our walking controller, permits highly precise execution of extended walking plans over non-flat terrain. We describe our complete system integration and experiments carried out on Atlas, a full-size hydraulic humanoid robot built by Boston Dynamics, Inc.},
author = {Kuindersma, Scott and Deits, Robin and Fallon, Maurice and Valenzuela, Andr{\'{e}}s and Dai, Hongkai and Permenter, Frank and Koolen, Twan and Marion, Pat and Tedrake, Russ},
doi = {10.1007/s10514-015-9479-3},
file = {::},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Humanoid,Legged locomotion,Optimization,State estimation},
number = {3},
pages = {429--455},
title = {{Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot}},
volume = {40},
year = {2016}
}
@article{Hidalgo-Carrio2017,
abstract = {Since early in robotics the performance of odometry techniques has been of constant research for mobile robots. This is due to its direct influence on localization. The pose error grows unbounded in dead-reckoning systems and its uncertainty has negative impacts in localization and mapping (i.e. SLAM). The dead-reckoning performance in terms of residuals, i.e. the difference between the expected and the real pose state, is related to the statistical error or uncertainty in probabilistic motion models. A novel approach to model odometry errors using Gaussian processes (GPs) is presented. The methodology trains a GP on the residual between the non-linear parametric motion model and the ground truth training data. The result is a GP over odometry residuals which provides an expected value and its uncertainty in order to enhance the belief with respect to the parametric model. The localization and mapping benefits from a comprehensive GP-odometry residuals model. The approach is applied to a planetary rover in an unstructured environment. We show that our approach enhances visual SLAM by efficiently computing image frames and effectively distributing keyframes.},
author = {Hidalgo-Carrio, Javier and Hennes, Daniel and Schwendner, Jakob and Kirchner, Frank},
doi = {10.1109/ICRA.2017.7989670},
file = {::},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {5696--5701},
title = {{Gaussian process estimation of odometry errors for localization and mapping}},
year = {2017}
}
@article{Montemerlo2002,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
doi = {10.1.1.16.2153},
file = {::},
isbn = {0262511290},
issn = {10450823},
journal = {Proceedings of the National Conference on Artificial Intelligence},
number = {2},
pages = {593--598},
title = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem#0},
volume = {68},
year = {2002}
}
@inproceedings{Zhang2016a,
abstract = {The transition of visual-odometry technology from research demonstrators to commercial applications naturally raises the question: what is the optimal camera for vision-based motion estimation? This question is crucial as the choice of camera has a tremendous impact on the robustness and accuracy of the employed visual odometry algorithm. While many properties of a camera (e.g. resolution, frame-rate, global-shutter/rolling-shutter) could be considered, in this work we focus on evaluating the impact of the camera field-of-view (FoV) and optics (i.e., fisheye or catadioptric) on the quality of the motion estimate. Since the motion-estimation performance depends highly on the geometry of the scene and the motion of the camera, we analyze two common operational environments in mobile robotics: an urban environment and an indoor scene. To confirm the theoretical observations, we implement a state-of-the-art VO pipeline that works with large FoV fisheye and catadioptric cameras. We evaluate the proposed VO pipeline in both synthetic and real experiments. The experiments point out that it is advantageous to use a large FoV camera (e.g., fisheye or catadioptric) for indoor scenes and a smaller FoV for urban canyon environments.},
author = {Zhang, Zichao and Rebecq, Henri and Forster, Christian and Scaramuzza, Davide},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487210},
isbn = {9781467380263},
issn = {10504729},
month = {may},
pages = {801--808},
publisher = {IEEE},
title = {{Benefit of large field-of-view cameras for visual odometry}},
url = {http://ieeexplore.ieee.org/document/7487210/},
volume = {2016-June},
year = {2016}
}
@article{Lepetit2009,
abstract = {We propose a non-iterative solution to the PnP problem-the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences-whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n ≥ 4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12 × 12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1007/s11263-008-0152-6},
file = {::},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
number = {2},
pages = {155--166},
title = {{EPnP: An accurate O(n) solution to the PnP problem}},
url = {http://link.springer.com/10.1007/s11263-008-0152-6},
volume = {81},
year = {2009}
}
@article{Chen2020,
abstract = {Deep learning based localization and mapping has recently attracted significant attention. Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. Benefiting from ever-increasing volumes of data and computational power, these methods are fast evolving into a new area that offers accurate and robust systems to track motion and estimate scenes and their structure for real-world applications. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. We also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). It is our hope that this work can connect emerging works from robotics, computer vision and machine learning communities, and serve as a guide for future researchers to apply deep learning to tackle localization and mapping problems.},
archivePrefix = {arXiv},
arxivId = {2006.12567},
author = {Chen, Changhao and Wang, Bing and Lu, Chris Xiaoxuan and Trigoni, Niki and Markham, Andrew},
eprint = {2006.12567},
file = {:home/matias/Documents/Mendeley Desktop/Chen et al/Unknown/Chen et al. - Unknown - A Survey on Deep Learning for Localization and Mapping Towards the Age of Spatial Machine Intelligence.pdf:pdf},
title = {{A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence}},
url = {http://arxiv.org/abs/2006.12567},
year = {2020}
}
@article{Petersen2012,
abstract = {Traditional mixed stock analyses use morphological, chemical, or genetic markers measured in several source populations and in a single mixed population to estimate the proportional contribution of each source to the mixed population. In many systems, however, different individuals from a particular source population may go to a variety of mixed populations. Now that data are becoming available from (meta)populations with multiple mixed stocks, the need arises to estimate contributions in this 'many-to-many' scenario. We suggest a Bayesian hierarchical approach, an extension of previous Bayesian mixed stock analysis algorithms, that can estimate contributions in this case. Applying the method to mitochondrial DNA data from green turtles (Chelonia mydas) in the Atlantic gives results that are largely consistent with previous results but makes some novel points, e.g. that the Florida, Bahamas and Corisco Bay foraging grounds have greater contributions than previously thought from distant foraging grounds. More generally, the 'many-to-many' approach gives a more complete understanding of the spatial ecology of organisms, which is especially important in species such as the green turtle that exhibit weak migratory connectivity (several distinct subpopulations at one end of the migration that mix in unknown ways at the other end). {\textcopyright} 2007 The Authors Journal compilation {\textcopyright} 2007 Blackwell Publishing Ltd.},
author = {Bolker, Benjamin M. and Okuyama, Toshinori and Bjorndal, Karen A. and Bolten, Alan B.},
doi = {10.1111/j.1365-294X.2006.03161.x},
isbn = {0962-1083 (Print)\r0962-1083 (Linking)},
issn = {09621083},
journal = {Molecular Ecology},
keywords = {Bayesian hierarchical model,Chelonia mydas,Connectivity,Mixed-stock analysis,Spatial population structure,mtDNA haplotype},
number = {4},
pages = {685--695},
pmid = {17284204},
title = {{Incorporating multiple mixed stocks in mixed stock analysis: 'Many-to-many' analyses}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Statistical+machine+learning+for+information+retrieval#5},
volume = {16},
year = {2007}
}
@article{Eade2013,
abstract = {Representation, exponential map, adjoint and jacobian of different manifolds with Lie algebra Different operations when sampling and statistics results when working with manifolds},
author = {Eade, Ethan},
file = {::},
journal = {URL http://ethaneade. com/lie. pdf, revised Dec},
pages = {1--24},
title = {{Lie Groups for 2D and 3D Transformations}},
year = {2013}
}
@article{Krajnik2019,
abstract = {This letter presents a novel method for introducing time into discrete and continuous spatial representations used in mobile robotics, by modeling long-term, pseudo-periodic variations caused by human activities or natural processes. Unlike previous approaches, the proposed method does not treat time and space separately, and its continuous nature respects both the temporal and spatial continuity of the modeled phenomena. The key idea is to extend the spatial model with a set of wrapped time dimensions that represent the periodicities of the observed events. By performing clustering over this extended representation, we obtain a model that allows the prediction of probabilistic distributions of future states and events in both discrete and continuous spatial representations. We apply the proposed algorithm to several long-term datasets acquired by mobile robots and show that the method enables a robot to predict future states of representations with different dimensions. The experiments further show that the method achieves more accurate predictions than the previous state of the art.},
archivePrefix = {arXiv},
arxivId = {1810.04285},
author = {Krajnik, Tomas and Vintr, Tomas and Molina, Sergi and Fentanes, Jaime Pulido and Cielniak, Grzegorz and Mozos, Oscar Martinez and Broughton, George and Duckett, Tom},
doi = {10.1109/LRA.2019.2926682},
eprint = {1810.04285},
file = {:home/matias/Documents/Mendeley Desktop/Krajnik et al/2019/Krajnik et al. - 2019 - Warped hypertime representations for long-term autonomy of mobile robots.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Learning and adaptive systems,Mapping,Service robots},
number = {4},
pages = {3310--3317},
publisher = {IEEE},
title = {{Warped hypertime representations for long-term autonomy of mobile robots}},
volume = {4},
year = {2019}
}
@article{Quadling1972,
abstract = {One of the great Zen masters had an eager disciple who never lost an opportunity to catch whatever pearls of wisdom might drop from the master's lips, and who followed him about constantly. One day, deferentially opening an iron gate for the old man, the disciple asked, 'How may I attain enlightenment?' The ancient sage, though withered and feeble, could be quick, and he deftly caused the heavy gate to shut on the pupil's leg, breaking it.},
author = {Quadling, D. A. and Linderholm, Carl E.},
doi = {10.2307/3617023},
file = {::},
isbn = {72340415-1},
issn = {00255572},
journal = {The Mathematical Gazette},
month = {oct},
number = {397},
pages = {255},
pmid = {21437032},
title = {{Mathematics Made Difficult}},
url = {http://www.jstor.org/stable/3617023?origin=crossref},
volume = {56},
year = {1972}
}
@article{Wei2014,
abstract = {Humanoid robot navigation in domestic environments remains a challenging task. In this paper, we present an approach for navigating such environments for the humanoid robot Nao. We assume that a map of the environment is given and focus on the localization task. The approach is based on the use only of odometry and a single camera. The camera is used to correct for the drift of odometry estimates. Additionally, scene-classification is used to obtain information about the robot's position when it gets close to the destination. The approach is tested in an office environment to demonstrate that it can be reliably used for navigation in a domestic environment. {\textcopyright} 2014 Springer-Verlag.},
author = {Wei, Changyun and Xu, Junchao and Wang, Chang and Wiggers, Pascal and Hindriks, Koen},
doi = {10.1007/978-3-662-43645-5_33},
isbn = {9783662436448},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Humanoid,Localization,Navigation},
pages = {298--310},
title = {{An approach to navigation for the humanoid robot Nao in domestic environments}},
volume = {8069 LNAI},
year = {2014}
}
@inproceedings{Ostafew2013,
abstract = {This paper presents a path-repeating, mobile robot controller that combines a feedforward, proportional Iterative Learning Control (ILC) algorithm with a feedback-linearized path-tracking controller to reduce path-tracking errors over repeated traverses along a reference path. Localization for the controller is provided by an on-board, vision-based mapping and navigation system enabling operation in large-scale, GPS-denied, extreme environments. The paper presents experimental results including over 600 m of travel by a four-wheeled, 50 kg robot travelling through challenging terrain including steep hills and sandy turns and by a six-wheeled, 160 kg robot at gradually-increased speeds up to three times faster than the nominal, safe speed. In the absence of a global localization system, ILC is demonstrated to reduce path-tracking errors caused by unmodelled robot dynamics and terrain challenges. {\textcopyright} 2013 IEEE.},
author = {Ostafew, Chris J. and Schoellig, Angela P. and Barfoot, Timothy D.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2013.6696350},
file = {:home/matias/Documents/Mendeley Desktop/Ostafew, Schoellig, Barfoot/2013/Ostafew, Schoellig, Barfoot - 2013 - Visual teach and repeat, repeat, repeat Iterative Learning Control to improve mobile robot path tra.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
pages = {176--181},
title = {{Visual teach and repeat, repeat, repeat: Iterative Learning Control to improve mobile robot path tracking in challenging outdoor environments}},
year = {2013}
}
@article{Work2015,
abstract = {We describe our full body humanoid control approach developed for the simulation phase of the DARPA Robotics Challenge (DRC), as well as the modifications made for the DARPA Robotics Challenge Trials. We worked with the Boston Dynamics Atlas robot. Our approach was initially targeted at walking, and it consisted of two levels of optimization: a high-level trajectory optimizer that reasons about center of mass and swing foot trajectories, and a low-level controller that tracks those trajectories by solving floating base full body inverse dynamics using quadratic programming. This controller is capable of walking on rough terrain, and it also achieves long footsteps, fast walking speeds, and heel-strike and toe-off in simulation. During development of these and other whole body tasks on the physical robot, we introduced an additional optimization component in the low-level controller, namely an inverse kinematics controller. Modeling and torque measurement errors and hardware features of the Atlas robot led us to this three-part approach, which was applied to three tasks in the DRC Trials in December 2013.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Feng, Siyuan and Whitman, Eric and Xinjilefu, X. and Atkeson, Christopher G.},
doi = {10.1002/rob.21559},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {293--312},
pmid = {22164016},
title = {{Optimization-based full body control for the DARPA Robotics Challenge}},
volume = {32},
year = {2015}
}
@inproceedings{Mur-Artal2015b,
abstract = {Simultaneous Localization and Mapping (SLAM) and Visual SLAM (V-SLAM) in particular have been an active area of research lately. In V-SLAM the main focus is most often laid on the localization part of the problem allowing for a drift free motion estimate. To this end, a sparse set of landmarks is tracked and their position is estimated. However, this set of landmarks (rendering the map) is often too sparse for tasks in autonomous driving such as navigation, path planning, obstacle avoidance etc. Some methods keep the raw measurements for past robot poses to address the sparsity problem often resulting in a pose only SLAM akin to laser scanner SLAM. For the stereo case, this is however impractical due to the high noise of stereo reconstructed point clouds. In this paper we propose a dense stereo V-SLAM algorithm that estimates a dense 3D map representation which is more accurate than raw stereo measurements. Thereto, we run a sparse VSLAM system, take the resulting pose estimates to compute a locally dense representation from dense stereo correspondences. This dense representation is expressed in local coordinate systems which are tracked as part of the SLAM estimate. This allows the dense part to be continuously updated. Our system is driven by visual odometry priors to achieve high robustness when tracking landmarks. Moreover, the sparse part of the SLAM system uses recently published sub mapping techniques to achieve constant runtime complexity most of the time. The improved accuracy over raw stereo measurements is shown in a Monte Carlo simulation. Finally, we demonstrate the feasibility of our method by presenting outdoor experiments of a car like robot. {\textcopyright} 2011 IEEE.},
author = {Lategahn, Henning and Geiger, Andreas and Kitt, Bernd},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5979711},
isbn = {9781612843865},
issn = {10504729},
number = {November},
pages = {1732--1737},
title = {{Visual SLAM for autonomous ground vehicles}},
url = {https://www.youtube.com/watch?v=HlBmq70LKrQ},
year = {2011}
}
@article{Martinez-Carranza2009,
abstract = {Point based visual SLAM suffers from a trade off between map density and computational efficiency. With too few mapped points, tracking range is restricted and resistance to occlusion is reduced, whilst expanding the map to give dense representation significantly increases computation. We address this by introducing higher order structure into the map using planar features. The parameterisation of structure allows frame by frame adaptation of measurements according to visibility criteria, increasing the map density without increasing computational load. This facilitates robust camera tracking over wide changes in viewpoint at significantly reduced computational cost. Results of real-time experiments with a hand-held camera demonstrate the effectiveness of the approach. {\textcopyright} 2009. The copyright of this document resides with its authors.},
author = {Mart{\'{i}}nez-Carranza, Jos{\'{e}} and Calway, Andrew},
doi = {10.5244/C.23.43},
isbn = {1901725391},
journal = {British Machine Vision Conference, BMVC 2009 - Proceedings},
pages = {1--11},
title = {{Efficiently increasing map density in visual SLAM using planar features with adaptive measurements}},
url = {http://www.cs.bris.ac.uk/Publications/Papers/2001079.pdf},
year = {2009}
}
@inproceedings{Booij2010,
abstract = {Estimating the relative pose between two camera positions given image point correspondences is a vital task in most view based SLAM and robot navigation approaches. In order to improve the robustness to noise and false point correspondences it is common to incorporate the constraint that the robot moves over a planar surface, as is the case for most indoor and outdoor mapping applications. We propose a novel estimation method that determines the full likelihood in the space of all possible planar relative poses. The likelihood function can be learned from existing data using standard Bayesian methods and is efficiently stored in a low dimensional look up table. Estimating the likelihood of a new pose given a set of correspondences boils down to a simple look up. As a result, the proposed method allows for very efficient creation of pose constraints for vision based SLAM applications, including a proper estimate of its uncertainty. It can handle ambiguous image data, such as acquired in long corridors, naturally. The method can be trained using either artificial or real data, and is applied on both controlled simulated data and challenging images taken in real home environments. By computing the maximum likelihood estimate we can compare our approach with state of the art estimators based on a combination of RANSAC and iterative reweighted least squares and show a significant increase in both the efficiency and accuracy.},
author = {Booij, Olaf and Kr{\"{o}}se, Ben and Zivkovic, Zoran},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/rss.2010.vi.026},
file = {::},
isbn = {9780262516815},
issn = {2330765X},
pages = {201--208},
title = {{Efficient probabilistic planar robot motion estimation given pairs of images}},
url = {http://www.science.uva.nl/research/isla/pub/Booij10rss.pdf},
volume = {6},
year = {2011}
}
@article{Hebert2015,
abstract = {This article presents the hardware design and software algorithms of RoboSimian, a statically stable quadrupedal robot capable of both dexterous manipulation and versatile mobility in difficult terrain. The robot has generalized limbs and hands capable of mobility and manipulation, along with almost fully hemispherical three-dimensional sensing with passive stereo cameras. The system is semiautonomous, enabling low-bandwidth, high latency control operated from a standard laptop. Because limbs are used formobility and manipulation, a single unified mobile manipulation planner is used to generate autonomous behaviors, including walking, sitting, climbing, grasping, and manipulating. The remote operator interface is optimized to designate, parametrize, sequence, and preview behaviors, which are then executed by the robot. RoboSimian placed fifth in the DARPA Robotics Challenge Trials, demonstrating its ability to perform disaster recovery tasks in degraded human environments.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Hebert, Paul and Bajracharya, Max and Ma, Jeremy and Hudson, Nicolas and Aydemir, Alper and Reid, Jason and Bergh, Charles and Borders, James and Frost, Matthew and Hagman, Michael and Leichty, John and Backes, Paul and Kennedy, Brett and Karplus, Paul and Satzinger, Brian and Byl, Katie and Shankar, Krishna and Burdick, Joel},
doi = {10.1002/rob.21566},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {255--274},
pmid = {22164016},
title = {{Mobile manipulation and mobility as manipulation - Design and algorithms of RoboSimian}},
volume = {32},
year = {2015}
}
@inproceedings{Pillai2015,
abstract = {In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.},
archivePrefix = {arXiv},
arxivId = {1506.01732},
author = {Pillai, Sudeep and Leonard, John J.},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.034},
eprint = {1506.01732},
isbn = {9780992374716},
issn = {2330765X},
keywords = {monocular,slam,vision},
pages = {34--42},
title = {{Monocular SLAM supported object recognition}},
volume = {11},
year = {2015}
}
@book{Wasserman2010,
abstract = {This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level.},
author = {Wasserman, Larry},
file = {:home/matias/Documents/Mendeley Desktop/Wasserman/2010/Wasserman - 2010 - All of Statistics A Concise Course in Statistical Inference (Springer Texts in Statistics).pdf:pdf},
isbn = {9781441923226},
pages = {461},
title = {{All of Statistics: A Concise Course in Statistical Inference (Springer Texts in Statistics)}},
url = {http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/1441923225},
year = {2010}
}
@phdthesis{Alcantarilla2011,
author = {Alcantarilla, Pablo F},
pages = {160},
school = {Universidad de Alcal{\'{a}}},
title = {{Vision Based Localization : From Humanoid Robots to Visually Impaired People}},
url = {http://www.robesafe.com/personal/pablo.alcantarilla/papers/Alcantarilla11PhD.pdf},
year = {2011}
}
@article{Sibley2010a,
abstract = {We are concerned with improving the range resolution of stereo vision for entry, descent, and landing (EDL) missions toMars and other planetary bodies. The goal is to create accurate and precise three-dimensional planetary surface-structure estimates by filtering sequences of stereo images taken from an autonomous landing vehicle.We describe a sliding window filter (SWF) approach based on delayed state marginalization. The SWF can run in constant time, yet still achieve experimental results close to those of the bundle adjustment solution. This technique can scale from the offline batch least-squares solution to fast online incremental solutions. For instance, if the window encompasses all time, the solution is equivalent to full bundle adjustment; if only one time step is maintained, the solution matches the extended Kalman filter; if poses and landmarks are slowly marginalized out over time such that the state vector ceases to grow, then the filter becomes constant time, like visual odometry. Within the constant time regime, the sliding window approach demonstrates convergence properties that are close to those of the full batch solution and strictly superior to visual odometry. Experiments with real data show that ground structure estimates follow the expected convergence pattern that is predicted by theory. These experiments indicate the effectiveness of filtering long-range stereo for EDL. {\textcopyright} 2010 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Sibley, Gabe and Matthies, Larry and Sukhatme, Gaurav},
doi = {10.1002/rob.20360},
eprint = {10.1.1.91.5767},
file = {::},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {jul},
number = {5},
pages = {587--608},
pmid = {22164016},
title = {{Sliding window filter with application to planetary landing}},
url = {http://doi.wiley.com/10.1002/rob.20360},
volume = {27},
year = {2010}
}
@techreport{Dellaert2016a,
abstract = {We will start with a small example of a robot moving in a plane, parameterized by a 2D pose (x, y, q). When we give it a small forward velocity v x , we know that the location changes as ˙ x = v x The solution to this trivial differential equation is, with x 0 the initial x-position of the robot, x t = x 0 + v x t A similar story holds for translation in the y direction, and in fact for translations in general: (x t , y t , q t) = (x 0 + v x t, y 0 + v y t, q 0) Similarly for rotation we have (x t , y t , q t) = (x 0 , y 0 , q 0 + wt) where w is angular velocity, measured in rad/s in counterclockwise direction. However, if we combine translation and rotation, the story breaks down! We cannot write (x t , y t , q t) = (x 0 + v x t, y 0 + v y t, q 0 + wt) The reason is that, if we move the robot a tiny bit according to the velocity vector (v x , v y , w), we have (to first order) (x d , y d , q d) = (x 0 + v x d , y 0 + v y d , q 0 + wd) but now the robot has rotated, and for the next incremental change, the velocity vector would have to be rotated before it can be applied. In fact, the robot will move on a circular trajectory. The reason is that translation and rotation do not commute: if we rotate and then move we will end up in a different place than if we moved first, then rotated. In fact, someone once said (I forget who, kudos for who can track down the exact quote): If rotation and translation commuted, we could do all rotations before leaving home.},
author = {Dellaert, Frank},
booktitle = {arXiv},
file = {::},
institution = {Georgia Institute of Technology},
pages = {1--21},
title = {{Lie Groups for Beginners}},
year = {2016}
}
@phdthesis{Mur-Artal2017,
author = {Mur-Artal, Ra{\'{u}}l},
booktitle = {Phd Paper},
school = {Universidad de Zaragoza},
title = {{(ORB-SLAM)Real-Time Accurate Visual SLAM with Place Recognition}},
year = {2017}
}
@article{Vega-Brown2013a,
abstract = {We present an algorithm for providing a dynamic model of sensor measurements. Rather than depending on a model of the vehicle state and environment to capture the distribution of possible sensor measurements, we provide an approximation that allows the sensor model to depend on the measurement itself. Building on previous work, we show how the sensor model predictor can be learned from data without access to ground truth labels of the vehicle state or true underlying distribution, and we show our approach to be a generalization of non-parametric kernel regressors. Our algorithm is demonstrated in simulation and on real world data for both laser-based scan matching odometry and RGB-D camera odometry in an unknown map. The performance of our algorithm is shown to quantitatively improve estimation, both in terms of consistency and absolute accuracy, relative to other algorithms and to fixed covariance models. {\textcopyright} 2013 IEEE.},
author = {Vega-Brown, William and Roy, Nicholas},
doi = {10.1109/IROS.2013.6696609},
file = {:home/matias/Documents/Mendeley Desktop/Vega-Brown, Roy/2013/Vega-Brown, Roy - 2013 - CELLO-EM Adaptive sensor models without ground truth.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1907--1914},
title = {{CELLO-EM: Adaptive sensor models without ground truth}},
year = {2013}
}
@article{Liu2018,
abstract = {We present a visual odometry (VO) algorithm for a multi-camera system and robust operation in challenging environments. Our algorithm consists of a pose tracker and a local mapper. The tracker estimates the current pose by minimizing photometric errors between the most recent keyframe and the current frame. The mapper initializes the depths of all sampled feature points using plane-sweeping stereo. To reduce pose drift, a sliding window optimizer is used to refine poses and structure jointly. Our formulation is flexible enough to support an arbitrary number of stereo cameras. We evaluate our algorithm thoroughly on five datasets. The datasets were captured in different conditions: daytime, night-time with near-infrared (NIR) illumination and nighttime without NIR illumination. Experimental results show that a multi-camera setup makes the VO more robust to challenging environments, especially night-time conditions, in which a single stereo configuration fails easily due to the lack of features.},
author = {Liu, Peidong and Geppert, Marcel and Heng, Lionel and Sattler, Torsten and Geiger, Andreas and Pollefeys, Marc},
doi = {10.1109/IROS.2018.8593561},
file = {:home/matias/Documents/Mendeley Desktop/Liu et al/2018/Liu et al. - 2018 - Towards Robust Visual Odometry with a Multi-Camera System.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1154--1161},
title = {{Towards Robust Visual Odometry with a Multi-Camera System}},
year = {2018}
}
@inproceedings{Sarlin2019a,
abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
archivePrefix = {arXiv},
arxivId = {1911.11763},
author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR42600.2020.00499},
eprint = {1911.11763},
file = {:home/matias/Documents/Mendeley Desktop/Sarlin et al/2020/Sarlin et al. - 2020 - SuperGlue Learning Feature Matching With Graph Neural Networks.pdf:pdf},
isbn = {978-1-7281-7168-5},
month = {jun},
pages = {4937--4946},
publisher = {IEEE},
title = {{SuperGlue: Learning Feature Matching With Graph Neural Networks}},
url = {http://arxiv.org/abs/1911.11763 https://ieeexplore.ieee.org/document/9157489/},
year = {2020}
}
@article{Reinstein2013,
abstract = {It is an important ability for any mobile robot to be able to estimate its posture and to gauge the distance it traveled. In this paper, we have addressed this problem in a dynamic quadruped robot by combining traditional state estimation methods with machine learning. We have designed and implemented a navigation algorithm for full body state (position, velocity, and attitude) estimation that uses no external reference but relies on multimodal proprioceptive sensory information only. The extended Kalman filter (EKF) was used to provide error estimation and data fusion from two independent sources of information: 1) strapdown mechanization algorithm processing raw inertial data and 2) legged odometry. We have devised a novel legged odometer that combines information from a multimodal combination of sensors (joint and pressure). We have shown our method to work for a dynamic turning gait, and we have also successfully demonstrated how it generalizes to different velocities and terrains. Furthermore, our solution proved to be immune to substantial slippage of the robot's feet. {\textcopyright} 2004-2012 IEEE.},
author = {Reinstein, Michal and Hoffmann, Matej},
doi = {10.1109/TRO.2012.2228309},
file = {::},
isbn = {1552-3098 VO - PP},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Dead reckoning,extended Kalman filter (EKF),legged robots,odometry,path integration,slippage},
number = {2},
pages = {563--571},
title = {{Dead reckoning in a dynamic quadruped robot based on multimodal proprioceptive sensory information}},
volume = {29},
year = {2013}
}
@book{Horn1986,
abstract = {Robot vision is widely studied in Japan to realize flexible manufacturing systems. The study of robot vision includes the development of the following themes: input devices for range information, feature extraction for inpsection or positioning, high-speed image processors, stereo vision, three dimensional shape recovery, object recognition by model matching, and so forth. This paper describes some interesting work in these fields. {\textcopyright} 1986.},
author = {Shirai, Y.},
booktitle = {Robotics},
doi = {10.1016/0167-8493(86)90028-8},
isbn = {0262081598},
issn = {01678493},
keywords = {Feature extraction,Image processors,Input device,Model matching,Object recognition,Shape recovery,Stereo visio},
number = {3},
pages = {175--203},
publisher = {MIT Press},
title = {{Robot vision}},
url = {http://www.amazon.com/dp/0262081598},
volume = {2},
year = {1986}
}
@article{Li2006,
abstract = {Estimating relative camera motion from two calibrated views is a classical problem in computer vision. The minimal case for such problem is the so-called five-point problem, for which the state-of-the-art solution is Nist{\'{e}}r's algorithm [1][2]. However, due to the heuristic nature of the procedures it applies, to implement it needs much effort for non-expert user. This paper provides a simpler algorithm based on the hidden variable resultant technique. Instead of eliminating the unknown variables one by one (i.e, sequentially) using the Gauss-Elimination as in [1], our algorithm eliminates many unknowns at once. Moreover, in the equation solving stage, instead of back-substituting and solve all the unknowns sequentially, we compute the minimal singular vector of the coefficient matrix, by which all the unknown parameters can be estimated simultaneously. Experiments on both simulation and real images have validated the new algorithm. {\textcopyright} 2006 IEEE.},
author = {Li, Hongdong and Hartley, Richard},
doi = {10.1109/ICPR.2006.579},
file = {::},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {630--633},
title = {{Five-point motion estimation made easy}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1698971},
volume = {1},
year = {2006}
}
@techreport{Valtazanos2012,
abstract = {This paper outlines the organization and architecture of our robotic soccer team, Team Edinferno, and serves as the qualification document for the 2012 RoboCup Standard Platform League competition. We competed for the first time in the RoboCup 2011 competition held in Istanbul, Turkey. Our primary research interests are centered on issues of robot learning, especially for effec-tive autonomous decision making and strategic behaviour in continually changing worlds. With this in mind, our entry this year will leverage the B-Human frame-work to provide a software base and modules for walking and low-level vision. Based on this, we are developing our own behaviour module, multi-robot local-ization and coordinated motion as well as specialized behaviours such as for the goal keeper, kicking and ball handling.},
author = {Valtazanos, Aris and Vafeias, Efstathios and Mico, Alejandro Bordallo and Mankowitz, Daniel and Nardelli, Nantas and Ramamoorthy, Subramanian and Vijayakumar, Sethu},
number = {Figure 1},
title = {{Team Edinferno Description Paper for RoboCup 2013 SPL}},
url = {http://wcms.inf.ed.ac.uk/ipab/robocup/research/EdinfernoTDDoc2012.pdf},
year = {2013}
}
@article{Abbott1992,
author = {Abbott, Edwin Abbott},
file = {::},
title = {{Flatland}},
year = {1992}
}
@inproceedings{Swedish2018,
abstract = {We propose an approach for solving Visual Teach and Repeat tasks for routes that consist of discrete directions along path networks using deep learning. Visual paths are specified by a single monocular image sequence and our approach does not query frames or image features during inference, but instead is composed of classifiers trained on each path. Our method is efficient for both storing or following paths and enables sharing of visual path specifications between parties without sharing visual data explicitly. We evaluate our approach in a simulated environment, and present qualitative results on real data captured with a smartphone.},
author = {Swedish, Tristan and Raskar, Ramesh},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00203},
file = {:home/matias/Documents/Mendeley Desktop/Swedish, Raskar/2018/Swedish, Raskar - 2018 - Deep visual teach and repeat on path networks.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
pages = {1614--1623},
title = {{Deep visual teach and repeat on path networks}},
volume = {2018-June},
year = {2018}
}
@article{Semini2010,
abstract = {Legged machines promise greater mobility in rough and unstructured ter- rain then wheeled vehicles. In the future, especially quadruped robots are expected to be employed for a variety of dangerous and dirty tasks in ﬁelds like search and rescue, humanitarian demining etc. The objective of this dissertation is to make a signiﬁcant contribution toward the development of a highly dynamic quadruped robot. This versatile platform is intended to serve as a tool to deepen the understanding of terrestrial locomotion, to assess the applicability of diﬀerent hydraulic actuation systems to legged robots and to facilitate the future construction of useful robots for various tasks. To this end, this dissertation:},
author = {Semini, Claudio},
journal = {Darwin},
number = {April},
pages = {210},
title = {{HyQ - Design and Development of a Hydraulically Actuated Quadruped Robot}},
url = {http://www.gizmag.com/iit-hyq-quadruped-robot-reflexes/28545/%5Cnhttp://www.skyscrapercity.com/showthread.php?t=1579498&page=5%255Cnhttp://www.iit.it/en/advr-dls-research/dls-robot-design-and-development.html%255Cnhttps://www.youtube.com/watch?v=wPxXwYGZm},
year = {2010}
}
@article{Ashwood2020,
author = {Ashwood, Zoe C and Roy, Nicholas A},
file = {:home/matias/Documents/Mendeley Desktop/Ashwood, Roy/2020/Ashwood, Roy - 2020 - Inferring learning rules from animal decision-making.pdf:pdf},
number = {NeurIPS},
pages = {1--11},
title = {{Inferring learning rules from animal decision-making}},
year = {2020}
}
@inproceedings{Kneip2011,
abstract = {The increasing demand for realtime high-precision Visual Odometry (VO) systems as part of navigation and localization tasks has recently been driving research towards more versatile and scalable solutions. In this paper, we present a novel framework for combining the ... },
author = {Kneip, Laurent and Chli, Margarita and Siegwart, Roland},
booktitle = {Procedings of the British Machine Vision Conference 2011},
doi = {10.5244/c.25.16},
isbn = {1-901725-43-X},
pages = {16.1--16.11},
publisher = {British Machine Vision Association},
title = {{Robust Real-Time Visual Odometry with a Single Camera and an IMU}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper16/index.html},
year = {2011}
}
@article{Loquercio2020,
abstract = {Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23% in accuracy. The video available at https://youtu.be/X7n-bRS5vSM shows qualitative results of our experiments. The project's code is available at: https://tinyurl.com/s3nygw7.},
archivePrefix = {arXiv},
arxivId = {1907.06890},
author = {Loquercio, Antonio and Segu, Mattia and Scaramuzza, Davide},
doi = {10.1109/LRA.2020.2974682},
eprint = {1907.06890},
file = {:home/matias/Documents/Mendeley Desktop/Loquercio, Segu, Scaramuzza/2020/Loquercio, Segu, Scaramuzza - 2020 - A General Framework for Uncertainty Estimation in Deep Learning.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {AI-based methods,Deep learning in robotics and automation,probability and statistical methods},
number = {2},
pages = {3153--3160},
title = {{A General Framework for Uncertainty Estimation in Deep Learning}},
volume = {5},
year = {2020}
}
@article{Hartley1997,
abstract = {In this paper, we consider the problem of finding the position of a point in space given its position in two images taken with cameras with known calibration and pose. This process requires the intersection of two known rays in space and is commonly known as triangulation. In the absence of noise, this problem is trivial. When noise is present, the two rays will not generally meet, in which case it is necessary to find the best point of intersection. This problem is especially critical in affine and projective reconstruction in which there is no meaningful metric information about the object space. It is desirable to find a triangulation method that is invariant to projective transformations of space. This paper solves that problem by assuming a Gaussian noise model for perturbation of the image coordinates. The triangulation problem may then be formulated as a least-squares minimization problem. In this paper a noniterative solution is given that finds the global minimum. It is shown that in certain configurations, local minima occur, which are avoided by the new method. Extensive comparisons of the new method with several other methods show that it consistently gives superior results. {\textcopyright} 1997 Academic Press.},
author = {Hartley, Richard I. and Sturm, Peter},
doi = {10.1006/cviu.1997.0547},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
number = {2},
pages = {146--157},
pmid = {20633411},
title = {{Triangulation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314297905476},
volume = {68},
year = {1997}
}
@article{Thrun2004,
abstract = {In this paper we describe a scalable algorithm for the simultaneous mapping and localization (SLAM) problem. SLAM is the problem of acquiring a map of a static environment with a mobile robot. The vast majority of SLAM algorithms are based on the extended Kalman filter (EKF). In this paper we advocate an algorithm that relies on the dual of the EKF, the extended information filter (EIF). We show that when represented in the information form, map posteriors are dominated by a small number of links that tie together nearby features in the map. This insight is developed into a sparse variant of the EIF, called the sparse extended information filter (SEIF). SEIFs represent maps by graphical networks of features that are locally interconnected, where links represent relative information between pairs of nearby features, as well as information about the robot's pose relative to the map. We show that all essential update equations in SEIFs can be executed in constant time, irrespective of the size of the map. We also provide empirical results obtained for a benchmark data set collected in an outdoor environment, and using a multi-robot mapping simulation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian and Liu, Yufeng and Koller, Daphne and Ng, Andrew Y. and Ghahramani, Zoubin and Durrant-Whyte, Hugh},
doi = {10.1177/0278364904045479},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Filters,Information filters,Kalman filters,Mapping,Mobile robotics,Multi-robot systems,Robot learning,Robotic perception,SLAM},
number = {7-8},
pages = {693--716},
pmid = {222793900003},
title = {{Simultaneous localization and mapping with sparse extended information filters}},
url = {http://journals.sagepub.com/doi/10.1177/0278364904045479},
volume = {23},
year = {2004}
}
@article{Jaeger2014,
abstract = {Conceptors provide an elementary neuro-computational mechanism which sheds a fresh and unifying light on a diversity of cognitive phenomena. A number of demanding learning and processing tasks can be solved with unprecedented ease, robustness and accuracy. Some of these tasks were impossible to solve before. This entirely informal paper introduces the basic principles of conceptors and highlights some of their usages.},
archivePrefix = {arXiv},
arxivId = {1406.2671},
author = {Jaeger, Herbert},
eprint = {1406.2671},
file = {::},
pages = {1--11},
title = {{Conceptors: an easy introduction}},
url = {http://arxiv.org/abs/1406.2671},
year = {2014}
}
@article{Iandola2016,
abstract = {Functional brain connectivity, as revealed through distant correlations in the signals measured by functional Magnetic Resonance Imaging (fMRI), is a promising source of biomarkers of brain pathologies. However, establishing and using diagnostic markers requires probabilistic inter-subject comparisons. Principled comparison of functional-connectivity structures is still a challenging issue. We give a new matrix-variate probabilistic model suitable for inter-subject comparison of functional connectivity matrices on the manifold of Symmetric Positive Definite (SPD) matrices. We show that this model leads to a new algorithm for principled comparison of connectivity coefficients between pairs of regions. We apply this model to comparing separately post-stroke patients to a group of healthy controls. We find neurologically-relevant connection differences and show that our model is more sensitive that the standard procedure. To the best of our knowledge, these results are the first report of functional connectivity differences between a single-patient and a group and thus establish an important step toward using functional connectivity as a diagnostic tool. {\textcopyright} 2010 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {1008.5070},
author = {Varoquaux, Ga{\"{e}}l and Baronnet, Flore and Kleinschmidt, Andreas and Fillard, Pierre and Thirion, Bertrand},
doi = {10.1007/978-3-642-15705-9_25},
eprint = {1008.5070},
file = {::},
isbn = {3642157041},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {feature extraction,machine learning},
month = {feb},
number = {PART 1},
pages = {200--208},
pmid = {20879232},
title = {{Detection of brain functional-connectivity difference in post-stroke patients using group-level covariance modeling}},
url = {http://arxiv.org/abs/1602.07360 http://link.springer.com/10.1007/978-3-319-24553-9 http://arxiv.org/abs/1008.5070 http://dx.doi.org/10.1007/978-3-642-15705-9_25},
volume = {6361 LNCS},
year = {2010}
}
@article{Thrun2005,
abstract = {Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles.},
author = {Thrun, Sebastian},
doi = {10.1145/504729.504754},
isbn = {0262201623},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {52--57},
title = {{Probabilistic robotics}},
volume = {45},
year = {2002}
}
@article{Toussaint2010,
abstract = {The problem of motion control and planning can be formulated as an optimization problem. In this paper we discuss an alternative view that casts the problem as one of probabilistic inference. In simple cases where the optimization problem can be solved analytically the inference view leads to equivalent solutions. However, when approximate methods are necessary to tackle the problem, the tight relation between optimization and probabilistic inference has fruitfully lead to a transfer of methods between both fields. Here we show that such a transfer is also possible in the realm of robotics. The general idea is that motion can be generated by fusing motion objectives (task constraints, goals, motion priors) by using probabilistic inference techniques. In realistic scenarios exact inference is infeasible (as is the analytic solution of the corresponding optimization problem) and the use of efficient approximate inference methods is a promising alternative to classical motion optimization methods. In this paper we first derive Bayesian control methods that are directly analogous to classical redundant motion rate control and optimal dynamic control (including operational space control). Then, by extending the probabilistic models to be Markovian models of the whole trajectory, we show that approximate probabilistic inference methods (message passing) efficiently compute solutions to trajectory optimization problems. Using Gaussian belief approximations and local linearization the algorithm becomes related to Differential Dynamic Programming (DDP) (aka. iterative Linear Quadratic Gaussian (iLQG)). {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Toussaint, Marc and Goerick, Christian},
doi = {10.1007/978-3-642-05181-4_11},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint, Goerick/2010/Toussaint, Goerick - 2010 - A bayesian view on motor control and planning.pdf:pdf},
isbn = {9783642051807},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {227--252},
title = {{A bayesian view on motor control and planning}},
volume = {264},
year = {2010}
}
@article{Zhen2020,
author = {Zhen, Weikun and Scherer, Sebastian},
file = {:home/matias/Documents/Mendeley Desktop/Zhen, Scherer/2020/Zhen, Scherer - 2020 - LiDAR-enhanced Structure-from-Motion.pdf:pdf},
isbn = {9781728173955},
pages = {6773--6779},
title = {{LiDAR-enhanced Structure-from-Motion}},
year = {2020}
}
@article{Laue2006,
abstract = {This paper describes SimRobot, a robot simulator which is able to simulate arbitrary user-defined robots in three-dimensional space. It includes a physical model which is based on rigid body dynamics. To allow an extensive flexibility in building accurate models, a variety of different generic bodies, sensors and actuators has been implemented. Furthermore, the simulator follows an user-oriented approach by including several mechanisms for visualization, direct actuator manipulation, and interaction with the simulated world. To demonstrate the general approach, this paper presents multiple examples of different robots which have been simulated so far. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Laue, Tim and Spiess, Kai and R{\"{o}}fer, Thomas},
doi = {10.1007/11780519_16},
file = {::},
isbn = {9783540354376},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {173--183},
title = {{SimRobot - A general physical robot simulator and its application in RoboCup}},
url = {http://www.springerlink.com/index/wh81484n700q3q3h.pdf},
volume = {4020 LNAI},
year = {2006}
}
@article{Ma2016,
abstract = {We address the following question: is it possible to reconstruct the geometry of an unknown environment using sparse and incomplete depth measurements? This problem is relevant for a resource-constrained robot that has to navigate and map an environment, but does not have enough on-board power or payload to carry a traditional depth sensor (e.g., a 3D lidar) and can only acquire few (point-wise) depth measurements. In general, reconstruction from incomplete data is not possible, but when the robot operates in man-made environments, the depth exhibits some regularity (e.g., many planar surfaces with few edges); we leverage this regularity to infer depth from incomplete measurements. Our formulation bridges robotic perception with the compressive sensing literature in signal processing. We exploit this connection to provide formal results on exact depth recovery in 2D and 3D problems. Taking advantage of our specific sensing modality, we also prove novel and more powerful results to completely characterize the geometry of the signals that we can reconstruct. Our results directly translate to practical algorithms for depth reconstruction; these algorithms are simple (they reduce to solving a linear program), and robust to noise. We test our algorithms on real and simulated data, and show that they enable accurate depth reconstruction from a handful of measurements, and perform well even when the assumption of structured environment is violated.},
archivePrefix = {arXiv},
arxivId = {1703.01398},
author = {Ma, Fangchang and Carlone, Luca and Ayaz, Ulas and Karaman, Sertac},
doi = {10.1109/IROS.2016.7759040},
eprint = {1703.01398},
file = {::},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {96--103},
title = {{Sparse sensing for resource-constrained depth reconstruction}},
volume = {2016-Novem},
year = {2016}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
eprint = {1504.00702},
file = {:home/matias/Documents/Mendeley Desktop/Levine et al/2016/Levine et al. - 2016 - End-to-end training of deep visuomotor policies.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural networks,Optimal control,Reinforcement learning,Vision},
pages = {1--40},
title = {{End-to-end training of deep visuomotor policies}},
volume = {17},
year = {2016}
}
@inproceedings{Whelan2015,
abstract = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.},
author = {Whelan, Thomas and Leutenegger, Stefan and Salas-Moreno, Renato F. and Glocker, Ben and Davison, Andrew J.},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.001},
isbn = {9780992374716},
issn = {2330765X},
title = {{ElasticFusion: Dense SLAM without a pose graph}},
volume = {11},
year = {2015}
}
@article{Popovic2005,
abstract = {The zero moment point (ZMP), foot rotation indicator (FRI) and centroidal moment pivot (CMP) are important ground reference points used for motion identification and control in biomechanics and legged robotics. In this paper, we study these reference points for normal human walking, and discuss their applicability in legged machine control. Since the FRI was proposed as an indicator of foot rotation, we hypothesize that the FRI will closely track the ZMP in early single support when the foot remains flat on the ground, but will then significantly diverge from the ZMP in late single support as the foot rolls during heel-off. Additionally, since spin angular momentum has been shown to remain small throughout the walking cycle, we hypothesize that the CMP will never leave the ground support base throughout the entire gait cycle, closely tracking the ZMP. We test these hypotheses using a morphologically realistic human model and kinetic and kinematic gait data measured from ten human subjects walking at self-selected speeds. We find that the mean separation distance between the FRI and ZMP during heel-off is within the accuracy of their measurement (0.1% of foot length). Thus, the FRI point is determined not to be an adequate measure of foot rotational acceleration and a modified FRI point is proposed. Finally, we find that the CMP never leaves the ground support base, and the mean separation distance between the CMP and ZMP is small (14% of foot length), highlighting how closely the human body regulates spin angular momentum in level ground walking. {\textcopyright} 2005 SAGE Publications.},
author = {Popovic, Marko B. and Goswami, Ambarish and Herr, Hugh},
doi = {10.1177/0278364905058363},
file = {:home/matias/Documents/Mendeley Desktop/Popovic, Goswami, Herr/2005/Popovic, Goswami, Herr - 2005 - Ground reference points in legged locomotion Definitions, biological trajectories and control implicatio.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Biomechanics,Center of pressure,Centroidal moment pivot,Control,Foot rotation indicator,Human,Legged locomotion,Zero moment point},
number = {12},
pages = {1013--1032},
title = {{Ground reference points in legged locomotion: Definitions, biological trajectories and control implications}},
volume = {24},
year = {2005}
}
@techreport{Moravec1980,
abstract = {The Stanford Al Lab cart is a card-table sized mobile robot controlled remotely through a radio link, and equipped with a TV camera and transmitter. A computer has been programmed to drive the cart through cluttered indoor and outdoor spaces, gaining its knowledge of the world entirely from images broadcast by the onboard TV system. The cart uses several kinds of stereo to locate objects around it in 3D and to deduce its own motion. It plans an obstacle avoiding path to a desired destination on the basis of a model built with this information. The plan changes as the cart perceives new obstacles on its journey . The system is reliable for short runs, but slow. The cart moves one meter every ten to fifteen minutes, in lurches. After rolling a meter it stops, takes some pictures and thinks about them for a long time. Then it plans a new path, executes a little of it, and pauses again. The program has successfully driven the cart through several 20 meter indoor courses (each taking about five hours) complex enough to necessitate three or four avoiding swerves. A less successful outdoor run, in which the cart skirted two obstacles but collided with a third, was also done. Harsh lighting (very bright surfaces next to very dark shadows) giving poor pictures and movement of shadows during the cart's creeping progress were major reasons for the poorer outdoor performance. The action portions of these runs were filmed by computer controlled cameras.},
author = {Moravec, Hans P.},
booktitle = {Technical Report CMU-RI-TR-80-03},
doi = {ADA092604},
file = {::},
institution = {Carnegie Mellon University},
pages = {1--175},
title = {{Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover, Tech. Report, Robotics Institute, Carnegie-Mellon University, pp.1-175}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA092604},
year = {1980}
}
@article{Heess2017,
abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .},
archivePrefix = {arXiv},
arxivId = {1707.02286},
author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
eprint = {1707.02286},
file = {::},
journal = {arXiv},
title = {{Emergence of Locomotion Behaviours in Rich Environments}},
url = {http://arxiv.org/abs/1707.02286},
year = {2017}
}
@article{Censi2015,
abstract = {Learning and adaptivity will play a large role in robotics in the future. Two questions are open: (1) in principle, how much it is possible to learn; and (2) in practice, how much should an agent be able to learn. The bootstrapping scenario describes the extreme case in which agents need to learn "everything" from scratch, including a torque-to-pixels model for its robotic body. This paper considers the bootstrapping problem for a subset of the set of all robots. The Simple Vehicles are an idealization of mobile robots equipped with a set of "canonical" exteroceptive sensors: the camera, the range finder and the field sampler. The sensorimotor dynamics of these sensors are derived and shown to be surprising similar. These sensorimotor dynamics are well approximated by a class of nonlinear systems that assume an instantaneous bilinear relation among observations, commands, and changes in the observations. The bilinear approximation is sufficient to guarantee success in the task of generalized "servoing": driving the observations to a given goal snapshot. Simulations and experiments substantiate the theoretical results. This is the first instance of a bootstrapping agent that can learn the model of the dynamics of a relatively large universe of systems and use the models to solve well-defined tasks, with no parameter tuning or hand-designed features.},
author = {Censi, Andrea and Murray, Richard M.},
doi = {10.1177/0278364914557708},
file = {::},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Learning,bootstrapping,sensorimotor dynamics,vehicles},
number = {8},
pages = {1087--1113},
title = {{Bootstrapping bilinear models of Simple Vehicles}},
volume = {34},
year = {2015}
}
@article{Bloesch2018,
abstract = {The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only. We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.},
archivePrefix = {arXiv},
arxivId = {1804.00874},
author = {Bloesch, Michael and Czarnowski, Jan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J.},
doi = {10.1109/CVPR.2018.00271},
eprint = {1804.00874},
file = {::},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2560--2568},
title = {{CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM}},
url = {http://arxiv.org/abs/1804.00874},
year = {2018}
}
@article{Matthies2007,
abstract = {Increasing the level of spacecraft autonomy is essential for broadening the reach of solar system exploration. Computer vision has and will continue to play an important role in increasing autonomy of both spacecraft and Earth-based robotic vehicles. This article addresses progress on computer vision for planetary rovers and landers and has four main parts. First, we review major milestones in the development of computer vision for robotic vehicles over the last four decades. Since research on applications for Earth and space has often been closely intertwined, the review includes elements of both. Second, we summarize the design and performance of computer vision algorithms used on Mars in the NASA/JPL Mars Exploration Rover (MER) mission, which was a major step forward in the use of computer vision in space. These algorithms did stereo vision and visual odometry for rover navigation and feature tracking for horizontal velocity estimation for the landers. Third, we summarize ongoing research to improve vision systems for planetary rovers, which includes various aspects of noise reduction, FPGA implementation, and vision-based slip perception. Finally, we briefly survey other opportunities for computer vision to impact rovers, landers, and orbiters in future solar system exploration missions. {\textcopyright} 2007 Springer Science+Business Media, LLC.},
author = {Matthies, Larry and Maimone, Mark and Johnson, Andrew and Cheng, Yang and Willson, Reg and Villalpando, Carlos and Goldberg, Steve and Huertas, Andres and Stein, Andrew and Angelova, Anelia},
doi = {10.1007/s11263-007-0046-z},
file = {::},
isbn = {0920569115731405},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Obstacle detection,Planetary exploration,Slip prediction,Stereo vision,Visual odometry,Visual velocity estimation},
number = {1},
pages = {67--92},
pmid = {25802067},
title = {{Computer vision on Mars}},
volume = {75},
year = {2007}
}
@article{Longuet-Higgins1981,
abstract = {A simple algorithm for computing the three-dimensional structure of a scene from a correlated pair of perspective projections is described here, when the spatial relationship between the two projections is unknown. This problem is relevant not only to photographic surveying1 but also to binocular vision2, where the non-visual information available to the observer about the orientation and focal length of each eye is much less accurate than the optical information supplied by the retinal images themselves. The problem also arises in monocular perception of motion3, where the two projections represent views which are separated in time as well as space. As Marr and Poggio4 have noted, the fusing of two images to produce a three-dimensional percept involves two distinct processes: the establishment of a 1:1 correspondence between image points in the two views - the 'correspondence problem' - and the use of the associated disparities for determining the distances of visible elements in the scene. I shall assume that the correspondence problem has been solved; the problem of reconstructing the scene then reduces to that of finding the relative orientation of the two viewpoints. {\textcopyright} 1981 Nature Publishing Group.},
author = {Longuet-higgins, H. C.},
doi = {10.1038/293133a0},
issn = {00280836},
journal = {Nature},
month = {sep},
number = {5828},
pages = {133--135},
title = {{A computer algorithm for reconstructing a scene from two projections}},
url = {http://www.nature.com/doifinder/10.1038/293133a0},
volume = {293},
year = {1981}
}
@article{Avena-Koenigsberger2017,
abstract = {Neuronal signalling and communication underpin virtually all aspects of brain activity and function. Network science approaches to modelling and analysing the dynamics of communication on networks have proved useful for simulating functional brain connectivity and predicting emergent network states. This Review surveys important aspects of communication dynamics in brain networks. We begin by sketching a conceptual framework that views communication dynamics as a necessary link between the empirical domains of structural and functional connectivity. We then consider how different local and global topological attributes of structural networks support potential patterns of network communication, and how the interactions between network topology and dynamic models can provide additional insights and constraints. We end by proposing that communication dynamics may act as potential generative models of effective connectivity and can offer insight into the mechanisms by which brain networks transform and process information.},
author = {Avena-Koenigsberger, Andrea and Misic, Bratislav and Sporns, Olaf},
doi = {10.1038/nrn.2017.149},
file = {::},
issn = {14710048},
journal = {Nature Reviews Neuroscience},
number = {1},
pages = {17--33},
pmid = {29238085},
publisher = {Nature Publishing Group},
title = {{Communication dynamics in complex brain networks}},
url = {http://www.nature.com/doifinder/10.1038/nrn.2017.149},
volume = {19},
year = {2018}
}
@article{Fallon2015,
abstract = {The DARPA Robotics Challenge Trials held in December 2013 provided a landmark demonstration of dexterous mobile robots executing a variety of tasks aided by a remote human operator using only data from the robot's sensor suite transmitted over a constrained, field-realistic communications link. We describe the design considerations, architecture, implementation, and performance of the software that Team MIT developed to command and control an Atlas humanoid robot. Our design emphasized human interaction with an efficient motion planner, where operators expressed desired robot actions in terms of affordances fit using perception and manipulated in a custom user interface. We highlight several important lessons we learned while developing our system on a highly compressed schedule.},
author = {Fallon, Maurice and Kuindersma, Scott and Karumanchi, Sisir and Antone, Matthew and Schneider, Toby and Dai, Hongkai and D'Arpino, Claudia P{\'{e}}rez and Deits, Robin and DiCicco, Matt and Fourie, Dehann and Koolen, Twan and Marion, Pat and Posa, Michael and Valenzuela, Andr{\'{e}}s and Yu, Kuan Ting and Shah, Julie and Iagnemma, Karl and Tedrake, Russ and Teller, Seth},
doi = {10.1002/rob.21546},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {mar},
number = {2},
pages = {229--254},
title = {{An architecture for online affordance-based perception and whole-body planning}},
url = {http://doi.wiley.com/10.1002/rob.21546},
volume = {32},
year = {2015}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
doi = {10.1177/0278364913491297},
file = {::},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
number = {11},
pages = {1231--1237},
title = {{Vision meets robotics: The KITTI dataset}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913491297},
volume = {32},
year = {2013}
}
@article{Carlone2015a,
abstract = {State-of-the-art techniques for simultaneous localization and mapping (SLAM) employ iterative nonlinear optimization methods to compute an estimate for robot poses. While these techniques often work well in practice, they do not provide guarantees on the quality of the estimate. This paper shows that Lagrangian duality is a powerful tool to assess the quality of a given candidate solution. Our contribution is threefold. First, we discuss a revised formulation of the SLAM inference problem. We show that this formulation is probabilistically grounded and has the advantage of leading to an optimization problem with quadratic objective. The second contribution is the derivation of the corresponding Lagrangian dual problem. The SLAM dual problem is a (convex) semidefinite program, which can be solved reliably and globally by off-the-shelf solvers. The third contribution is to discuss the relation between the original SLAM problem and its dual. We show that from the dual problem, one can evaluate the quality (i.e., the suboptimality gap) of a candidate SLAM solution, and ultimately provide a certificate of optimality. Moreover, when the duality gap is zero, one can compute a guaranteed optimal SLAM solution from the dual problem, circumventing non-convex optimization. We present extensive (real and simulated) experiments supporting our claims and discuss practical relevance and open problems.},
archivePrefix = {arXiv},
arxivId = {1506.00746},
author = {Carlone, Luca and Rosen, David M. and Calafiore, Giuseppe and Leonard, John J. and Dellaert, Frank},
doi = {10.1109/IROS.2015.7353364},
eprint = {1506.00746},
file = {::},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Convergence,Maximum likelihood estimation,Optimization,Simultaneous localization and mapping,Standards,Three-dimensional displays},
pages = {125--132},
title = {{Lagrangian duality in 3D SLAM: Verification techniques and optimal solutions}},
url = {http://arxiv.org/abs/1506.00746},
volume = {2015-Decem},
year = {2015}
}
@article{SZefrany1996,
abstract = {The set of rigid-body motions forms a Lie group called SE(3), the special Euclidean group in three dimensions. In this paper, we investigate possilble choices of Riemannian metrics and affine connections on SE(3) for applications to kinematic analysis and robot-trajectory planning. In the first part of the paper, we study metrics whose geodesics are screw motions. We prove that no Riemannian metrics call have such geodesics, and we show that the metrics whose geodesics are screw motions form a two-parameter family of semi-Riemannan metrics. In the second part of the paper, we investigate affine connections which through the covariant derivative give the correct expression for the acceleration of a rigid body. We prove that there is a unique symmetric connection with this property. Furthermore, we show that there is a family of Riemannian metrics that are compatible with such a connection. These metrics are products of the bi-invariant metric on the group of rotations and a positive-definite constant metric on the group of translations. {\textcopyright} 1999, SAGE Publications. All rights reserved.},
author = {{\v{Z}}efran, Milo{\v{s}} and Kumar, Vijay and Croke, Christopher},
doi = {10.1177/027836499901800208},
issn = {17413176},
journal = {The International Journal of Robotics Research},
number = {2},
pages = {1--16},
title = {{Metrics and Connections for Rigid-Body Kinematics}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.1525&rep=rep1&type=pdf},
volume = {18},
year = {1999}
}
@article{Su1992,
abstract = {One key issue in a robotic assembly system is handling uncertainties in the real world-uncertainties arising from object representation, robot motion, and sensory information. It is especially important when using robots in precision assembly tasks, in which a slight error on a manipulator's position/orientation or an object's position/orientation may make an assembly task/plan fail. To execute successfully in the presence of uncertainties, a robotic assembly system must be equipped with the ability to reason the effects of uncertainties in assembly tasks. A systematic methodology of manipulating and propagating spatial uncertainties in the form of homogeneous transforms and in a probabilistic sense is provided. Uncertainties are represented by covariance matrices and the manipulation of uncertainties is focused on the spatial uncertainty propagation and the uncertainty fusion. To integrate the uncertainty information into a task plan that consists of a sequence of primitive actions, the propagation of uncertainty before and after a primitive action such as moving action, perception action, and contact action has been developed. A simple and optimal solution for maintaining the consistency in the world state is proposed for perception actions. Using conditional probabilities, the correlation of motion parameters when a contact occurs is also taken into account. To determinate the applicability of an action, forward propagation and backward propagation are proposed to verify the success of an action in the presence of uncertainties. Furthermore, the backward propagation method can be used to determine an admissible set of an action with a specified acceptable success confidence and/or to efficiently apply perception actions to reduce the uncertainty. An example is provided to illustrate the proposed uncertainty propagation method. {\textcopyright} 1992 IEEE},
author = {Su, Shun Feng and Lee, C. S.George},
doi = {10.1109/21.199463},
isbn = {0-8186-2163-X},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {6},
pages = {1376--1389},
title = {{Manipulation and Propagation of Uncertainty and Verification of Applicability of Actions in Assembly Tasks}},
volume = {22},
year = {1992}
}
@article{Kim2015,
abstract = {Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.},
archivePrefix = {arXiv},
arxivId = {1601.06071},
author = {Kim, Minje and Smaragdis, Paris},
eprint = {1601.06071},
title = {{Bitwise Neural Networks}},
url = {http://arxiv.org/abs/1601.06071},
volume = {37},
year = {2016}
}
@article{Majdik2017,
abstract = {This paper presents a dataset recorded on-board a camera-equipped micro aerial vehicle flying within the urban streets of Zurich, Switzerland, at low altitudes (i.e. 5-15 m above the ground). The 2 km dataset consists of time synchronized aerial high-resolution images, global position system and inertial measurement unit sensor data, ground-level street view images, and ground truth data. The dataset is ideal to evaluate and benchmark appearance-based localization, monocular visual odometry, simultaneous localization and mapping, and online three-dimensional reconstruction algorithms for micro aerial vehicles in urban environments.},
author = {Majdik, Andr{\'{a}}s L. and Till, Charles and Scaramuzza, Davide},
doi = {10.1177/0278364917702237},
file = {:home/matias/Documents/Mendeley Desktop/Majdik, Till, Scaramuzza/2017/Majdik, Till, Scaramuzza - 2017 - The Zurich urban micro aerial vehicle dataset.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Visual localization,aerial robotics,air-ground matching},
month = {mar},
number = {3},
pages = {269--273},
title = {{The Zurich urban micro aerial vehicle dataset}},
url = {http://journals.sagepub.com/doi/10.1177/0278364917702237},
volume = {36},
year = {2017}
}
@article{MacTavish2016,
abstract = {Colour-constant images have been shown to improve visual navigation taking place over extended periods of time. These images use a colour space that aims to be invariant to lighting conditions—a quality that makes them very attractive for place recognition, which tries to identify temporally distant image matches. Place recognition after extended periods of time is especially useful for SLAM algorithms, since it bounds growing odometry errors. We present results from the FAB-MAP 2.0 place recognition algorithm, using colour-constant images for the first time, tested with a robot driving a 1kmloop 11 times over the course of several days. Computation can be improved by grouping short sequences of images and describing them with a single descriptor. Colour-constant images are shown to improve performance without a significant impact on computation, and the grouping strategy greatly speeds up computation while improving some performance measures. These two simple additions contribute robustness and speed, without modifying FAB-MAP 2.0.},
author = {MacTavish, Kirk and Paton, Michael and Barfoot, Timothy D.},
doi = {10.1007/978-3-319-27702-8_13},
file = {:home/matias/Documents/Mendeley Desktop/MacTavish, Paton, Barfoot/2016/MacTavish, Paton, Barfoot - 2016 - Beyond a shadow of a doubt Place recognition with colour-constant images.pdf:pdf},
isbn = {9783319277004},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
pages = {187--199},
title = {{Beyond a shadow of a doubt: Place recognition with colour-constant images}},
volume = {113},
year = {2016}
}
@article{Wang2006,
abstract = {Error propagation on the Euclidean motion group arises in a number of areas such as errors that accumulate from the base to the distal end of manipulators. We address error propagation in rigid-body poses in a coordinate-free way, and explain how this differs from other approaches proposed in the literature. In this paper, we show that errors propagate by convolution on the Euclidean motion group, SE(3). When local errors are small, they can be described well as distributions on the Lie algebra se(3). We show how the concept of a highly concentrated Gaussian distribution on SE(3) is equivalent to one on se(3). We also develop closure relations for these distributions under convolution on SE(3). Numerical examples illustrate how convolution is a valuable tool for computing the propagation of both small and large errors. {\textcopyright} 2006 IEEE.},
author = {Wang, Yunfeng and Chirikjian, Gregory S.},
doi = {10.1109/TRO.2006.878978},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Error propagation,Euclidean group,Manipulator kinematics,Spatial uncertainty},
number = {4},
pages = {591--602},
title = {{Error propagation on the Euclidean group with applications to manipulator kinematics}},
volume = {22},
year = {2006}
}
@article{Semini2011,
abstract = {A new versatile hydraulically powered quadruped robot (HyQ) has been developed to serve as a platform to study not only highly dynamic motions, such as running and jumping, but also careful navigation over very rough terrain. HyQ stands 1m tall, weighs roughly 90 kg, and features 12 torque-controlled joints powered by a combination of hydraulic and electric actuators. The hydraulic actuation permits the robot to perform powerful and dynamic motions that are hard to achieve with more traditional electrically actuated robots. This paper describes design and specifications of the robot and presents details on the hardware of the quadruped platform, such as the mechanical design of the four articulated legs and of the torso frame, and the configuration of the hydraulic power system. Results from the first walking experiments are presented, along with test studies using a previously built prototype leg. {\textcopyright} Authors 2011.},
author = {Semini, C. and Tsagarakis, N. G. and Guglielmino, E. and Focchi, M. and Cannella, F. and Caldwell, D. G.},
doi = {10.1177/0959651811402275},
isbn = {0959-6518},
issn = {20413041},
journal = {Proceedings of the Institution of Mechanical Engineers. Part I: Journal of Systems and Control Engineering},
keywords = {Hydraulic actuation,Hydraulic quadruped,Legged machine,Quadruped robot design},
number = {6},
pages = {831--849},
title = {{Design of HyQ -A hydraulically and electrically actuated quadruped robot}},
volume = {225},
year = {2011}
}
@article{Nitsche2020,
abstract = {Teach and Repeat (T&R) refers to the technology that allows a robot to autonomously follow a previously traversed route, in a natural scene and using only its onboard sensors. In this paper we present a Visual-Inertial Teach and Repeat (VI-T&R) algorithm that uses stereo and inertial data and targets Unmanned Aerial Vehicles with limited on-board computational resources. We propose a tightly-coupled relative formulation of the visual-inertial constraints that is tailored to the T&R application. In order to achieve real-time operation on limited hardware, we reduce the problem to motion-only visual-inertial Bundle Adjustment. In the repeat stage, we detail how to generate a trajectory and smoothly follow it with a constantly changing relative frame. The proposed method is validated in simulated environments, using real sensor data from the public EuRoC dataset, and using our own robotic setup and closed-loop control. Our experimental results demonstrate high accuracy and real-time performance both on a standard desktop system and on a low-cost Odroid X-U4 embedded computer.},
author = {Nitsche, Mat{\'{i}}as and Pessacg, Facundo and Civera, Javier},
doi = {10.1016/j.robot.2020.103577},
file = {:home/matias/Documents/Mendeley Desktop/Nitsche, Pessacg, Civera/2020/Nitsche, Pessacg, Civera - 2020 - Visual-inertial teach and repeat.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Embedded,Navigation,Relative,Stereo,Teach-and-replay,UAV,Visual-inertial},
pages = {103577},
publisher = {Elsevier B.V.},
title = {{Visual-inertial teach and repeat}},
url = {https://doi.org/10.1016/j.robot.2020.103577},
volume = {131},
year = {2020}
}
@article{Kappen2012,
abstract = {We reformulate a class of non-linear stochastic optimal control problems introduced by Todorov (in Advances in Neural Information Processing Systems, vol. 19, pp. 1369-1376, 2007) as a Kullback-Leibler (KL) minimization problem. As a result, the optimal control computation reduces to an inference computation and approximate inference methods can be applied to efficiently compute approximate optimal controls. We show how this KL control theory contains the path integral control method as a special case. We provide an example of a block stacking task and a multi-agent cooperative game where we demonstrate how approximate inference can be successfully applied to instances that are too complex for exact computation. We discuss the relation of the KL control approach to other inference approaches to control. {\textcopyright} The Author(s) 2012. This article is published with open access at Springerlink.com.},
archivePrefix = {arXiv},
arxivId = {0901.0633},
author = {Kappen, Hilbert J. and G{\'{o}}mez, Vicen{\c{c}} and Opper, Manfred},
doi = {10.1007/s10994-012-5278-7},
eprint = {0901.0633},
file = {:home/matias/Documents/Mendeley Desktop/Kappen, G{\'{o}}mez, Opper/2012/Kappen, G{\'{o}}mez, Opper - 2012 - Optimal control as a graphical model inference problem.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Approximate inference,Belief propagation,Cluster variation method,Graphical model,Kullback-leibler divergence,Optimal control,Uncontrolled dynamics},
number = {2},
pages = {159--182},
title = {{Optimal control as a graphical model inference problem}},
volume = {87},
year = {2012}
}
@article{Wang2010,
abstract = {In the paper, a novel moving object detection (MOD) algorithm is developed and integrated with robot visual Simultaneous Localization and Mapping (vSLAM). The moving object is assumed to be a rigid body and its coordinate system in space is represented by a position vector and a rotation matrix. The MOD algorithm is composed of detection of image features, initialization of image features, and calculation of object coordinates. Experimentation is implemented on a small-size humanoid robot and the results show that the performance of the proposed algorithm is efficient for robot visual SLAM and moving object detection.},
author = {Wang, Yin Tien and Lin, Ming Chun and Ju, Rung Chi},
doi = {10.5772/9700},
file = {::},
isbn = {1729-8806},
issn = {17298814},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Humanoid robot,Moving object detection,Simultaneous Localization and Mapping (SLAM)},
number = {2},
pages = {133--138},
title = {{Visual SLAM and moving-object detection for a small-size humanoid robot}},
volume = {7},
year = {2010}
}
@article{Rodriguez-Arevalo2018,
abstract = {The purpose of this work is to highlight the paramount importance of representing and quantifying uncertainty to correctly report the associated confidence of the robot's location estimate at each time step along its trajectory and therefore decide the correct course of action in an active SLAM mission. We analyze the monotonicity property of different decision-making criteria, both in 2-D and 3-D, with respect to the representation of uncertainty and of the orientation of the robot's pose. Monotonicity, the property that uncertainty increases as the robot moves, is essential for adequate decision making. We analytically show that, by using differential representations to propagate spatial uncertainties, monotonicity is preserved for all optimality criteria, A-opt, D-opt, and E-opt, and for Shannon's entropy. We also show that monotonicity does not hold for any criteria in absolute representations using Roll-Pitch-Yaw and Euler angles. Finally, using unit quaternions in absolute representations, the only criteria that preserve monotonicity are D-opt and Shannon's entropy.},
author = {Rodriguez-Arevalo, Maria L. and Neira, Jose and Castellanos, Jose A.},
doi = {10.1109/TRO.2018.2808902},
file = {:home/matias/Documents/Mendeley Desktop/Rodriguez-Arevalo, Neira, Castellanos/2018/Rodriguez-Arevalo, Neira, Castellanos - 2018 - On the Importance of Uncertainty Representation in Active SLAM.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Active simultaneous localization and mapping (Acti,differential error,entropy,exploration,optimality,uncertainty},
number = {3},
pages = {829--834},
title = {{On the Importance of Uncertainty Representation in Active SLAM}},
volume = {34},
year = {2018}
}
@inproceedings{Lanillos2018,
abstract = {The predictive functions that permit humans to infer their body state by sensorimotor integration are critical to perform safe interaction in complex environments. These functions are adaptive and robust to non-linear actuators and noisy sensory information. This paper introduces a computational perceptual model based on predictive processing that enables any multisensory robot to learn, infer and update its body configuration when using arbitrary sensors with Gaussian additive noise. The proposed method integrates different sources of information (tactile, visual and proprioceptive) to drive the robot belief to its current body configuration. The motivation is to provide robots with the embodied perception needed for self-calibration and safe physical human-robot interaction. We formulate body learning as obtaining the forward model that encodes the sensor values depending on the body variables, and we solve it by Gaussian process regression. We model body estimation as minimizing the discrepancy between the robot body configuration belief and the observed posterior. We minimize the variational free energy using the sensory prediction errors (sensed vs expected). In order to evaluate the model we test it on a real multi-sensory robotic arm. We show how different sensor modalities contributions, included as additive errors, improve the refinement of the body estimation and how the system adapts itself to provide the most plausible solution even when injecting strong sensory visuo-tactile perturbations. We further analyse the reliability of the model when different sensor modalities are disabled. This provides grounded evidence about the correctness of the perceptual model and shows how the robot estimates and adjusts its body configuration just by means of sensory information.},
archivePrefix = {arXiv},
arxivId = {1805.03104},
author = {Lanillos, Pablo and Cheng, Gordon},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2018.8593684},
eprint = {1805.03104},
file = {:home/matias/Documents/Mendeley Desktop/Lanillos, Cheng/2018/Lanillos, Cheng - 2018 - Adaptive Robot Body Learning and Estimation Through Predictive Coding.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
keywords = {Bio-inspired perception,body-schema,embodied artificial intelligence,humanoid robotics,learning and adaptive systems,predictive processing},
pages = {4083--4090},
title = {{Adaptive Robot Body Learning and Estimation Through Predictive Coding}},
year = {2018}
}
@article{Zhou2019,
abstract = {Controlled experiments indicate that explicit intermediate representations help action.},
archivePrefix = {arXiv},
arxivId = {1905.12887},
author = {Zhou, Brady and Kr{\"{a}}henb{\"{u}}hl, Philipp and Koltun, Vladlen},
doi = {10.1126/scirobotics.aaw6661},
eprint = {1905.12887},
file = {:home/matias/Documents/Mendeley Desktop/Zhou, Kr{\"{a}}henb{\"{u}}hl, Koltun/2019/Zhou, Kr{\"{a}}henb{\"{u}}hl, Koltun - 2019 - Does computer vision matter for action.pdf:pdf},
issn = {24709476},
journal = {Science Robotics},
month = {may},
number = {30},
pages = {eaaw6661},
title = {{Does computer vision matter for action?}},
url = {http://arxiv.org/abs/1807.06757 https://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aaw6661},
volume = {4},
year = {2019}
}
@article{Yun2017,
abstract = {In this contribution we introduce the Shape Flow algorithm (SF), a novel method for spatio-temporal 3D pose estimation of a 3D parametric curve. The SF is integrated into a tracking system and its suitability for tracking human body parts in 3D is examined. Based on the example of tracking the human hand-forearm limb it is shown that the use of two SF instances with different initialisations leads to an accurate and temporally stable tracking system. In our framework, the temporal pose derivative is available instantaneously, therefore we avoid delays typically encountered when filtering the pose estimation results over time. All necessary information is obtained from the images, only a coarse initialisation of the model parameters is required. Experimental investigations are performed on 5 real-world test sequences showing 3 different test persons in an average distance of 1.2–3.3 m to the camera in front of cluttered background. We achieve typical pose estimation accuracies of 40–100 mm for the mean distance to the ground truth and 4–6 mm for the pose differences between subsequent images.},
author = {Yun, Peng and Jiao, Jianhao and B, Ming Liu},
doi = {10.1007/978-3-319-68345-4},
file = {:home/matias/Documents/Mendeley Desktop/Wang/2020/Wang - 2020 - On Scale Initialization in Non-overlapping Multi-perspective Visual Odometry On Scale Initialization in Non-Overlapping Mu.pdf:pdf},
isbn = {9783319683454},
keywords = {cloud robotics},
number = {6140021318},
pages = {3--15},
title = {{Towards a Cloud Robotics Platform}},
url = {http://link.springer.com/10.1007/978-3-319-68345-4},
volume = {2},
year = {2017}
}
@article{Agarwal2020,
abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.},
archivePrefix = {arXiv},
arxivId = {2004.13912},
author = {Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E.},
eprint = {2004.13912},
file = {:home/matias/Documents/Mendeley Desktop/Agarwal et al/2020/Agarwal et al. - 2020 - Neural Additive Models Interpretable Machine Learning with Neural Nets.pdf:pdf},
title = {{Neural Additive Models: Interpretable Machine Learning with Neural Nets}},
url = {http://arxiv.org/abs/2004.13912},
year = {2020}
}
@article{Tang2019a,
abstract = {Simultaneous trajectory estimation and mapping (STEAM) offers an efficient approach to continuous-time trajectory estimation, by representing the trajectory as a Gaussian process (GP). Previous formulations of the STEAM framework use a GP prior that assumes white-noise-on-acceleration, with the prior mean encouraging constant body-centric velocity. We show that such a prior cannot sufficiently represent trajectory sections with nonzero acceleration, resulting in a bias to the posterior estimates. This letter derives a novel motion prior that assumes white-noise-on-jerk, where the prior mean encourages constant body-centric acceleration. With the new prior, we formulate a variation of STEAM that estimates the pose, body-centric velocity, and body-centric acceleration. By evaluating across several datasets, we show that the new prior greatly outperforms the white-noise-on-acceleration prior in terms of the solution accuracy.},
archivePrefix = {arXiv},
arxivId = {1809.06518},
author = {Tang, Tim Yuqing and Yoon, David Juny and Barfoot, Timothy D.},
doi = {10.1109/LRA.2019.2891492},
eprint = {1809.06518},
file = {:home/matias/Documents/Mendeley Desktop/Tang, Yoon, Barfoot/2019/Tang, Yoon, Barfoot - 2019 - A White-Noise-on-Jerk Motion Prior for Continuous-Time Trajectory Estimation on SE(3).pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,localization},
number = {2},
pages = {594--601},
publisher = {IEEE},
title = {{A White-Noise-on-Jerk Motion Prior for Continuous-Time Trajectory Estimation on SE(3)}},
volume = {4},
year = {2019}
}
@article{DeDonato2017,
abstract = {In the DARPA Robotics Challenge (DRC), participating human-robot teams were required to integrate mobility, manipulation, perception, and operator interfaces to complete a simulated disaster mission. We describe our approach using the humanoid robot Atlas Unplugged developed by Boston Dynamics. We focus on our approach, results, and lessons learned from the DRC Finals to demonstrate our strategy, including extensive operator practice, explicit monitoring for robot errors, adding additional sensing, and enabling the operator to control and monitor the robot at varying degrees of abstraction. Our safety-first strategy worked: we avoided falling, and remote operators could safely recover from difficult situations. We were the only team in the DRC Finals that attempted all tasks, scored points (14/16), did not require physical human intervention (a reset), and did not fall in the two missions during the two days of tests. We also had the most consistent pair of runs.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {DeDonato, Mathew and Polido, Felipe and Knoedler, Kevin and Babu, Benzun P.W. and Banerjee, Nandan and Bove, Christoper P. and Cui, Xiongyi and Du, Ruixiang and Franklin, Perry and Graff, Joshua P. and He, Peng and Jaeger, Aaron and Li, Lening and Berenson, Dmitry and Gennert, Michael A. and Feng, Siyuan and Liu, Chenggang and Xinjilefu, X. and Kim, Joohyung and Atkeson, Christopher G. and Long, Xianchao and Padır, Taşkın},
doi = {10.1002/rob.21685},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {381--399},
pmid = {22164016},
title = {{Team WPI-CMU: Achieving Reliable Humanoid Behavior in the DARPA Robotics Challenge}},
url = {http://doi.wiley.com/10.1002/rob.21685},
volume = {34},
year = {2017}
}
@article{Alismail2016,
abstract = {Binary descriptors have been instrumental in the recent evolution of computationally efficient sparse image alignment algorithms. Increasingly, however, the vision community is interested in dense image alignment methods, which are more suitable for estimating correspondences from high frame rate cameras as they do not rely on exhaustive search. However, classic dense alignment approaches are sensitive to illumination change. In this paper, we propose an easy to implement and low complexity dense binary descriptor, which we refer to as bit-planes, that can be seamlessly integrated within a multi-channel Lucas & Kanade framework. This novel approach combines the robustness of binary descriptors with the speed and accuracy of dense alignment methods. The approach is demonstrated on a template tracking problem achieving state-of-the-art robustness and faster than real-time performance on consumer laptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+ fps on an iPad Air 2).},
archivePrefix = {arXiv},
arxivId = {1602.00307},
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
eprint = {1602.00307},
file = {:home/matias/Documents/Mendeley Desktop/Alismail, Browning, Lucey/2016/Alismail, Browning, Lucey - 2016 - Bit-Planes Dense Subpixel Alignment of Binary Descriptors.pdf:pdf},
title = {{Bit-Planes: Dense Subpixel Alignment of Binary Descriptors}},
url = {http://arxiv.org/abs/1602.00307},
year = {2016}
}
@article{Soatto2011,
abstract = {This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of "complete information" that relates to the minimal sufficient statistics of a complete representation. It is shown that the "actionable information gap" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called "signal-to-symbol barrier" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video. This manuscript has been developed from a set of lecture notes for a summer course at the First International Computer Vision Summer School (ICVSS) in Scicli, Italy, in July of 2008. They were later expanded and amended for subsequent lectures in the same School in July 2009. Starting on November 1, 2009, they were further expanded for a special topics course, CS269, taught at UCLA in the Spring term of 2010.},
archivePrefix = {arXiv},
arxivId = {1110.2053},
author = {Soatto, Stefano},
eprint = {1110.2053},
keywords = {Computer Vision and Pattern Recognition},
pages = {151},
title = {{Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control}},
url = {http://arxiv.org/abs/1110.2053},
year = {2011}
}
@article{Zhang2018a,
abstract = {To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forward-looking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).},
author = {Zhang, Zichao and Scaramuzza, Davide},
doi = {10.1109/ICRA.2018.8461133},
file = {:home/matias/Documents/Mendeley Desktop/Zhang, Scaramuzza/2018/Zhang, Scaramuzza - 2018 - Perception-aware receding horizon navigation for MAVs.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2534--2541},
title = {{Perception-aware receding horizon navigation for MAVs}},
year = {2018}
}
@article{Mankowitz2014,
abstract = {We address the problem of devising vision-based feature extraction for the purpose of localisation on resource constrained robots that nonetheless require reasonably agile visual processing. We present modifications to a state-of-the-art Feature Extraction Algorithm (FEA) called Binary Robust Invariant Scalable Keypoints (BRISK) [8]. A key aspect of our contribution is the combined use of BRISK0 and U-BRISK as the FEA detector-descriptor pair for the purpose of localisation. We present a novel scoring function to find optimal parameters for this FEA. Also, we present two novel geometric matching constraints that serve to remove invalid interest point matches, which is key to keeping computations tractable. This work is evaluated using images captured on the Nao humanoid robot. In experiments, we show that the proposed procedure outperforms a previously implemented state-of-the-art vision-based FEA called 1D SURF (developed by the rUNSWift RoboCup SPL team), on the basis of accuracy and generalisation performance. Our experiments include data from indoor and outdoor environments, including a comparison to datasets such as based on Google Streetview. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Mankowitz, Daniel Jaymin and Ramamoorthy, Subramanian},
doi = {10.1007/978-3-662-44468-9_18},
isbn = {9783662444672},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {BRISK,BRISK0 - U-BRISK,Nao Humanoid Robot,feature extraction,localisation,resource constrained robot},
pages = {195--206},
title = {{BRISK-based visual feature extraction for resource constrained robots}},
volume = {8371 LNAI},
year = {2014}
}
@article{Laue2014,
author = {Thomas, R and Laue, Tim and Judith, M and Bartsch, Michel and Batram, Malte Jonas and Arne, B and Martin, B and Kroker, Martin and Maa{\ss}, Florian and Thomas, M and Steinbeck, Marcel and Stolpmann, Andreas and Taddiken, Simon},
file = {::;::},
pages = {1--194},
title = {{Team Report and Code Release 2013}},
year = {2014}
}
@article{Michael2010,
abstract = {In the last five years, advances in materials, electronics, sensors, and batteries have fueled a growth in the development of microunmanned aerial vehicles (MAVs) that are between 0.1 and 0.5 m in length and 0.1-0.5 kg in mass [1]. A few groups have built and analyzed MAVs in the 10-cm range [2], [3]. One of the smallest MAV is the Picoftyer with a 60-mmpropellor diameter and a mass of 3.3 g [4]. Platforms in the 50-cm range are more prevalent with several groups having built and flown systems of this size [5]-[7]. In fact, there are severalcommercially available radiocontrolled (PvC) helicopters and research-grade helicopters in this size range [8].},
author = {Michael, Nathan and Mellinger, Daniel and Lindsey, Quentin and Kumar, Vijay},
doi = {10.1109/MRA.2010.937855},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {56--65},
title = {{The GRASP multiple micro-UAV testbed}},
volume = {17},
year = {2010}
}
@article{Zhang2017a,
abstract = {We present an approach for agents to learn representations of a global map from sensor data, to aid their exploration in new environments. To achieve this, we embed procedures mimicking that of traditional Simultaneous Localization and Mapping (SLAM) into the soft attention based addressing of external memory architectures, in which the external memory acts as an internal representation of the environment. This structure encourages the evolution of SLAM-like behaviors inside a completely differentiable deep neural network. We show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential. We validate our approach in both challenging grid-world environments and preliminary Gazebo experiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.},
archivePrefix = {arXiv},
arxivId = {1706.09520},
author = {Zhang, Jingwei and Tai, Lei and Boedecker, Joschka and Burgard, Wolfram and Liu, Ming},
eprint = {1706.09520},
file = {::},
title = {{Neural SLAM: Learning to Explore with External Memory}},
url = {http://arxiv.org/abs/1706.09520},
year = {2017}
}
@inproceedings{Roncone2015,
abstract = {Gaze stabilization is an important requisite for humanoid robots. Previous work on this topic has focused on the integration of inertial and visual information. Little attention has been given to a third component, which is the knowledge that the robot has about its own movement. In this work we propose a comprehensive framework for gaze stabilization in a humanoid robot. We focus on the problem of compensating for disturbances induced in the cameras due to self-generated movements of the robot. In this work we employ two separate signals for stabilization: (1) an anticipatory term obtained from the velocity commands sent to the joints while the robot moves autonomously; (2) a feedback term from the on board gyroscope, which compensates unpredicted external disturbances. We first provide the mathematical formulation to derive the forward and the differential kinematics of the fixation point of the stereo system. We finally test our method on the iCub robot. We show that the stabilization consistently reduces the residual optical flow during the movement of the robot and in presence of external disturbances. We also demonstrate that proper integration of the neck DoF is crucial to achieve correct stabilization.},
archivePrefix = {arXiv},
arxivId = {1411.3525},
author = {Roncone, Alessandro and Pattacini, Ugo and Metta, Giorgio and Natale, Lorenzo},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2014.7041369},
eprint = {1411.3525},
file = {::},
isbn = {9781479971749},
issn = {21640580},
month = {nov},
pages = {259--264},
publisher = {IEEE},
title = {{Gaze stabilization for humanoid robots: A comprehensive framework}},
url = {http://ieeexplore.ieee.org/document/7041369/},
volume = {2015-Febru},
year = {2015}
}
@article{Wong2020,
abstract = {We present parameter learning in a Gaussian variational inference setting using only noisy measurements (i.e., no groundtruth). This is demonstrated in the context of vehicle trajectory estimation, although the method we propose is general. The letter extends the Exactly Sparse Gaussian Variational Inference (ESGVI) framework, which has previously been used for large-scale nonlinear batch state estimation. Our contribution is to additionally learn parameters of our system models (which may be difficult to choose in practice) within the ESGVI framework. In this letter, we learn the covariances for the motion and sensor models used within vehicle trajectory estimation. Specifically, we learn the parameters of a white-noise-on-acceleration motion model and the parameters of an Inverse-Wishart prior over measurement covariances for our sensor model. We demonstrate our technique using a 36 km dataset consisting of a car using lidar to localize against a high-definition map; we learn the parameters on a training section of the data and then show that we achieve high-quality state estimates on a test section, even in the presence of outliers. Lastly, we show that our framework can be used to solve pose graph optimization even with many false loop closures.},
archivePrefix = {arXiv},
arxivId = {2003.09736},
author = {Wong, Jeremy Nathan and Yoon, David Juny and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/LRA.2020.3007381},
eprint = {2003.09736},
file = {:home/matias/Documents/Mendeley Desktop/Wong et al/2020/Wong et al. - 2020 - Variational Inference with Parameter Learning Applied to Vehicle Trajectory Estimation.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,localization},
number = {4},
pages = {5291--5298},
title = {{Variational Inference with Parameter Learning Applied to Vehicle Trajectory Estimation}},
url = {http://arxiv.org/abs/2003.09736},
volume = {5},
year = {2020}
}
@article{Wang2017b,
abstract = {Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.},
archivePrefix = {arXiv},
arxivId = {1707.02747},
author = {Wang, Ziyu and Merel, Josh and Reed, Scott and Wayne, Greg and {De Freitas}, Nando and Heess, Nicolas},
eprint = {1707.02747},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {5321--5330},
title = {{Robust imitation of diverse behaviors}},
url = {http://arxiv.org/abs/1707.02747},
volume = {2017-Decem},
year = {2017}
}
@article{Tanner,
abstract = {Dense reconstructions often contain errors that prior work has so far minimised using high quality sensors and regularising the output. Nevertheless, errors still persist. This paper proposes a machine learning technique to identify errors in three dimensional (3D) meshes. Beyond simply identifying errors, our method quantifies both the magnitude and the direction of depth estimate errors when viewing the scene. This enables us to Improve the reconstruction accuracy. We train a suitably deep network architecture with two 3D meshes: a high-quality laser reconstruction, and a lower quality stereo image reconstruction. The network predicts the amount of error in the lower quality reconstruction with respect to the high-quality one, having only view the former through its input. We evaluate our approach by correcting two dimensional (2D) inverse-depth images extracted from the 3D model, and show that our method improves the quality of these depth reconstructions by up to a relative 10% RMSE.},
archivePrefix = {arXiv},
arxivId = {1801.09128},
author = {Tanner, Michael and Sftescu, Stefan and Bewley, Alex and Newman, Paul},
doi = {10.1109/ICRA.2018.8460977},
eprint = {1801.09128},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3201--3206},
title = {{Meshed Up: Learnt error correction in 3D reconstructions}},
url = {https://arxiv.org/pdf/1801.09128.pdf},
year = {2018}
}
@inproceedings{Lynen2015,
abstract = {Accurately estimating a robot's pose relative to a global scene model and precisely tracking the pose in real-time is a fundamental problem for navigation and obstacle avoidance tasks. Due to the computational complexity of localization against a large map and the memory consumed by the model, state-of- the-art approaches are either limited to small workspaces or rely on a server-side system to query the global model while tracking the pose locally. The latter approaches face the problem of smoothly integrating the server's pose estimates into the trajectory computed locally to avoid temporal discontinuities. In this paper, we demonstrate that large-scale, real-time pose estimation and tracking can be performed on mobile platforms with limited resources without the use of an external server. This is achieved by employing map and descriptor compression schemes as well as efficient search algorithms from computer vision. We derive a formulation for integrating the global pose information into a local state estimator that produces much smoother trajectories than current approaches. Through detailed experiments, we evaluate each of our design choices individually and document its impact on the overall system performance, demonstrating that our approach outperforms state-of-the-art algorithms},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.05949v1},
author = {Localization, Real-time Visual-inertial and Lynen, Simon and Sattler, Torsten and Bosse, Michael and Hesch, Joel and Pollefeys, Marc and Siegwart, Roland},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.037},
eprint = {arXiv:1610.05949v1},
isbn = {9780992374716},
issn = {2330765X},
keywords = {inertial,localisation,vision},
number = {November},
pages = {18},
title = {{Get Out of My Lab :}},
year = {2015}
}
@inproceedings{Mattamala2017a,
author = {Mattamala, Mat{\'{i}}as and Olave, Gonzalo and Campusano, Miguel and G{\'{o}}mez, Cristopher and Mart{\'{i}}nez, Luz and Estef{\'{o}}, Pablo and Ugalde, Joakin and Urrutia, Javier and {San Mart{\'{i}}n}, Felipe and Z{\'{u}}{\~{n}}iga, Pablo and Carrasco, Javier and Villar, Camila and Gonz{\'{a}}lez, Roc{\'{i}}o},
booktitle = {Sochedi 2017},
file = {::},
keywords = {aprendizaje activo,interdisciplinariedad,juego de roles,rob{\'{o}}tica,s},
title = {{Aprendizaje interdisciplinario en rob{\'{o}}tica: la experiencia innovadora de duckietown chile}},
year = {2017}
}
@techreport{Huang2009,
abstract = {We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, targeted at real-time robotics applications. LCM is comprised of several components: a data type specification language, a message passing system, logging/playback tools, and real-time analysis tools. LCM provides a platform- and language-independent type specification language. These specifications can be compiled into platform and language specific implementations, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. Messages can be transmitted between different processes using LCMs message-passing system, which implements a publish/subscribe model. LCMs implementation is notable in providing low-latency messaging and eliminating the need for a central communications hub. This architecture makes it easy to mix simulated, recorded, and live data sources. A number of logging, playback, and traffic inspection tools simplify common development and debugging tasks. LCM is targeted at robotics and other real-time systems where low latency is critical; its messaging model permits dropping messages in order to minimize the latency of new messages. In this paper, we explain LCMs design, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.},
author = {Huang, Albert and Olson, Edwin and Moore, David},
booktitle = {Mit-Csail-Tr-2009-041},
file = {::},
institution = {Massachusetts Institute of Technology},
pages = {1--17},
title = {{Lightweight Communications and Marshalling for Low-Latency Interprocess Communication}},
year = {2009}
}
@article{Oskiper2007b,
abstract = {Over the past decade, tremendous amount of research activity has focused around the problem of localization in GPS denied environments. Challenges with localization are highlighted in human wearable systems where the operator can freely move through both indoors and outdoors. In this paper, we present a robust method that addresses these challenges using a human wearable system with two pairs of backward and forward looking stereo cameras together with an inertial measurement unit (IMU). This algorithm can run in real-time with 15Hz update rate on a dual-core 2GHz laptop PC and it is designed to be a highly accurate local (relative) pose estimation mechanism acting as the front-end to a Simultaneous Localization and Mapping (SLAM) type method capable of global corrections through landmark matching. Extensive tests of our prototype system so far, reveal that without any global landmark matching, we achieve between 0.5% and 1% accuracy in localizing a person over a 500 meter travel indoors and outdoors. To our knowledge, such performance results with a real time system have not been reported before. {\textcopyright} 2007 IEEE.},
author = {Oskiper, Taragay and Zhu, Zhiwei and Samarasekera, Supun and Kumar, Rakesh},
doi = {10.1109/CVPR.2007.383087},
file = {:home/matias/Documents/Mendeley Desktop/Oskiper, Zhu/2007/Oskiper, Zhu - 2007 - Visual Odometry System Using Multiple Stereo Cameras and Inertial Measurement Unit Visual Odometry System Using(3).pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {May 2014},
title = {{Visual odometry system using multiple stereo cameras and inertial measurement unit}},
year = {2007}
}
@inproceedings{K.2016,
abstract = {We describe the characterization and removal of noise present in the Inertial Measurement Unit (IMU) MPU-6050. This IMU was initially used in an attitude sensor (AS) developed in-house, and subsequently implemented in a pointing and stabilization platform developed for small balloon-borne astronomical payloads. We found that the performance of the IMU degrades with time due to the accumulation of different errors. Using the Allan variance analysis method, we identified the different components of noise present in the IMU and verified the results using a power spectral density analysis (PSD). We tried to remove the high-frequency noise using smoothing filters, such as moving average filter and Savitzky-Golay filter. Although we did manage to filter some of the high-frequency noise, the performance of these filters was not satisfactory for our application. We found the distribution of the random noise present in the IMU using a probability density analysis, and identified the noise to be white Gaussian in nature which we successfully removed by a Kalman filter in real time.},
archivePrefix = {arXiv},
arxivId = {1608.07053},
author = {K., Nirmal and {A. G.}, Sreejith and Mathew, Joice and Sarpotdar, Mayuresh and Suresh, Ambily and Prakash, Ajin and Safonova, Margarita and Murthy, Jayant},
booktitle = {Advances in Optical and Mechanical Technologies for Telescopes and Instrumentation II},
doi = {10.1117/12.2234255},
editor = {Navarro, Ram{\'{o}}n and Burge, James H.},
eprint = {1608.07053},
file = {::},
isbn = {9781510602038},
issn = {0277-786X},
keywords = {attitude sensor,balloon experiment,mems sensors,pointing system},
pages = {99126W},
title = {{Noise modeling and analysis of an IMU-based attitude sensor: improvement of performance by filtering and sensor fusion}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2234255},
volume = {9912},
year = {2016}
}
@phdthesis{Kummerle2013,
abstract = {building block for a robot assisting humans in potentially dangerous situations, such as search- and-rescue scenarios. Hence, navigation is one of the major research topics in the robotics community. To realize the above mentioned applications, we need to fulfill certain requirements, so that a robot is regarded as useful. For example, a robot which performs pick-and-place tasks or offers guidance in city centers needs to be aware of its own position in the environment and it needs to have an accurate model of the environment for planning an appropriate path. A robot which should guide a human to a certain place or has to deliver goods is only regarded as helping hand, if the location is reliably reached within the expected time frame. Particularly, estimating the state which describes the current situation of the navigation sys- tem is complex. In this thesis, we focus on efficient and accurate state estimation techniques which apply probabilistic algorithms. An example for such a state estimation task is the Si- multaneous Localization and Mapping (SLAM) problem, in which a robot has to address both aspects. First, it needs to estimate what the environment looks like. This is the mapping part which deals with integrating the information obtained by the sensors of the robot into an ap- propriate representation. Second, the localization component has to estimate the position of the robot with respect to the model of the environment. In the first part of this thesis, we present efficient approaches to estimate the state of the robot while performing SLAM. Our approach allows a robot to accurately estimate the model of the environment in an online setting and also in situations when provided with a poor initial guess. Additionally, we provide an empirical evaluation which demonstrates the advantages of our approach compared to other state-of-the-art methods. Subsequently, we extend our state estimation approach to also include the unknown calibration parameters, which might change during the lifetime of the robot, to incorporate prior information about the structure of the environment, and to improve the fine-grained details of the estimated models. In the second part of this thesis, we demonstrate two challenging applications which we realized by building upon and extending the algorithms presented in the first part. In detail, we discuss an approach which allows a car to autonomously park in a complex multi-level parking garage. As second applicationwe present a robotic pedestrian assistantwhich is able to navigate in densely populated pedestrian zones. All techniques presented in this thesis have been implemented and tested using both real- world data collected with mobile robots and simulated data. To support our claims, we per- formed an extensive collection of experiments, in which we compared the performance of our approaches with the state-of-the-art. We believe that the proposed approaches will allow us in the future to build systems that can assist humans in their homes and at their workplaces.},
author = {Kuemmerle, Rainer},
pages = {1 -- 191},
school = {Albert-Ludwigs-Universit¨ at Freiburg},
title = {{State Estimation and Optimization for Mobile Robot Navigation}},
url = {http://www.freidok.uni-freiburg.de/volltexte/8967/},
year = {2013}
}
@article{Paton2015,
abstract = {Autonomous path-following robots that use vision-based navigation are appealing for a wide variety of tedious and dangerous applications. However, a reliance on matching point-based visual features often renders vision-based navigation unreliable over extended periods of time in unstructured, outdoor environments. Specifically, scene change caused by lighting, weather, and seasonal variation lead to changes in visual features and result in a reduction of feature associations across time. This paper presents an autonomous, path-following system that uses multiple stereo cameras to increase the algorithm field of view and reliably navigate in these feature-limited scenarios. The addition of a second camera in the localization pipeline greatly increases the probability that a stable feature will be in the robot's field of view at any point in time, extending the amount of time the robot can reliably navigate. We experimentally validate our algorithm through a challenging winter field trial, where the robot autonomously traverses a 250m path six times with an autonomy rate of 100% despite significant changes in the appearance of the scene due to lighting and melting snow. We show that the addition of a second stereo camera to the system significantly increases the autonomy window when compared to current state-of-the-art path-following methods.},
author = {Paton, Michael and Pomerleau, Francois and Barfoot, Timothy D.},
doi = {10.1109/CRV.2015.16},
file = {:home/matias/Documents/Mendeley Desktop/Paton, Barfoot/2015/Paton, Barfoot - 2015 - Eyes in the Back of Your Head Robust Visual Teach & Repeat Using Multiple Stereo Cameras Eyes in the Back of Yo.pdf:pdf},
isbn = {9781479919864},
journal = {Proceedings -2015 12th Conference on Computer and Robot Vision, CRV 2015},
keywords = {Autonomous Path Following,Computer Vision,Field Robotics,Localization,Long-Term Autonomy},
number = {June},
pages = {46--53},
title = {{Eyes in the Back of Your Head: Robust Visual Teach & Repeat Using Multiple Stereo Cameras}},
year = {2015}
}
@article{Bush2015,
abstract = {Mammals are able to navigate to hidden goal locations by direct routes that may traverse previously unvisited terrain. Empirical evidence suggests that this "vector navigation" relies on an internal representation of space provided by the hippocampal formation. The periodic spatial firing patterns of grid cells in the hippocampal formation offer a compact combinatorial code for location within large-scale space. Here, we consider the computational problem of how to determine the vector between start and goal locations encoded by the firing of grid cells when this vector may be much longer than the largest grid scale. First, we present an algorithmic solution to the problem, inspired by the Fourier shift theorem. Second, we describe several potential neural network implementations of this solution that combine efficiency of search and biological plausibility. Finally, we discuss the empirical predictions of these implementations and their relationship to the anatomy and electrophysiology of the hippocampal formation.},
author = {Bush, Daniel and Barry, Caswell and Manson, Daniel and Burgess, Neil},
doi = {10.1016/j.neuron.2015.07.006},
file = {:home/matias/Documents/Mendeley Desktop/Bush et al/2015/Bush et al. - 2015 - Using Grid Cells for Navigation.pdf:pdf},
issn = {10974199},
journal = {Neuron},
number = {3},
pages = {507--520},
pmid = {26247860},
publisher = {The Authors},
title = {{Using Grid Cells for Navigation}},
url = {http://dx.doi.org/10.1016/j.neuron.2015.07.006},
volume = {87},
year = {2015}
}
@article{Alcantarilla2013,
abstract = {In this paper, we propose a real-time vision-based localization approach for humanoid robots using a single camera as the only sensor. In order to obtain an accurate localization of the robot, we first build an accurate 3D map of the environment. In the map computation process, we use stereo visual SLAM techniques based on non-linear least squares optimization methods (bundle adjustment). Once we have computed a 3D reconstruction of the environment, which comprises of a set of camera poses (keyframes) and a list of 3D points, we learn the visibility of the 3D points by exploiting all the geometric relationships between the camera poses and 3D map points involved in the reconstruction. Finally, we use the prior 3D map and the learned visibility prediction for monocular vision-based localization. Our algorithm is very efficient, easy to implement and more robust and accurate than existing approaches. By means of visibility prediction we predict for a query pose only the highly visible 3D points, thus, speeding up tremendously the data association between 3D map points and perceived 2D features in the image. In this way, we can solve very efficiently the Perspective-n-Point (PnP) problem providing robust and fast vision-based localization. We demonstrate the robustness and accuracy of our approach by showing several vision-based localization experiments with the HRP-2 humanoid robot. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Alcantarilla, Pablo F. and Stasse, Olivier and Druon, Sebastien and Bergasa, Luis M. and Dellaert, Frank},
doi = {10.1007/s10514-012-9312-1},
file = {:home/matias/Documents/Mendeley Desktop/Alcantarilla et al/2013/Alcantarilla et al. - 2013 - How to localize humanoids with a single camera.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Bundle adjustment,Humanoid robots,Locally weighted learning,Visibility prediction,Vision-based localization},
number = {1-2},
pages = {47--71},
title = {{How to localize humanoids with a single camera?}},
volume = {34},
year = {2013}
}
@inproceedings{Mattamala2017,
abstract = {We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot's autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human's framework and ROS to have access to the robot's sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community.},
archivePrefix = {arXiv},
arxivId = {1706.06696},
author = {Mattamala, Mat{\'{i}}as and Olave, Gonzalo and Gonz{\'{a}}lez, Clayder and Hasb{\'{u}}n, Nicol{\'{a}}s and Ruiz-Del-Solar, Javier},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-00308-1_25},
eprint = {1706.06696},
file = {::},
isbn = {9783030003074},
issn = {16113349},
keywords = {NAO robot,ODROID XU-4,ROS,SPL},
month = {jun},
pages = {302--311},
title = {{The NAO Backpack: An open-hardware add-on for fast software development with the NAO robot}},
url = {http://arxiv.org/abs/1706.06696},
volume = {11175 LNAI},
year = {2018}
}
@phdthesis{Kastner2014,
author = {Kastner, Tobias},
number = {April},
school = {Universit{\"{a}}t Bremen},
title = {{Automatische Roboterkalibrierung f{\"{u}}r den humanoiden Roboter NAO}},
year = {2014}
}
@article{Schwendner2010,
abstract = {Autonomous mobile robots have the potential to change our everyday life. Unresolved challenges which span a large spectrum of artificial intelligence research need to be answered to progress further towards this vision. This article addresses the problem of robot localisation and mapping, which plays a vital role for robot autonomy in unknown environments. An analysis of the potential for using embodied data is performed, and the notion of direct and indirect embodied data is introduced. Further, the implications of embodied data for an embodied SLAM algorithm are investigated and set into a robotic context.},
author = {Schwendner, Jakob and Kirchner, Frank},
doi = {10.1007/s13218-010-0033-3},
file = {::},
issn = {16101987},
journal = {KI - Kunstliche Intelligenz},
keywords = {Data Association,Inertial Sensor,Locomotion Parameter,Robot Locomotion,Rotation Sensor},
number = {3},
pages = {241--244},
title = {{eSLAM—Self Localisation and Mapping Using Embodied Data}},
url = {http://www.springerlink.com/index/10.1007/s13218-010-0033-3},
volume = {24},
year = {2010}
}
@article{Kundu2011,
abstract = {This paper presents a realtime, incremental multibody visual SLAM system that allows choosing between full 3D reconstruction or simply tracking of the moving objects. Motion reconstruction of dynamic points or objects from a monocular camera is considered very hard due to well known problems of observability. We attempt to solve the problem with a Bearing only Tracking (BOT) and by integrating multiple cues to avoid observability issues. The BOT is accomplished through a particle filter, and by integrating multiple cues from the reconstruction pipeline. With the help of these cues, many real world scenarios which are considered unobservable with a monocular camera is solved to reasonable accuracy. This enables building of a unified dynamic 3D map of scenes involving multiple moving objects. Tracking and reconstruction is preceded by motion segmentation and detection which makes use of efficient geometric constraints to avoid difficult degenerate motions, where objects move in the epipolar plane. Results reported on multiple challenging real world image sequences verify the efficacy of the proposed framework. {\textcopyright} 2011 IEEE.},
author = {Kundu, Abhijit and Krishna, K. Madhava and Jawahar, C. V.},
doi = {10.1109/ICCV.2011.6126482},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2080--2087},
title = {{Realtime multibody visual SLAM with a smoothly moving monocular camera}},
year = {2011}
}
@article{Foote2013,
abstract = {The tf library was designed to provide a standard way to keep track of coordinate frames and transform data within an entire system such that individual component users can be confident that the data is in the coordinate frame that they want without requiring knowledge of all the coordinate frames in the system. During early development of the Robot Operating System (ROS), keeping track of coordinate frames was identified as a common pain point for developers. The complexity of this task made it a common place for bugs when developers improperly applied transforms to data. The problem is also a challenge due to the often distributed sources of information about transformations between different sets of coordinate frames. This paper will explain the complexity of the problem and distill the requirements. Then it will discuss the design of the tf library in relation to the requirements. A few use cases will be presented to demonstrate successful deployment of the library. And powerful extensions to the core capabilities such as being able to transform data in time as well as in space. {\textcopyright} 2013 IEEE.},
author = {Foote, Tully},
doi = {10.1109/TePRA.2013.6556373},
file = {::},
isbn = {9781467362252},
issn = {23250526},
journal = {IEEE Conference on Technologies for Practical Robot Applications, TePRA},
title = {{Tf: The transform library}},
year = {2013}
}
@article{Davison2007,
abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera. {\textcopyright} 2007 IEEE.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
doi = {10.1109/TPAMI.2007.1049},
eprint = {there is not},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D/stereo scene analysis,Autonomous vehicles,Tracking},
number = {6},
pages = {1052--1067},
pmid = {17431302},
title = {{MonoSLAM: Real-time single camera SLAM}},
volume = {29},
year = {2007}
}
@article{Gerndt2015,
abstract = {This article describes the history and major achievements of the RoboCup Humanoid League from its start in 2002 to today. Furthermore, it gives an indication on how the league may evolve over the coming years until 2050, when a team of autonomous humanoid robots shall play soccer against the human world champion. We show how the competition drives humanoid robot research and serves as a benchmark to measure progress.},
author = {Gerndt, Reinhard and Seifert, Daniel and Baltes, Jacky Hansjoerg and Sadeghnejad, Soroush and Behnke, Sven},
doi = {10.1109/MRA.2015.2448811},
file = {::},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
month = {sep},
number = {3},
pages = {147--154},
title = {{Humanoid Robots in Soccer: Robots Versus Humans in RoboCup 2050}},
url = {http://ieeexplore.ieee.org/document/7254325/},
volume = {22},
year = {2015}
}
@article{Deisenroth2015,
abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
archivePrefix = {arXiv},
arxivId = {1502.02860},
author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
doi = {10.1109/TPAMI.2013.218},
eprint = {1502.02860},
file = {::},
isbn = {0162-8828 VO  - 37},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bayesian inference,Control,Gaussian processes,Policy search,Reinforcement learning,Robotics},
number = {2},
pages = {408--423},
pmid = {26353251},
title = {{Gaussian processes for data-efficient learning in robotics and control}},
volume = {37},
year = {2015}
}
@article{Zhang2019,
abstract = {Despite the existence of different error metrics for trajectory evaluation in SLAM, their theoretical justifications and connections are rarely studied, and few methods handle temporal association properly. In this work, we propose to formulate the trajectory evaluation problem in a probabilistic, continuous-time framework. By modeling the groundtruth as random variables, the concepts of absolute and relative error are generalized to be likelihood. Moreover, the groundtruth is represented as a piecewise Gaussian Process in continuous-time. Within this framework, we are able to establish theoretical connections between relative and absolute error metrics and handle temporal association in a principled manner.},
archivePrefix = {arXiv},
arxivId = {1906.03996},
author = {Zhang, Zichao and Scaramuzza, Davide},
eprint = {1906.03996},
file = {:home/matias/Documents/Mendeley Desktop/Zhang, Scaramuzza/2019/Zhang, Scaramuzza - 2019 - Rethinking Trajectory Evaluation for SLAM a Probabilistic, Continuous-Time Approach.pdf:pdf},
number = {3},
title = {{Rethinking Trajectory Evaluation for SLAM: a Probabilistic, Continuous-Time Approach}},
url = {http://arxiv.org/abs/1906.03996},
year = {2019}
}
@article{Marchant2012,
abstract = {Environmental Monitoring (EM) is typically performed using sensor networks that collect measurements in predefined static locations. The possibility of having one or more autonomous robots to perform this task increases versatility and reduces the number of necessary sensor nodes to cover the same area. However, several problems arise when making use of autonomous moving robots for EM. The main challenges are how to build an accurate spatial-temporal model while choosing locations for measuring the phenomenon. This paper addresses the problem by using Bayesian Optimisation for choosing sensing locations, and presents a new utility function that takes into account the distance travelled by a moving robot. The proposed methodology is tested in simulation and in a real environment. Compared to existing strategies, our approach exhibits slightly better accuracy in terms of RMSE error and considerably reduces the total distance travelled by the robot. {\textcopyright} 2012 IEEE.},
author = {Marchant, Roman and Ramos, Fabio},
doi = {10.1109/IROS.2012.6385653},
file = {::},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2242--2249},
title = {{Bayesian optimisation for Intelligent Environmental Monitoring}},
year = {2012}
}
@inproceedings{Levi2015,
abstract = {We present a novel means of describing local image appearances using binary strings. Binary descriptors have drawn increasing interest in recent years due to their speed and low memory footprint. A known shortcoming of these representations is their inferior performance compared to larger, histogram based descriptors such as the SIFT. Our goal is to close this performance gap while maintaining the benefits attributed to binary representations. To this end we propose the Learned Arrangements of Three Patch Codes descriptors, or LATCH. Our key observation is that existing binary descriptors are at an increased risk from noise and local appearance variations. This, as they compare the values of pixel pairs: changes to either of the pixels can easily lead to changes in descriptor values and compromise their performance. In order to provide more robustness, we instead propose a novel means of comparing pixel patches. This ostensibly small change, requires a substantial redesign of the descriptors themselves and how they are produced. Our resulting LATCH representation is rigorously compared to state-of-the-art binary descriptors and shown to provide far better performance for similar computation and space requirements.},
archivePrefix = {arXiv},
arxivId = {1501.03719},
author = {Levi, Gil and Hassner, Tal},
booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
doi = {10.1109/WACV.2016.7477723},
eprint = {1501.03719},
isbn = {9781509006410},
month = {mar},
pages = {1--9},
publisher = {IEEE},
title = {{LATCH: Learned arrangements of three patch codes}},
url = {http://www.openu.ac.il/home/hassner/projects/LATCH http://ieeexplore.ieee.org/document/7477723/},
volume = {abs/1501.0},
year = {2016}
}
@inproceedings{Harrison2011,
abstract = {Modern robotic systems are composed of many distributed processes sharing a common communications infrastructure. High bandwidth sensor data is often collected on one computer and served to many consumers. It is vital that every device on the network agrees on how time is measured. If not, sensor data may be at best inconsistent and at worst useless. Typical clocks in consumer grade PCs are highly inaccurate and temperature sensitive. We argue that traditional approaches to clock synchronization, such as the use of NTP are inappropriate in the robotics context. We present an extremely efficient algorithm for learning the mapping between distributed clocks, which typically achieves better than millisecond accuracy within just a few seconds. We also give a probabilistic analysis providing an upper-bound error estimate. {\textcopyright} 2011 IEEE.},
author = {Harrison, Alastair and Newman, Paul},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980112},
file = {::},
isbn = {9781612843865},
issn = {10504729},
month = {may},
pages = {356--363},
publisher = {IEEE},
title = {{TICSync: Knowing when things happened}},
url = {http://ieeexplore.ieee.org/document/5980112/},
year = {2011}
}
@book{Corke2011,
abstract = {This chapter depicts an architecture that aims at designing a multi-UAV framework enabling cooperative operations in a system in which some UAVs are directly controlled by an operator, others are only endowed with operational autonomy, and others have decisional autonomy capacities. The architecture provides with the possibility to configure dynamically the decisional scheme, depending on the available robots and on the operational context. A taxonomy of robots decisional autonomy is introduced, and used as a foundation to state the proposed architecture. The various functionalities on-board each robot are organized among a repartition that exhibits on-board functional components, and on-board or on-ground generic executive and decision making processes. A set of algorithms that fulfill the three main decision-making functionalities required in a multi-robot system are then presented: a contract-net protocol that can handle task allocation for complex multi-UAV missions, a planning scheme based on a Hierarchical Task Networks planner completed with plan-refiners that consider the actual domain models, and an executive system that handles the coordination and task execution issues. {\textcopyright} 2007 Springer-Verlag Berlin Heidelberg.},
archivePrefix = {arXiv},
arxivId = {978 3 642 20143 1},
author = {Lacroix, Simon and Alami, Rachid and Lemaire, Thomas and Hattenberger, Gautier and Gancet, J{\'{e}}r{\'{e}}mi},
booktitle = {Springer Tracts in Advanced Robotics},
doi = {10.1007/978-3-540-73958-6_2},
eprint = {978 3 642 20143 1},
isbn = {3540739572},
issn = {16107438},
pages = {15--48},
pmid = {15003161},
publisher = {Springer-Verlag},
title = {{Decision making in multi-UAVs systems: Architecture and algorithms}},
volume = {37},
year = {2007}
}
@article{Lupton2012,
abstract = {In this paper, we present a novel method to fuse observations from an inertial measurement unit (IMU) and visual sensors, such that initial conditions of the inertial integration, including gravity estimation, can be recovered quickly and in a linear manner, thus removing any need for special initialization procedures. The algorithm is implemented using a graphical simultaneous localization and mapping like approach that guarantees constant time output. This paper discusses the technical aspects of the work, including observability and the ability for the system to estimate scale in real time. Results are presented of the system, estimating the platforms position, velocity, and attitude, as well as gravity vector and sensor alignment and calibration on-line in a built environment. This paper discusses the system setup, describing the real-time integration of the IMU data with either stereo or monocular vision data. We focus on human motion for the purposes of emulating high-dynamic motion, as well as to provide a localization system for future human-robot interaction. {\textcopyright} 2006 IEEE.},
author = {Lupton, Todd and Sukkarieh, Salah},
doi = {10.1109/TRO.2011.2170332},
file = {::},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Field robots,localization,search-and-rescue robots,sensor fusion},
number = {1},
pages = {61--76},
pmid = {6092505},
title = {{Visual-inertial-aided navigation for high-dynamic motion in built environments without initial conditions}},
volume = {28},
year = {2012}
}
@article{Sola2017,
abstract = {This article is an exhaustive revision of concepts and formulas related to quaternions and rotations in 3D space, and their proper use in estimation engines such as the error-state Kalman filter. The paper includes an in-depth study of the rotation group and its Lie structure, with formulations using both quaternions and rotation matrices. It makes special attention in the definition of rotation perturbations, derivatives and integrals. It provides numerous intuitions and geometrical interpretations to help the reader grasp the inner mechanisms of 3D rotation. The whole material is used to devise precise formulations for error-state Kalman filters suited for real applications using integration of signals from an inertial measurement unit (IMU).},
archivePrefix = {arXiv},
arxivId = {1711.02508},
author = {Sol{\`{a}}, Joan},
eprint = {1711.02508},
title = {{Quaternion kinematics for the error-state Kalman filter}},
url = {http://arxiv.org/abs/1711.02508},
year = {2017}
}
@article{Wolfe2011,
abstract = {An increasing number of real-world problems involve the measurement of data, andthe computation of estimates, on Lie groups. Moreover, establishing confidence in the resultingestimates is important. This paper therefore seeks to contribute to a larger theoretical frameworkthat generalizes classical multivariate statistical analysis from Euclidean space to the setting of Liegroups. The particular focus here is on extending Bayesian fusion, based on exponential familiesof probability densities, from the Euclidean setting to Lie groups. The definition and properties ofa new kind of Gaussian distribution for connected unimodular Lie groups are articulated, and ex-plicit formulas and algorithms are given for finding the mean and covariance of the fusion modelbased on the means and covariances of the constituent probability densities. The Lie groups thatfind the most applications in engineering are rotation groups and groups of rigid-body motions.Orientational (rotation-group) data and associated algorithms for estimation arise in problemsincluding satellite attitude, molecular spectroscopy, and global geological studies. In robotics andmanufacturing, quantifying errors in the position and orientation of tools and parts are impor-tant for task performance and quality control. Developing a general way to handle problemson Lie groups can be applied to all of these problems. In particular, we study the issue of howto ‘fuse' two such Gaussians and how to obtain a new Gaussian of the same form that is ‘closeto' the fused density.This is done at two levels of approximation that result from truncating theBaker-Campbell-Hausdorff formula with different numbers of terms. Algorithms are developedand numerical results are presented that are shown to generate the equivalent fused density withgood accuracy},
author = {Wolfe, Kevin C. and Mashner, Michael},
doi = {10.18409/jas.v2i1.11},
issn = {1309-3452},
journal = {Journal of Algebraic Statistics},
keywords = {and phrases,bayesian fusion,belief propaga-,lie groups,parametric distributions},
month = {apr},
number = {1},
pages = {75--97},
title = {{Bayesian Fusion on Lie Groups}},
url = {http://216.47.157.207/jalgstat/article/view/11},
volume = {2},
year = {2011}
}
@article{Sardi2017,
abstract = {Neurons are the computational elements that compose the brain and their fundamental principles of activity are known for decades. According to the long-lasting computational scheme, each neuron sums the incoming electrical signals via its dendrites and when the membrane potential reaches a certain threshold the neuron typically generates a spike to its axon. Here we present three types of experiments, using neuronal cultures, indicating that each neuron functions as a collection of independent threshold units. The neuron is anisotropically activated following the origin of the arriving signals to the membrane, via its dendritic trees. The first type of experiments demonstrates that a single neuron's spike waveform typically varies as a function of the stimulation location. The second type reveals that spatial summation is absent for extracellular stimulations from different directions. The third type indicates that spatial summation and subtraction are not achieved when combining intra- and extra- cellular stimulations, as well as for nonlocal time interference, where the precise timings of the stimulations are irrelevant. Results call to re-examine neuronal functionalities beyond the traditional framework, and the advanced computational capabilities and dynamical properties of such complex systems.},
author = {Sardi, Shira and Vardi, Roni and Sheinin, Anton and Goldental, Amir and Kanter, Ido},
doi = {10.1038/s41598-017-18363-1},
file = {::},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {18036},
pmid = {29269849},
publisher = {Springer US},
title = {{New Types of Experiments Reveal that a Neuron Functions as Multiple Independent Threshold Units}},
url = {http://www.nature.com/articles/s41598-017-18363-1},
volume = {7},
year = {2017}
}
@article{Yoon2013,
abstract = {In this paper, we develop an onboard real-time 3D visual simultaneous localization and mapping system for a dynamic walking humanoid robot. With the constraints of processing and real-time operation, the system uses a lightweight localization and mapping approach based around the well-known extended Kalman filter but that features a robust and real-time relocalization system able to allow loop-closing and robust localization in 6D. The robot is controlled by torque references at the joints using its dynamic properties. This results in more energy efficient motion but also in lager movement than the one found in a conventional ZMP-based humanoid which carefully maintains the position of the center of mass on the plane. These more agile motions pose challenges for a visual mapping system having to operate in real time. The developed system features a combination of stereo camera, robust visual descriptors, and motion model switching to compensate for the larger motion and uncertainty. We provide practical implementation details of the system and methods, and test on the real humanoid robot. We compare our results with motion obtained with a motion capture system. {\textcopyright} 2013 Copyright Taylor & Francis and The Robotics Society of Japan.},
author = {Yoon, Sukjune and Hyung, Seungyong and Lee, Minhyung and Roh, Kyung Shik and Ahn, Sunghwan and Gee, Andrew and Bunnun, Pished and Calway, Andrew and Mayol-Cuevas, Waterio W.},
doi = {10.1080/01691864.2013.785379},
file = {::},
issn = {01691864},
journal = {Advanced Robotics},
keywords = {SLAM,dynamic walking,humanoid},
number = {10},
pages = {759--772},
title = {{Real-time 3D simultaneous localization and map-building for a dynamic walking humanoid robot}},
volume = {27},
year = {2013}
}
@book{R.BFisher2005,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
author = {Fisher, Robert},
booktitle = {Journal of Electronic Imaging},
doi = {10.1117/1.2179077},
isbn = {9781119941866},
issn = {1017-9909},
keywords = {FUNDAMENTALS dictionary},
number = {1},
pages = {019902},
title = {{Dictionary of Computer Vision and Image Processing}},
volume = {15},
year = {2006}
}
@article{Vidal2017,
abstract = {Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this letter, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate-to the best of our knowledge-the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high dynamic range scenes. Videos of the experiments: Http://rpg.ifi.uzh.ch/ultimateslam.html.},
archivePrefix = {arXiv},
arxivId = {1709.06310},
author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
doi = {10.1109/LRA.2018.2793357},
eprint = {1709.06310},
file = {::},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,aerial systems: Perception and autonomy,visual-based navigation},
number = {2},
pages = {994--1001},
title = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios}},
url = {http://arxiv.org/abs/1709.06310%0Ahttp://dx.doi.org/10.1109/LRA.2018.2793357},
volume = {3},
year = {2018}
}
@article{Paton2016,
abstract = {Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for routefollowing applications; it provides continuous, six-degree-offreedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
author = {Paton, Michael and Mactavish, Kirk and Warren, Michael and Barfoot, Timothy D.},
doi = {10.1109/IROS.2016.7759303},
file = {:home/matias/Documents/Mendeley Desktop/Paton et al/2016/Paton et al. - 2016 - Bridging the appearance gap Multi-experience localization for long-term visual teach and repeat.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1918--1925},
publisher = {IEEE},
title = {{Bridging the appearance gap: Multi-experience localization for long-term visual teach and repeat}},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{Hutter2011,
abstract = {This paper introduces the mechanical design and the control concept of the Series Compliant Articulated Robotic Leg ScarlETH which was developed at ETH Zurich for fast, efficient, and versatile locomotion. Inspired by biological sys-tems, we seek to achieve this through large compliances in the joints which enable natural dynamics, allow temporary energy storage, and improve the passive adaptability. A sophisticated chain and cable pulley design minimizes the segment masses, places the overall CoG close to the hip joint, and maximizes the range of motion. Nonlinearities in the damping and an appro-priate low-level controller allow for precise torque control during stance and for fast task space position control during swing. This paved the road for the combined application of a virtual model controller for ground contact and a modified Raibert style controller for flight phase which was successfully tested in planar running.},
author = {Hutter, M. and Remy, C. D. and Hoepflinger, M. A. and Siegwart, R.},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/iros.2011.6094504},
file = {::},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {562--567},
publisher = {IEEE},
title = {{ScarlETH: Design and control of a planar running robot}},
url = {http://ieeexplore.ieee.org/document/6094504/},
year = {2011}
}
@article{Zotowski2015,
abstract = {The uncanny valley theory proposed by Mori has been heavily investigated in the recent years by researchers from various fields. However, the videos and images used in these studies did not permit any human interaction with the uncanny objects. Therefore, in the field of human-robot interaction it is still unclear what, if any, impact an uncanny-looking robot will have in the context of an interaction. In this paper we describe an exploratory empirical study using a live interaction paradigm that involved repeated interactions with robots that differed in embodiment and their attitude toward a human.We found that both investigated components of the uncanniness (likeability and eeriness) can be affected by an interaction with a robot. Likeability of a robot was mainly affected by its attitude and this effect was especially prominent for a machine-like robot. On the other hand, merely repeating interactions was sufficient to reduce eeriness irrespective of a robot's embodiment. As a result we urge other researchers to investigate Mori's theory in studies that involve actual human-robot interaction in order to fully understand the changing nature},
author = {Z{\l}otowski, Jakub A. and Sumioka, Hidenobu and Nishio, Shuichi and Glas, Dylan F. and Bartneck, Christoph and Ishiguro, Hiroshi},
doi = {10.3389/fpsyg.2015.00883},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {anthropomorphism,dehumanization,eeriness,human-robot interaction,likeability,multiple-interactions,uncanny valley},
number = {June},
pages = {1--14},
pmid = {26175702},
title = {{Persistence of the uncanny valley: the influence of repeated interactions and a robot's attitude on its perception}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.00883/abstract},
volume = {6},
year = {2015}
}
@article{Scona2017,
abstract = {In this paper we investigate the application of semi-dense visual Simultaneous Localisation and Mapping (SLAM) to the humanoid robotics domain. Challenges of visual SLAM applied to humanoids include the type of dynamic motion executed by the robot, a lack of features in man-made environments and the presence of dynamics in the scene. Previous research on humanoid SLAM focused mostly on feature-based methods which result in sparse environment reconstructions. Instead, we investigate the application of a modern direct method to obtain a semi-dense visually interpretable map which can be used for collision free motion planning. We tackle the challenge of using direct visual SLAM on a humanoid by proposing a more robust pose tracking method. This is formulated as an optimisation problem over a cost function which combines information from the stereo camera and a low-drift kinematic-inertial motion prior. Extensive experimental demonstrations characterise the performance of our method using the NASA Valkyrie humanoid robot in a laboratory environment equipped with a Vicon motion capture system. Our experiments demonstrate pose tracking robustness to challenges such as sudden view change, motion blur in the image, change in illumination and tracking through sequences of featureless areas in the environment. Finally, we provide a qualitative evaluation of our stereo reconstruction against a LIDAR map.},
author = {Scona, Raluca and Nobili, Simona and Petillot, Yvan R. and Fallon, Maurice},
doi = {10.1109/IROS.2017.8205943},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Figure 1},
pages = {1419--1426},
title = {{Direct visual SLAM fusing proprioception for a humanoid robot}},
volume = {2017-Septe},
year = {2017}
}
@article{Wang2017c,
abstract = {This paper presents a non-iterative solution to RGB-D-inertial odometry system. Traditional odometry methods resort to iterative algorithms which are usually computationally expensive or require well-designed initialization. To overcome this problem, this paper proposes to combine a non-iterative front-end (odometry) with an iterative back-end (loop closure) for the RGB-D-inertial SLAM system. The main contribution lies in the novel non-iterative front-end, which leverages on inertial fusion and kernel cross-correlators (KCC) to match point clouds in frequency domain. Dominated by the fast Fourier transform (FFT), our method is only of complexity $\mathcal{O}(n\log{n})$, where $n$ is the number of points. Map fusion is conducted by element-wise operations, so that both time and space complexity are further reduced. Extensive experiments show that, due to the lightweight of the proposed front-end, the framework is able to run at a much faster speed yet still with comparable accuracy with the state-of-the-arts.},
archivePrefix = {arXiv},
arxivId = {1710.05502},
author = {Wang, Chen and Hoang, Minh-Chung and Xie, Lihua and Yuan, Junsong},
eprint = {1710.05502},
file = {::},
pages = {1--14},
title = {{Non-iterative RGB-D-inertial Odometry}},
url = {http://arxiv.org/abs/1710.05502},
year = {2017}
}
@article{Ranganathan2009,
abstract = {Automatic detection of landmarks, usually special places in the environment such as gateways, for topological mapping has proven to be a difficult task. We present the use of Bayesian surprise, introduced in computer vision, for landmark detection. Further, we provide a novel hierarchical, graphical model for the appearance of a place and use this model to perform surprise-based landmark detection. Our scheme is agnostic to the sensor type, and we demonstrate this by implementing a simple laser model for computing surprise. We evaluate our landmark detector using appearance and laser measurements in the context of a topological mapping algorithm, thus demonstrating the practical applicability of the detector.{\textcopyright} 2009 IEEE.},
author = {Ranganathan, Ananth and Dellaert, Frank},
doi = {10.1109/ROBOT.2009.5152376},
file = {:home/matias/Documents/Mendeley Desktop/Ranganathan, Dellaert/2009/Ranganathan, Dellaert - 2009 - Bayesian surprise and landmark detection.pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2017--2023},
publisher = {IEEE},
title = {{Bayesian surprise and landmark detection}},
year = {2009}
}
@article{Vespa2018,
abstract = {We present a dense volumetric simultaneous localisation and mapping (SLAM) framework that uses an octree representation for efficient fusion and rendering of either a truncated signed distance field (TSDF) or an occupancy map. The primary aim of this letter is to use one single representation of the environment that can be used not only for robot pose tracking and high-resolution mapping, but seamlessly for planning. We show that our highly efficient octree representation of space fits SLAM and planning purposes in a real-time control loop. In a comprehensive evaluation, we demonstrate dense SLAM accuracy and runtime performance on-par with flat hashing approaches when using TSDF-based maps, and considerable speed-ups when using occupancy mapping compared to standard occupancy maps frameworks. Our SLAM system can run at 10-40 Hz on a modern quadcore CPU, without the need for massive parallelization on a GPU. We, furthermore, demonstrate a probabilistic occupancy mapping as an alternative to TSDF mapping in dense SLAM and show its direct applicability to online motion planning, using the example of informed rapidly-exploring random trees (RRT∗).},
author = {Vespa, Emanuele and Nikolov, Nikolay and Grimm, Marius and Nardi, Luigi and Kelly, Paul H.J. and Leutenegger, Stefan},
doi = {10.1109/LRA.2018.2792537},
file = {:home/matias/Documents/Mendeley Desktop/Vespa et al/2018/Vespa et al. - 2018 - Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mapping,simultaneous localisation and mapping (SLAM),visual-based navigation},
number = {2},
pages = {1144--1151},
title = {{Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping}},
volume = {3},
year = {2018}
}
@phdthesis{Dellaert2001,
abstract = {Approximate inference by sampling from an appropriately constructed posterior has recently \nseen a dramatic increase in popularity in both the robotics and computer vision community. In this \npaper, I will describe a number of approaches in which my co-authors and I have used Sequential \nMonte Carlo methods and Markov chain Monte Carlo sampling to solve a variety of difficult and \nchallenging inference problems. Very recently, we have also used sampling over variable dimension \nstate spaces to perform automatic model selection. I will present two examples of this, one in the \ndomain of computer vision, the other in mobile robotics. In both cases Rao-Blackwellization was \nused to integrate out the variable dimension-part of the state space, and hence the sampling was done \npurely over the (combinatorially large) space of different models. \nThis paper describes joint work with many collaborators over the past 5 years, both at Carnegie \nMellon University and at the Georgia Institute of Technology, including Dieter Fox, Sebastian \nThrun, Wolfram Burgard, Zia Khan, Tucker Balch, Michael Kaess, Rafal Zboinski, and Ananth \nRanganathan. \n},
author = {Dellaert, Frank},
file = {:home/matias/Documents/Mendeley Desktop/Dellaert/2001/Dellaert - 2001 - Monte Carlo EM for Data-Association and its Applications in Computer Vision.pdf:pdf},
school = {Carnegie Mellon University},
title = {{Monte Carlo EM for Data-Association and its Applications in Computer Vision}},
url = {http://frank.dellaert.com/pubs/Dellaert03ism.pdf},
year = {2001}
}
@article{Nishiwaki2012,
abstract = {The present paper describes the integration of laser-based perception, footstep planning, and walking control of a humanoid robot for navigation over previously unknown rough terrain. A perception system that obtains the shape of the surrounding environment to an accuracy of a few centimeters is realized based on input obtained using a scanning laser range sensor. A footstep planner decides the sequence of stepping positions using the obtained terrain shape. A walking controller that can cope with a few centimeters of error in terrain shape measurement is achieved by combining the generation of a 40-ms cycle online walking pattern and a ground reaction force controller with sensor feedback. An operational interface was developed to send commands to the robot. A mixed-reality display was adopted to realize an intuitive interface. The navigation system was implemented on the HRP-2, a full-size humanoid robot. The performance of the proposed system for navigation over unknown rough terrain and the accuracy of the terrain shape measurement were investigated through several experiments. {\textcopyright} 2012 The Author(s).},
author = {Nishiwaki, Koich I. and Chestnutt, Joel and Kagami, Satoshi},
doi = {10.1177/0278364912455720},
isbn = {0278364912455},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Humanoid robots,SLAM,computer vision,design and control,human-centered and life-like robots,legged robots,mapping,mobile and distributed robotics,range sensing,sensing and perception},
number = {11},
pages = {1251--1262},
title = {{Autonomous navigation of a humanoid robot over unknown rough terrain using a laser range sensor}},
url = {http://journals.sagepub.com/doi/10.1177/0278364912455720},
volume = {31},
year = {2012}
}
@article{Bry2012,
abstract = {In this paper we present a state estimation method based on an inertial measurement unit (IMU) and a planar laser range finder suitable for use in real-time on a fixed-wing micro air vehicle (MAV). The algorithm is capable of maintaing accurate state estimates during aggressive flight in unstructured 3D environments without the use of an external positioning system. Our localization algorithm is based on an extension of the Gaussian Particle Filter. We partition the state according to measurement independence relationships and then calculate a pseudo-linear update which allows us to use 20x fewer particles than a naive implementation to achieve similar accuracy in the state estimate. We also propose a multi-step forward fitting method to identify the noise parameters of the IMU and compare results with and without accurate position measurements. Our process and measurement models integrate naturally with an exponential coordinates representation of the attitude uncertainty. We demonstrate our algorithms experimentally on a fixed-wing vehicle flying in a challenging indoor environment. {\textcopyright} 2012 IEEE.},
author = {Bry, Adam and Bachrach, Abraham and Roy, Nicholas},
doi = {10.1109/ICRA.2012.6225295},
file = {::},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {Icra},
pages = {1--8},
title = {{State estimation for aggressive flight in GPS-denied environments using onboard sensing}},
year = {2012}
}
@article{Mur-Artal2015,
abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, J. M.M. and Tardos, Juan D.},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
file = {:home/matias/Documents/Mendeley Desktop/Mur-Artal, Montiel, Tardos/2015/Mur-Artal, Montiel, Tardos - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM System.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Lifelong mapping,Simultaneous localization and mapping (SLAM),localization,monocular vision,recognition},
number = {5},
pages = {1147--1163},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
url = {http://arxiv.org/abs/1502.00956},
volume = {31},
year = {2015}
}
@article{Michel2004,
abstract = {Cyberbotics Ltd. develops WebotsTM, a mobile robotics simulation software that provides you with a rapid prototyping environment for modelling, programming and simulating mobile robots. The provided robot libraries enable you to transfer your control programs to several commercially available real mobile robots. WebotsTM lets you define and modify a complete mobile robotics setup, even several different robots sharing the same environment. For each object, you can define a number of properties, such as shape, color, texture, mass, friction, etc. You can equip each robot with a large number of available sensors and actuators. You can program these robots using your favorite development environment, simulate them and optionally transfer the resulting programs onto your real robots. WebotsTM has been developed in collaboration with the Swiss Federal Institute of Technology in Lausanne, thoroughly tested, well documented and continuously maintained for over 7 years. It is now the main commercial product available from Cyberbotics Ltd.},
archivePrefix = {arXiv},
arxivId = {cs/0412052},
author = {Michel, Olivier},
doi = {10.1.1.86.1278},
eprint = {0412052},
file = {::},
isbn = {978-3-642-45345-8},
issn = {1729-8806},
journal = {International Journal of Advanced Robotic Systems},
keywords = {commercial software,mobile robot simulation,rapid prototyping,transfer to real robots,webots tm},
number = {1},
pages = {39--42},
pmid = {14599324},
primaryClass = {cs},
title = {{WebotsTM: Professional Mobile Robot Simulation}},
url = {http://arxiv.org/abs/cs/0412052},
volume = {1},
year = {2004}
}
@article{Cowan2002,
abstract = {This paper presents a framework for visual servoing that guarantees convergence to a visible goal from almost every initially visible configurations while maintaining full view of all the feature points along the way. The method applies to first- and second-order fully actuated plant models. The solution entails three components: a model for the "occlusion-free" configurations; a change of coordinates from image to model coordinates; and a navigation function for the model space. We present three example applications of the framework, along with experimental validation of its practical efficacy.},
author = {Cowan, Noah J. and Weingarten, Joel D. and Koditschek, Daniel E.},
doi = {10.1109/TRA.2002.802202},
file = {:home/matias/Documents/Mendeley Desktop/Cowan, Weingarten, Koditschek/2002/Cowan, Weingarten, Koditschek - 2002 - Visual servoing via navigation functions.pdf:pdf},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Dynamics,Finite field of view (FOV),Navigation functions,Obstacle avoidance,Occlusions,Vision-based control,Visual servoing},
number = {4},
pages = {521--533},
title = {{Visual servoing via navigation functions}},
volume = {18},
year = {2002}
}
@article{Vieville1995,
abstract = {The problem of computing structure and motion, given asset point correspondences in a monocular image sequence, considering small motions when the camera is not calibrated is addressed. The equations defining the calibration, rigid motion and scene structure are set. The first order expansion of these equations is developed and the observability of the related infinitesimal quantities is analyzed. It is shown that a complete correspondence between these equations and the equation derived in the discrete case are obtained. However, in the case of infinitesimal displacements, the projection of the translation is clearly separated from the rotational component of the motion.},
author = {Vieville, T. and Faugeras, O. D.},
doi = {10.1109/iccv.1995.466863},
isbn = {0818670428},
journal = {IEEE International Conference on Computer Vision},
keywords = {Dynamic vision,Motion analysis},
pages = {750--756},
title = {{Motion analysis with a camera with unknown, and possibly varying intrinsic parameters}},
year = {1995}
}
@article{Bruce2017,
abstract = {Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.},
archivePrefix = {arXiv},
arxivId = {1711.10137},
author = {Bruce, Jake and Suenderhauf, Niko and Mirowski, Piotr and Hadsell, Raia and Milford, Michael},
eprint = {1711.10137},
file = {:home/matias/Documents/Mendeley Desktop/Bruce et al/2017/Bruce et al. - 2017 - One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay.pdf:pdf},
number = {Nips},
title = {{One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay}},
url = {http://arxiv.org/abs/1711.10137},
year = {2017}
}
@article{Anderson2015,
abstract = {In this paper, we revisit batch state estimation through the lens of Gaussian process (GP) regression. We consider continuous-discrete estimation problems wherein a trajectory is viewed as a one-dimensional GP, with time as the independent variable. Our continuous-time prior can be defined by any nonlinear, time-varying stochastic differential equation driven by white noise; this allows the possibility of smoothing our trajectory estimates using a variety of vehicle dynamics models (e.g. ‘constant-velocity'). We show that this class of prior results in an inverse kernel matrix (i.e., covariance matrix between all pairs of measurement times) that is exactly sparse (block-tridiagonal) and that this can be exploited to carry out GP regression (and interpolation) very efficiently. When the prior is based on a linear, time-varying stochastic differential equation and the measurement model is also linear, this GP approach is equivalent to classical, discrete-time smoothing (at the measurement times); when a nonlinearity is present, we iterate over the whole trajectory to maximize accuracy. We test the approach experimentally on a simultaneous trajectory estimation and mapping problem using a mobile robot dataset.},
archivePrefix = {arXiv},
arxivId = {1412.0630},
author = {Anderson, Sean and Barfoot, Timothy D. and Tong, Chi Hay and S{\"{a}}rkk{\"{a}}, Simo},
doi = {10.1007/s10514-015-9455-y},
eprint = {1412.0630},
file = {:home/matias/Documents/Mendeley Desktop/Anderson et al/2015/Anderson et al. - 2015 - Batch nonlinear continuous-time trajectory estimation as exactly sparse Gaussian process regression.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Continuous time,Gaussian process regression,Localization,State estimation},
number = {3},
pages = {221--238},
publisher = {Springer US},
title = {{Batch nonlinear continuous-time trajectory estimation as exactly sparse Gaussian process regression}},
volume = {39},
year = {2015}
}
@article{Ballard2009,
abstract = {Almost all perception is active in the sense that we are aware of the percept and can use it to direct behaviors. Neuroscience research has demonstrated precise neural correlates of such percepts in the brain's neural firing patterns. Computational models programmed on binocular camera robot 'heads' have shown that the embodiment of active perception produces great economies in cost. The spatiotemporal coding of an active percept tends to be punctate, as can be demonstrated in virtual environments. These coding mechanisms can be succinctly described with Bayesian statistics, which can form the basic currency in graphical models that provide concise descriptions of extended tasks. {\textcopyright} 2009 unknown. Published by null.},
author = {Ballard, D. H.},
doi = {10.1016/B978-008045046-9.01436-4},
file = {:home/matias/Documents/Mendeley Desktop/Ballard/2009/Ballard - 2009 - Active Perception.pdf:pdf},
isbn = {9780080450469},
journal = {Encyclopedia of Neuroscience},
keywords = {Active vision,Bayesian statistics,Binocular robot heads,Graphical models,Virtual reality},
pages = {31--37},
title = {{Active Perception}},
year = {2009}
}
@article{Lupashin2011,
abstract = {The Flying Machine Arena (FMA) is an indoor research space built specifically for the study of autonomous systems and aerial robotics. In this video, we give an overview of this testbed and some of its capabilities. We show the FMA infrastructure and hardware, which includes a fleet of quadrocopters and a motion capture system for vehicle localization. The physical components of the FMA are complemented by specialized software tools and components that facilitate the use of the space and provide a unified framework for communication and control. The flexibility and modularity of the experimental platform is highlighted by various research projects and demonstrations. {\textcopyright} 2011 IEEE.},
author = {Lupashin, Sergei and Sch{\"{o}}llig, Angela and Hehn, Markus and D'Andrea, Raffaello},
doi = {10.1109/ICRA.2011.5980308},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2970--2971},
title = {{The flying machine arena as of 2010}},
year = {2011}
}
@article{Deutsch2015,
author = {Advisors, E T H Zurich},
title = {{Bringing State of the Art SLAM to the NAO Robot}},
year = {2015}
}
@article{Solway2012,
abstract = {Recent work has given rise to the view that reward-based decision making is governed by two key controllers: a habit system, which stores stimulus-response associations shaped by past reward, and a goal-oriented system that selects actions based on their anticipated outcomes. The current literature provides a rich body of computational theory addressing habit formation, centering on temporal-difference learning mechanisms. Less progress has been made toward formalizing the processes involved in goal-directed decision making. We draw on recent work in cognitive neuroscience, animal conditioning, cognitive and developmental psychology, and machine learning to outline a new theory of goal-directed decision making. Our basic proposal is that the brain, within an identifiable network of cortical and subcortical structures, implements a probabilistic generative model of reward, and that goal-directed decision making is effected through Bayesian inversion of this model. We present a set of simulations implementing the account, which address benchmark behavioral and neuroscientific findings, and give rise to a set of testable predictions. We also discuss the relationship between the proposed framework and other models of decision making, including recent models of perceptual choice, to which our theory bears a direct connection. {\textcopyright} 2012 American Psychological Association.},
author = {Solway, Alec and Botvinick, Matthew M.},
doi = {10.1037/a0026435},
file = {:home/matias/Documents/Mendeley Desktop/Solway, Botvinick/2012/Solway, Botvinick - 2012 - Goal-directed decision making as probabilistic inference A computational framework and potential neural corre.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Decision making,Neuroeconomics,Planning,Probabilistic inference,Reward},
number = {1},
pages = {120--154},
pmid = {22229491},
title = {{Goal-directed decision making as probabilistic inference: A computational framework and potential neural correlates}},
volume = {119},
year = {2012}
}
@article{Kuo2020,
abstract = {Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information-theoretic keyframe selection algorithm, and a scalable voxel-based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual-inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.},
archivePrefix = {arXiv},
arxivId = {2003.02014},
author = {Kuo, Juichung and Muglikar, Manasi and Zhang, Zichao and Scaramuzza, Davide},
eprint = {2003.02014},
file = {:home/matias/Documents/Mendeley Desktop/Kuo et al/2020/Kuo et al. - 2020 - Redesigning SLAM for Arbitrary Multi-Camera Systems.pdf:pdf},
title = {{Redesigning SLAM for Arbitrary Multi-Camera Systems}},
url = {http://arxiv.org/abs/2003.02014},
year = {2020}
}
@article{Ortin2001,
abstract = {The estimation of the 2D relative motion of an indoor robot using monocular vision is presented. The camera calibration is known, and its motion is limited to be planar. These constraints are included in the robust regression of epipolar geometry from point matches. Motion is derived from the epipolar geometry. A sequence of 54 real images is used to test the algorithm. Accurate motion, both in rotation and translation angles of 0.4 and 1.7 deg, is successfully derived.},
author = {Ort{\'{i}}n, D. and Montiel, J. M.M.},
doi = {10.1017/S0263574700003143},
file = {:home/matias/Documents/Mendeley Desktop/Ort{\'{i}}n, Montiel/2001/Ort{\'{i}}n, Montiel - 2001 - Indoor robot motion based on monocular images.pdf:pdf},
issn = {02635747},
journal = {Robotica},
keywords = {Accurate motion,Indoor robot,Monocular vision,Robust epipolar geometry},
number = {3},
pages = {331--342},
title = {{Indoor robot motion based on monocular images}},
volume = {19},
year = {2001}
}
@inproceedings{Nobili2017,
abstract = {In this paper we present a system for the state estimation of a dynamically walking and trotting quadruped. The approach fuses four heterogeneous sensor sources (inertial, kinematic, stereo vision and LIDAR) to maintain an accurate and consistent estimate of the robot's base link velocity and position in the presence of disturbances such as slips and missteps. We demonstrate the performance of our system, which is robust to changes in the structure and lighting of the environment, as well as the terrain over which the robot crosses. Our approach builds upon a modular inertial-driven Extended Kalman Filter which incorporates a rugged, probabilistic leg odometry component with additional inputs from stereo visual odometry and LIDAR registration. The simultaneous use of both stereo vision and LIDAR helps combat operational issues which occur in real applications. To the best of our knowledge, this paper is the first to discuss the complexity of consistent estimation of pose and velocity states, as well as the fusion of multiple exteroceptive signal sources at largely different frequencies and latencies, in a manner which is acceptable for a quadruped's feedback controller. A substantial experimental evaluation demonstrates the robustness and accuracy of our system, achieving continuously accurate localization and drift per distance traveled below 1cm/m.},
author = {Nobili, Simona and Camurri, Marco and Barasuol, Victor and Focchi, Michele and Caldwell, Darwin G. and Semini, Claudio and Fallon, Maurice},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/rss.2017.xiii.007},
isbn = {9780992374730},
issn = {2330765X},
title = {{Heterogeneous sensor fusion for accurate state estimation of dynamic legged robots}},
volume = {13},
year = {2017}
}
@inproceedings{Nobili2017a,
abstract = {State estimation techniques for humanoid robots are typically based on proprioceptive sensing and accumulate drift over time. This drift can be corrected using exteroceptive sensors such as laser scanners via a scene registration procedure. For this procedure the common assumption of high point cloud overlap is violated when the scenario and the robot's point-of-view are not static and the sensor's field-of-view (FOV) is limited. In this paper we focus on the localization of a robot with limited FOV in a semi-structured environment. We analyze the effect of overlap variations on registration performance and demonstrate that where overlap varies, outlier filtering needs to be tuned accordingly. We define a novel parameter which gives a measure of this overlap. In this context, we propose a strategy for robust non-incremental registration. The pre-filtering module selects planar macro-features from the input clouds, discarding clutter. Outlier filtering is automatically tuned at run-time to allow registration to a common reference in conditions of non-uniform overlap. An extensive experimental demonstration is presented which characterizes the performance of the algorithm using two humanoids: the NASA Valkyrie, in a laboratory environment, and the Boston Dynamics Atlas, during the DARPA Robotics Challenge Finals.},
author = {Nobili, Simona and Scona, Raluca and Caravagna, Marco and Fallon, Maurice},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989547},
isbn = {9781509046331},
issn = {10504729},
month = {may},
pages = {4721--4728},
publisher = {IEEE},
title = {{Overlap-based ICP tuning for robust localization of a humanoid robot}},
url = {http://ieeexplore.ieee.org/document/7989547/},
year = {2017}
}
@phdthesis{Scaramuzza2008,
abstract = {For mobile robots to be able to work with and for people and thus operate in our everyday environments, they need to be able to acquire knowledge through perception. In other words they need to collect sensor measure- ments from which they extract meaningful information. This thesis covers some of the essential components of a robot perception system combining omnidirectional vision, odometry, and 3D laser range finders, from modeling to extrinsic calibration, from feature extraction to ego-motion estimation. We covers all these topics from the point of view of an omnidirectional camera. The contributions of this work are several and are listed here. The thesis starts with an overview of the geometry of central omnidirec- tional cameras and gives also an overview of previous calibration methods. The contributions of this section are three. The first two are a new generalized model for describing both dioptric and catadioptric cameras and a calibration method which takes advantage of planar grids shown around the cameras, like the method in use for standard perspective cameras. The third contribution is the implementation of a toolbox for Matlab (called OCamCalib and freely available on-line), which implements the proposed calibration procedure. The second part of the thesis is dedicated to the extraction and matching of vertical features from omnidirectional images. Vertical features are usu- ally very predominant in indoor and outdoor structured environments and can then be very useful for robot navigation. The contribution of this part is a new method for matching vertical lines. The proposed method takes ad- vantage of a descriptor that is very distinctive for each feature. Furthermore, this descriptor is invariant to rotation and slight changes of illumination. The third part of the thesis is devoted to the extrinsic calibration of an omnidirectional camera with the odometry (i.e. wheel encoders) of a mobile robot. The contribution of this part is a new method of automatic self-calibration while the robot is moving. The method is based on an extended Kalman filter that combines the encoder readings with the bearing angle observations of one ore more vertical features in the environment. Further- more, an example of robot motion estimation is shown using the so calibrated camera-odometry system. The fourth part of the thesis is dedicated to the extrinsic calibration of an omnidirectional camera with a 3D laser range finder. The contribution of this method is that it uses no calibration object. Conversely, calibration is performed using laser-camera correspondences of natural points that are manually selected by the user. The novelty of the method resides in a new technique to visualize the usually ambiguous 3D information of range find- ers. We show that is possible to transform the range information into a new image where natural features of the environment are highlighted. Therefore, finding laser-camera correspondences becomes as easy as image pairing. The last part of the thesis is devoted to visual odometry for outdoor ground vehicles. We show a new method to recover the trajectory of a cali- brated omnidirectional camera over several hundred of meters by combining a feature based with an appearance based approach. All the contributions of this thesis are validated through experimental results using both simulated and real data.},
author = {Scaramuzza, Davide},
booktitle = {ETH Zurich, PhD Thesis},
doi = {10.3929/ethz-a-005567197},
keywords = {camera calibration,computer vision,omnidirectional vision,ransac,robotics,slam,structure from motion,visual odometry},
number = {17635},
pages = {189},
school = {ETH Zurich},
title = {{Omnidirectional vision: from calibration to robot motion estimation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.4525&rep=rep1&type=pdf%5Cnhttp://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Omnidirectional+vision:+from+calibration+to+robot+motion+estimation#0},
year = {2007}
}
@inproceedings{Wittmann2015,
abstract = {This paper introduces a new state estimator for biped robots fusing encoder, inertial and force torque measurements. The estimator is implemented as a Kalman filter that uses the dynamical model of the linear inverted pendulum with the center of mass (CoM) state as output. In order to compensate for disturbances and model errors we extend the model by a state for the external force and an additional input which is calculated from the dynamics error in pattern generation. Several simulation results underline the effectiveness of the proposed filter and show its robustness against disturbances. Experimental results and an application example validate the method under real world conditions.},
author = {Wittmann, Robert and Hildebrandt, Arne Christoph and Wahrmann, Daniel and Rixen, Daniel and Buschmann, Thomas},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353667},
file = {::},
isbn = {9781479999941},
issn = {21530866},
keywords = {Dynamics,Foot,Force measurement,Mathematical model,Planning,Robot sensing systems},
month = {sep},
pages = {2166--2172},
publisher = {IEEE},
title = {{State estimation for biped robots using multibody dynamics}},
url = {http://ieeexplore.ieee.org/document/7353667/},
volume = {2015-Decem},
year = {2015}
}
@article{Ranganathan2006,
abstract = {While probabilistic techniques have previously been investigated extensively for performing inference over the space of metric maps, no corresponding general-purpose methods exist for topological maps. We present the concept of probabilistic topological maps (PTMs), a sample-based representation that approximates the posterior distribution over topologies, given available sensor measurements. We show that the space of topologies is equivalent to the intractably large space of set partitions on the set of available measurements. The combinatorial nature of the problem is overcome by computing an approximate, sample-based representation of the posterior. The PTM is obtained by performing Bayesian inference over the space of all possible topologies, and provides a systematic solution to the problem of perceptual aliasing in the domain of topological mapping. In this paper, we describe a general framework for modeling measurements, and the use of a Markov-chain Monte Carlo algorithm that uses specific instances of these models for odometry and appearance measurements to estimate the posterior distribution. We present experimental results that validate our technique and generate good maps when using odometry and appearance, derived from panoramic images, as sensor measurements. {\textcopyright} 2006 IEEE.},
author = {Ranganathan, Ananth and Menegatti, Emanuele and Dellaert, Frank},
doi = {10.1109/TRO.2005.861457},
file = {:home/matias/Documents/Mendeley Desktop/Ranganathan, Menegatti, Dellaert/2006/Ranganathan, Menegatti, Dellaert - 2006 - Bayesian inference in the space of topological maps.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Bayesian inference,Markov-chain Monte Carlo (MCMC),Mobile robots,Perceptual aliasing,Probability distributions,Sample-based representations,Topological maps},
number = {1},
pages = {92--107},
title = {{Bayesian inference in the space of topological maps}},
volume = {22},
year = {2006}
}
@article{Yan2017,
abstract = {Recent work on simultaneous trajectory estimation and mapping (STEAM) for mobile robots has used Gaussian processes (GPs) to efficiently represent the robot's trajectory through its environment. GPs have several advantages over discrete-time trajectory representations: they can represent a continuous-time trajectory, elegantly handle asynchronous and sparse measurements, and allow the robot to query the trajectory to recover its estimated position at any time of interest. A major drawback of the GP approach to STEAM is that it is formulated as a batch trajectory estimation problem. In this paper we provide the critical extensions necessary to transform the existing GP-based batch algorithm for STEAM into an extremely efficient incremental algorithm. In particular, we are able to vastly speed up the solution time through efficient variable reordering and incremental sparse updates, which we believe will greatly increase the practicality of Gaussian process methods for robot mapping and localization. Finally, we demonstrate the approach and its advantages on both synthetic and real datasets.},
archivePrefix = {arXiv},
arxivId = {1504.02696},
author = {Yan, Xinyan and Indelman, Vadim and Boots, Byron},
doi = {10.1016/j.robot.2016.10.004},
eprint = {1504.02696},
file = {:home/matias/Documents/Mendeley Desktop/Yan, Indelman, Boots/2017/Yan, Indelman, Boots - 2017 - Incremental sparse GP regression for continuous-time trajectory estimation and mapping.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Continuous time,Gaussian process regression,Localization,SLAM,State estimation},
pages = {120--132},
publisher = {Elsevier B.V.},
title = {{Incremental sparse GP regression for continuous-time trajectory estimation and mapping}},
url = {http://dx.doi.org/10.1016/j.robot.2016.10.004},
volume = {87},
year = {2017}
}
@book{Hellier2016,
author = {Hellier, Jennifer L.},
isbn = {9781440834172},
publisher = {ABC-CLIO},
title = {{The Five Senses and Beyond: The Encyclopedia of Perception: The Encyclopedia of Perception}},
url = {https://books.google.cl/books?id=4sFCDQAAQBAJ},
year = {2016}
}
@article{Sunderhauf2012a,
abstract = {Current state of the art solutions of the SLAM problem are based on efficient sparse optimization techniques and represent the problem as probabilistic constraint graphs. For example in pose graphs the nodes represent poses and the edges between them express spatial information (e.g. obtained from odometry) and information on loop closures. The task of constructing the graph is delegated to a front-end that has access to the available sensor information. The optimizer, the so called back-end of the system, relies heavily on the topological correctness of the graph structure and is not robust against misplaced constraint edges. Especially edges representing false positive loop closures will lead to the divergence of current solvers. We propose a novel formulation that allows the back-end to change parts of the topological structure of the graph during the optimization process. The back-end can thereby discard loop closures and converge towards correct solutions even in the presence of false positive loop closures. This largely increases the overall robustness of the SLAM system and closes a gap between the sensor-driven front-end and the back-end optimizers. We demonstrate the approach and present results both on large scale synthetic and real-world datasets. {\textcopyright} 2012 IEEE.},
author = {S{\"{u}}nderhauf, Niko and Protzel, Peter},
doi = {10.1109/ICRA.2012.6224709},
file = {:home/matias/Documents/Mendeley Desktop/S{\"{u}}nderhauf, Protzel/2012/S{\"{u}}nderhauf, Protzel - 2012 - Towards a robust back-end for pose graph SLAM.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1254--1261},
title = {{Towards a robust back-end for pose graph SLAM}},
year = {2012}
}
@article{Barfoot2019,
abstract = {We present a Gaussian variational inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit both the mean and (inverse) covariance of a Gaussian to the posterior efficiently, by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. This is different than maximum a posteriori (MAP) estimation, which seeks the point estimate for the state that maximizes the posterior (i.e., the mode). The proposed exactly sparse Gaussian variational inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch–Tung–Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the ‘extended' RTS smoother in the nonlinear case because it finds the best-fit Gaussian (mean and covariance), not the MAP point estimate. We demonstrate the technique on controlled simulation problems and a batch nonlinear simultaneous localization and mapping problem with an experimental dataset.},
archivePrefix = {arXiv},
arxivId = {1911.08333},
author = {Barfoot, Timothy D. and Forbes, James R. and Yoon, David J.},
doi = {10.1177/0278364920937608},
eprint = {1911.08333},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Gaussian variational inference,derivative-free state estimation,exact sparsity},
number = {1},
pages = {1--31},
title = {{Exactly sparse Gaussian variational inference with application to derivative-free batch nonlinear state estimation}},
url = {http://arxiv.org/abs/1911.08333},
volume = {1},
year = {2020}
}
@article{Upper1974,
abstract = {Funny.},
author = {Upper, Dennis},
doi = {10.1901/jaba.1974.7-497a},
isbn = {0021-8855 (Print) 0021-8855 (Linking)},
issn = {00218855},
journal = {Journal of Applied Behavior Analysis},
number = {3},
pages = {497--497},
pmid = {16795475},
title = {{The unsuccessful self-treatment of a case of “writer's block”1}},
url = {http://www.pubmedcentral.gov/articlerender.fcgi?artid=1311997},
volume = {7},
year = {1974}
}
@article{Vogiatzis2011a,
abstract = {We investigate the problem of obtaining a dense reconstruction in real-time, from a live video stream. In recent years, multi-view stereo (MVS) has received considerable attention and a number of methods have been proposed. However, most methods operate under the assumption of a relatively sparse set of still images as input and unlimited computation time. Video based MVS has received less attention despite the fact that video sequences offer significant benefits in terms of usability of MVS systems. In this paper we propose a novel video based MVS algorithm that is suitable for real-time, interactive 3d modeling with a hand-held camera. The key idea is a per-pixel, probabilistic depth estimation scheme that updates posterior depth distributions with every new frame. The current implementation is capable of updating 15 million distributions/s. We evaluate the proposed method against the state-of-the-art real-time MVS method and show improvement in terms of accuracy. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Vogiatzis, George and Hern{\'{a}}ndez, Carlos},
doi = {10.1016/j.imavis.2011.01.006},
file = {:home/matias/Documents/Mendeley Desktop/Vogiatzis, Hern{\'{a}}ndez/2011/Vogiatzis, Hern{\'{a}}ndez - 2011 - Video-based, real-time multi-view stereo.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {3d reconstruction,Multi-view stereo,Real-time,Shape-from-X},
number = {7},
pages = {434--441},
publisher = {Elsevier B.V.},
title = {{Video-based, real-time multi-view stereo}},
url = {http://dx.doi.org/10.1016/j.imavis.2011.01.006},
volume = {29},
year = {2011}
}
@article{Rotella2016,
abstract = {This work presents methods for the determination of a humanoid robot's joint velocities and accelerations directly from link-mounted Inertial Measurement Units (IMUs) each containing a three-axis gyroscope and a three-axis accelerometer. No information about the global pose of the floating base or its links is required and precise knowledge of the link IMU poses is not necessary due to presented calibration routines. Additionally, a filter is introduced to fuse gyroscope angular velocities with joint position measurements and compensate the computed joint velocities for time-varying gyroscope biases. The resulting joint velocities are subject to less noise and delay than filtered velocities computed from numerical differentiation of joint potentiometer signals, leading to superior performance in joint feedback control as demonstrated in experiments performed on a SARCOS hydraulic humanoid.},
archivePrefix = {arXiv},
arxivId = {1602.05134},
author = {Rotella, Nicholas and Mason, Sean and Schaal, Stefan and Righetti, Ludovic},
doi = {10.1109/ICRA.2016.7487328},
eprint = {1602.05134},
file = {::},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1825--1831},
title = {{Inertial sensor-based humanoid joint state estimation}},
volume = {2016-June},
year = {2016}
}
@article{Clement2017,
abstract = {Visual Teach and Repeat (VT&R) allows an autonomous vehicle to accurately repeat a previously traversed route using only vision sensors. Most VT&R systems rely on natively three-dimensional (3D) sensors such as stereo cameras for mapping and localization, but many existing mobile robots are equipped with only 2D monocular vision, typically for teleoperation. In this paper, we extend VT&R to the most basic sensor configuration—a single monocular camera. We show that kilometer-scale route repetition can be achieved with centimeter-level accuracy by approximating the local ground surface near the vehicle as a plane with some uncertainty. This allows our system to recover absolute scale from the known position and orientation of the camera relative to the vehicle, which simplifies threshold-based outlier rejection and the estimation and control of lateral path-tracking error—essential components of high-accuracy route repetition. We enhance the robustness of our monocular VT&R system to common failure cases through the use of color-constant imagery, which provides it with a degree of resistance to lighting changes and moving shadows where keypoint matching on standard gray images tends to struggle. Through extensive testing on a combined 30 km of autonomous navigation data collected on multiple vehicles in a variety of highly nonplanar terrestrial and planetary-analogue environments, we demonstrate that our system is capable of achieving route-repetition accuracy on par with its stereo counterpart, with only a modest tradeoff in robustness.},
author = {Clement, Lee and Kelly, Jonathan and Barfoot, Timothy D.},
doi = {10.1002/rob.21655},
file = {:home/matias/Documents/Mendeley Desktop/Clement, Kelly, Barfoot/2017/Clement, Kelly, Barfoot - 2017 - Robust Monocular Visual Teach and Repeat Aided by Local Ground Planarity and Color-constant Imagery.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {74--97},
title = {{Robust Monocular Visual Teach and Repeat Aided by Local Ground Planarity and Color-constant Imagery}},
volume = {34},
year = {2017}
}
@article{Anderson2013a,
abstract = {Visual odometry (VO) is a highly efficient and powerful 6D motion estimation technique; state-of-the-art bundle adjustment algorithms now optimize over several frames of temporally tracked, appearance-based features in real time. It is well known that the temporal feature correspondence process is highly prone to mismatches. The standard technique used for outlier rejection in this process is random sample consensus (RANSAC), which is an iterative and non-deterministic process used to find the parameters of a mathematical model that best describe a likely set of inliers. The traditional model used for RANSAC in the visual odometry pipeline is a rigid transformation between two camera poses; this model has long assumed the use of an imaging sensor with a global shutter. In order to use imaging sensors that do not operate with a global shutter, it is proposed that the RANSAC algorithm be modified to use a constant-camera-velocity model. Specifically, this paper investigates the use of a two-axis scanning lidar in the visual-odometry pipeline. Images are formed using lidar intensity data, and due to the scanning-while-moving nature of the lidar, the behaviour of the sensor resembles that of a slow rolling-shutter camera. We formulate a Motion-Compensated RANSAC algorithm that uses a constant-velocity model and the individual timestamp of each extracted feature. The algorithm is validated using 6880 lidar frames with a resolution of 480 × 360, captured at 2Hz, over a 1.1km traversal. Our results show that the new algorithm results in far more inlying feature tracks for rolling-shutter-type images and ultimately higher-accuracy VO results. {\textcopyright} 2013 IEEE.},
author = {Anderson, Sean and Barfoot, Timothy D.},
doi = {10.1109/IROS.2013.6696649},
file = {:home/matias/Documents/Mendeley Desktop/Anderson, Barfoot/2013/Anderson, Barfoot - 2013 - RANSAC for motion-distorted 3D visual sensors.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2093--2099},
publisher = {IEEE},
title = {{RANSAC for motion-distorted 3D visual sensors}},
year = {2013}
}
@book{Asada1987,
abstract = {This book describes the design concept and discusses the control issues related to the performance of a direct-drive robot, specifically, a direct-drive mechanical arm capable of carrying up to 10 kilograms, at 10 meters per second, accelerating at 5 G (a unit of acceleration equal to the acceleration of gravity). These are remarkable achievements compared to current industrial robots that move with speeds on the order of 1 meter per second. Direct-Drive Robot presents the most current research in manipulator design and control, emphasizing the high-performance direct-drive robot arm in which the shafts of articulated joints are directly coupled to the rotors of motors with high torque. It describes fundamental technologies of key components such as motors, amplifiers and sensors, arm linkage design, and control system design, and makes significant contributions in the areas of power efficiency analysis, dynamic mass balancing, and decoupling theory. The book provides a good balance between theory and practice, covering the practical design and implementation of this special robot as well as the theoretical design tools.  Contents:    Part I: Direct-Drive Technologies. Introduction. Components.    Part II: Arm Design Theory. Power Efficiency. Arm Design for Simplified Dynamics. Actuator Relocation. Design of Decoupled Arm Structures.    Part III: Development of the MIT Arm. Mechanisms. Control Systems.    Part IV: Selected Papers on Direct-Drive Robot Design and Control.},
author = {Asada, H and Youcef-Toumi, Kamal},
file = {::},
isbn = {0262010887},
keywords = {Direct drive motor,Manipulators (Mechanism),Robotics.},
title = {{Direct-drive robots: theory and practice}},
year = {1987}
}
@article{Nava2019,
author = {Nava, Yoshua and Wulf, Maximillian and Khambhaita, Harmish and Gehring, Christian},
file = {:home/matias/Documents/Mendeley Desktop/Nava et al/2019/Nava et al. - 2019 - How to Configure the Intel Realsense D435 Camera for Optimal Terrain Mapping on ANYmal Setup of the Intel RealSense.pdf:pdf},
pages = {1--9},
title = {{How to Configure the Intel Realsense D435 Camera for Optimal Terrain Mapping on ANYmal Setup of the Intel RealSense D435}},
year = {2019}
}
@article{VonStumberg2020a,
abstract = {We present LM-Reloc -- a novel approach for visual relocalization based on direct image alignment. In contrast to prior works that tackle the problem with a feature-based formulation, the proposed method does not rely on feature matching and RANSAC. Hence, the method can utilize not only corners but any region of the image with gradients. In particular, we propose a loss formulation inspired by the classical Levenberg-Marquardt algorithm to train LM-Net. The learned features significantly improve the robustness of direct image alignment, especially for relocalization across different conditions. To further improve the robustness of LM-Net against large image baselines, we propose a pose estimation network, CorrPoseNet, which regresses the relative pose to bootstrap the direct image alignment. Evaluations on the CARLA and Oxford RobotCar relocalization tracking benchmark show that our approach delivers more accurate results than previous state-of-the-art methods while being comparable in terms of robustness.},
archivePrefix = {arXiv},
arxivId = {2010.06323},
author = {von Stumberg, Lukas and Wenzel, Patrick and Yang, Nan and Cremers, Daniel},
eprint = {2010.06323},
file = {:home/matias/Documents/Mendeley Desktop/von Stumberg et al/2020/von Stumberg et al. - 2020 - LM-Reloc Levenberg-Marquardt Based Direct Visual Relocalization.pdf:pdf},
title = {{LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization}},
url = {http://arxiv.org/abs/2010.06323},
year = {2020}
}
@article{Kaess2008,
abstract = {In this paper, we present incremental smoothing and mapping (iSAM), which is a novel approach to the simultaneous localization and mapping problem that is based on fast incremental matrix factorization. iSAM provides an efficient and exact solution by updating a QR factorization of the naturally sparse smoothing information matrix, thereby recalculating only those matrix entries that actually change. iSAM is efficient even for robot trajectories with many loops as it avoids unnecessary fill-in in the factor matrix by periodic variable reordering. Also, to enable data association in real time, we provide efficient algorithms to access the estimation uncertainties of interest based on the factored information matrix. We systematically evaluate the different components of iSAM as well as the overall algorithm using various simulated and real-world datasets for both landmark and pose-only settings. {\textcopyright} 2008 IEEE.},
author = {Kaess, Michael and Ranganathan, Ananth and Dellaert, Frank},
doi = {10.1109/TRO.2008.2006706},
file = {:home/matias/Documents/Mendeley Desktop/Kaess, Ranganathan, Dellaert/2008/Kaess, Ranganathan, Dellaert - 2008 - iSAM Incremental smoothing and mapping.pdf:pdf},
isbn = {9781439813287},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Data association,Localization,Mapping,Mobile robots,Nonlinear estimation,Simultaneous localization and mapping (SLAM),Smoothing},
number = {6},
pages = {1365--1378},
title = {{iSAM: Incremental smoothing and mapping}},
volume = {24},
year = {2008}
}
@incollection{Fojtu2012,
abstract = {Nao humanoid robot from Aldebaran Robotics is equipped with an odometry sensor providing rather inaccurate robot pose estimates. We propose using Structure from Motion (SfM) to enable visual odometry from Nao camera without the necessity to add artificial markers to the scene and show that the robot pose estimates can be significantly improved by fusing the data from the odometry sensor and visual odometry. The implementation consists of the sensor modules streaming robot data, the mapping module creating a 3D model, the visual localization module estimating camera pose w.r.t. the model, and the navigation module planning robot trajectories and performing the actual movement. All of the modules are connected through the RSB middleware, which makes the solution independent on the given robot type. {\textcopyright} Springer-Verlag Berlin Heidelberg 2012.},
author = {Fojtů, {\v{S}}imon and Havlena, Michal and Pajdla, Tom{\'{a}}{\v{s}}},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33515-0_43},
isbn = {9783642335143},
issn = {03029743},
keywords = {Nao humanoid robot,Robot localization,Robot navigation,Structure from motion},
number = {PART 2},
pages = {427--438},
title = {{Nao robot localization and navigation using fusion of odometry and visual sensor data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33515-0_43 http://link.springer.com/10.1007/978-3-642-33515-0_43},
volume = {7507 LNAI},
year = {2012}
}
@article{Wang2017,
abstract = {The goal of this paper is to create a new framework for dense SLAM that is light enough for micro-robot systems based on depth camera and inertial sensor. Feature-based and direct methods are two mainstreams in visual SLAM. Both methods minimize photometric or reprojection error by iterative solutions, which are computationally expensive. To overcome this problem, we propose a non-iterative framework to reduce computational requirement. First, the attitude and heading reference system (AHRS) and axonometric projection are utilized to decouple the 6 Degree-of-Freedom (DoF) data, so that point clouds can be matched in independent spaces respectively. Second, based on single key-frame training, the matching process is carried out in frequency domain by Fourier transformation, which provides a closed-form non-iterative solution. In this manner, the time complexity is reduced to O(n log n), where n is the number of matched points in each frame. To the best of our knowledge, this method is the first non-iterative and online trainable approach for data association in visual SLAM. Compared with the state-of-the-arts, it runs at a faster speed and obtains 3-D maps with higher resolution yet still with comparable accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05294},
author = {Wang, Chen and Yuan, Junsong and Xie, Lihua},
doi = {10.1109/ICAR.2017.8023500},
eprint = {1701.05294},
file = {::},
isbn = {9781538631577},
journal = {2017 18th International Conference on Advanced Robotics, ICAR 2017},
keywords = {Depth Camera,Localization,Mapping,Non-Iterative Method,Visual-Inertial Odometry},
pages = {83--90},
title = {{Non-iterative SLAM}},
year = {2017}
}
@inproceedings{Won2020,
abstract = {In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.},
archivePrefix = {arXiv},
arxivId = {2003.08056},
author = {Won, Changhee and Seok, Hochang and Cui, Zhaopeng and Pollefeys, Marc and Lim, Jongwoo},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA40945.2020.9196695},
eprint = {2003.08056},
file = {:home/matias/Documents/Mendeley Desktop/Won et al/2020/Won et al. - 2020 - OmniSLAM Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems.pdf:pdf},
isbn = {9781728173955},
title = {{OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems}},
url = {http://arxiv.org/abs/2003.08056},
year = {2020}
}
@article{Turing1950,
abstract = {1. The Imitation Game. Ipropose {tO} consider the {question,'Can} machines think? This should begin with definitions of the meaning of the terms' machine'and'think'. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is ...},
author = {Turing, A. M.},
doi = {10.2307/2251299},
isbn = {0-89391-369-3},
journal = {Mind},
keywords = {Children,Computer programming,Disabilities,Imperative logic,Machinery,Mathematics,Nervous system,Soul,Turing test,Wheels},
number = {236},
pages = {433--460},
title = {{Turing+Mind+1950.pdf}},
url = {https://www.jstor.org/stable/2251299%0Ahttp://www.jstor.org/stable/2251299%0Ahttps://www.jstor.org/stable/2251299%0Ahttp://www.jstor.org/stable/2251299},
volume = {59},
year = {1950}
}
@article{Mallat2016,
abstract = {Deep convolutional networks provide state-of-the-art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and nonlinearities. A mathematical framework is introduced to analyse their properties. Computations of invariants involve multiscale contractions with wavelets, the linearization of hierarchical symmetries and sparse separations. Applications are discussed.},
archivePrefix = {arXiv},
arxivId = {1601.04920},
author = {Mallat, St{\'{e}}phane},
doi = {10.1098/rsta.2015.0203},
eprint = {1601.04920},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Deep convolutional neural networks,Learning,Wavelets},
number = {2065},
pages = {1--17},
title = {{Understanding deep convolutional networks}},
url = {http://arxiv.org/abs/1601.04920},
volume = {374},
year = {2016}
}
@article{Hirose2019,
abstract = {Humans can routinely follow a trajectory defined by a list of images/landmarks. However, traditional robot navigation methods require accurate mapping of the environment, localization, and planning. Moreover, these methods are sensitive to subtle changes in the environment. In this paper, we propose a Deep Visual MPC-policy learning method that can perform visual navigation while avoiding collisions with unseen objects on the navigation path. Our model PoliNet takes in as input a visual trajectory and the image of the robot's current view and outputs velocity commands for a planning horizon of N steps that optimally balance between trajectory following and obstacle avoidance. PoliNet is trained using a strong image predictive model and traversability estimation model in a MPC setup, with minimal human supervision. Different from prior work, PoliNet can be applied to new scenes without retraining. We show experimentally that the robot can follow a visual trajectory when varying start position and in the presence of previously unseen obstacles. We validated our algorithm with tests both in a realistic simulation environment and in the real world. We also show that we can generate visual trajectories in simulation and execute the corresponding path in the real environment. Our approach outperforms classical approaches as well as previous learning-based baselines in success rate of goal reaching, sub-goal coverage rate, and computational load.},
author = {Hirose, Noriaki and Xia, Fei and Martin-Martin, Roberto and Sadeghian, Amir and Savarese, Silvio},
doi = {10.1109/LRA.2019.2925731},
file = {:home/matias/Documents/Mendeley Desktop/Hirose et al/2019/Hirose et al. - 2019 - Deep Visual MPC-Policy Learning for Navigation.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
month = {oct},
number = {4},
pages = {3184--3191},
title = {{Deep Visual MPC-Policy Learning for Navigation}},
url = {https://ieeexplore.ieee.org/document/8750823/},
volume = {4},
year = {2019}
}
@article{Indelman2012,
abstract = {This paper presents a new approach for high-rate information fusion in modern inertial navigation systems, that have a variety of sensors operating at different frequencies. Optimal information fusion corresponds to calculating the maximum a posteriori estimate over the joint probability distribution function (pdf) of all states, a computationally-expensive process in the general case. Our approach consists of two key components, which yields a flexible, high-rate, near-optimal inertial navigation system. First, the joint pdf is represented using a graphical model, the factor graph, that fully exploits the system sparsity and provides a plug and play capability that easily accommodates the addition and removal of measurement sources. Second, an efficient incremental inference algorithm over the factor graph is applied, whose performance approaches the solution that would be obtained by a computationally-expensive batch optimization at a fraction of the computational cost. To further aid high-rate performance, we introduce an equivalent IMU factor based on a recently developed technique for IMU pre-integration, drastically reducing the number of states that must be added to the system. The proposed approach is experimentally validated using real IMU and imagery data that was recorded by a ground vehicle, and a statistical performance study is conducted in a simulated aerial scenario. A comparison to conventional fixed-lag smoothing demonstrates that our method provides a considerably improved trade-off between computational complexity and performance. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Indelman, Vadim and Williams, Stephen and Kaess, Michael and Dellaert, Frank},
doi = {10.1016/j.robot.2013.05.001},
file = {::},
isbn = {9780982443859},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Graphical models,Incremental inference,Inertial navigation,Multi-sensor fusion,Plug and play architecture},
number = {8},
pages = {721--738},
title = {{Information fusion in navigation systems via factor graph based incremental smoothing}},
volume = {61},
year = {2013}
}
@article{Dabney2020,
abstract = {Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1–3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning4–6. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.},
author = {Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'{e}}mi and Botvinick, Matthew},
doi = {10.1038/s41586-019-1924-6},
file = {:home/matias/Documents/Mendeley Desktop/Dabney et al/2020/Dabney et al. - 2020 - A distributional code for value in dopamine-based reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7792},
pages = {671--675},
pmid = {31942076},
publisher = {Springer US},
title = {{A distributional code for value in dopamine-based reinforcement learning}},
url = {http://dx.doi.org/10.1038/s41586-019-1924-6},
volume = {577},
year = {2020}
}
@article{Kazik2012,
abstract = {In this paper, we present a framework for 6D absolute scale motion and structure estimation of a multi-camera system in challenging indoor environments. It operates in real-time and employs information from two cameras with non-overlapping fields of view. Monocular Visual Odometry supplying up-to-scale 6D motion information is carried out in each of the cameras, and the metric scale is recovered via a linear solution by imposing the known static transformation between both sensors. The redundancy in the motion estimates is finally exploited by a statistical fusion to an optimal 6D metric result. The proposed technique is robust to outliers and able to continuously deliver a reasonable measurement of the scale factor. The quality of the framework is demonstrated by a concise evaluation on indoor datasets, including a comparison to accurate ground truth data provided by an external motion tracking system. {\textcopyright} 2012 IEEE.},
author = {Kazik, Tim and Kneip, Laurent and Nikolic, Janosch and Pollefeys, Marc and Siegwart, Roland},
doi = {10.1109/CVPR.2012.6247843},
file = {:home/matias/Documents/Mendeley Desktop/Kazik et al/2012/Kazik et al. - 2012 - Real-time 6D stereo Visual Odometry with non-overlapping fields of view.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1529--1536},
title = {{Real-time 6D stereo Visual Odometry with non-overlapping fields of view}},
year = {2012}
}
@article{Gomez-Ojeda2017,
abstract = {One of the main open challenges in visual odometry (VO) is the robustness to difficult illumination conditions or high dynamic range (HDR) environments. The main difficulties in these situations come from both the limitations of the sensors and the inability to perform a successful tracking of interest points because of the bold assumptions in VO, such as brightness constancy. We address this problem from a deep learning perspective, for which we first fine-tune a deep neural network with the purpose of obtaining enhanced representations of the sequences for VO. Then, we demonstrate how the insertion of long short term memory allows us to obtain temporally consistent sequences, as the estimation depends on previous states. However, the use of very deep networks enlarges the computational burden of the VO framework; therefore, we also propose a convolutional neural network of reduced size capable of performing faster. Finally, we validate the enhanced representations by evaluating the sequences produced by the two architectures in several state-of-art VO algorithms, such as ORB-SLAM and DSO.},
archivePrefix = {arXiv},
arxivId = {1707.01274},
author = {Gomez-Ojeda, Ruben and Zhang, Zichao and Gonzalez-Jimenez, Javier and Scaramuzza, Davide},
doi = {10.1109/ICRA.2018.8462876},
eprint = {1707.01274},
file = {::},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {805--811},
title = {{Learning-Based Image Enhancement for Visual Odometry in Challenging HDR Environments}},
url = {http://arxiv.org/abs/1707.01274},
year = {2018}
}
@article{Mourikis2007,
abstract = {In this paper, we present an Extended Kalman Filter (EKF)-based algorithm for real-time vision-aided inertial navigation. The primary contribution of this work is the derivation of a measurement model that is able to express the geometric constraints that arise when a static feature is observed from multiple camera poses. This measurement model does not require including the 3D feature position in the state vector of the EKF and is optimal, up to linearization errors. The vision-aided inertial navigation algorithm we propose has computational complexity only linear in the number of features, and is capable of high-precision pose estimation in large-scale real-world environments. The performance of the algorithm is demonstrated in extensive experimental results, involving a camera/IMU system localizing within an urban area. {\textcopyright} 2007 IEEE.},
author = {Mourikis, Anastasios I. and Roumeliotis, Stergios I.},
doi = {10.1109/ROBOT.2007.364024},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3565--3572},
pmid = {4209642},
title = {{A multi-state constraint Kalman filter for vision-aided inertial navigation}},
year = {2007}
}
@article{Maier2012,
abstract = {In this paper, we present an integrated approach for robot localization, obstacle mapping, and path planning in 3D environments based on data of an onboard consumer-level depth camera. We rely on state-of-the-art techniques for environment modeling and localization, which we extend for depth camera data. We thoroughly evaluated our system with a Nao humanoid equipped with an Asus Xtion Pro Live depth camera on top of the humanoid's head and present navigation experiments in a multi-level environment containing static and non-static obstacles. Our approach performs in real-time, maintains a 3D environment representation, and estimates the robot's pose in 6D. As our results demonstrate, the depth camera is well-suited for robust localization and reliable obstacle avoidance in complex indoor environments. {\textcopyright} 2012 IEEE.},
author = {Maier, Daniel and Hornung, Armin and Bennewitz, Maren},
doi = {10.1109/HUMANOIDS.2012.6651595},
isbn = {9781467313698},
issn = {21640572},
journal = {IEEE-RAS International Conference on Humanoid Robots},
pages = {692--697},
title = {{Real-time navigation in 3D environments based on depth camera data}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6651595%5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6651595},
year = {2012}
}
@article{Krajnik2017,
abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
author = {Krajnik, Tomas and Fentanes, Jaime P. and Santos, Joao M. and Duckett, Tom},
doi = {10.1109/TRO.2017.2665664},
file = {:home/matias/Documents/Mendeley Desktop/Krajnik et al/2017/Krajnik et al. - 2017 - FreMEn Frequency map enhancement for long-term mobile robot autonomy in changing environments.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Localization,long-term autonomy,mapping},
number = {4},
pages = {964--977},
title = {{FreMEn: Frequency map enhancement for long-term mobile robot autonomy in changing environments}},
volume = {33},
year = {2017}
}
@article{Burri2016a,
abstract = {As the applications of Micro Aerial Vehicles (MAVs) get more and more complex, and require highly dynamic motions, it becomes essential to have an accurate dynamic model of the MAV. Such a model can be used for reliable state estimation, control, and for realistic simulation. A good model requires accurate estimates of physical parameters of the system, which we aim to estimate from recorded flight data. In this paper, we present a detailed physical model of the MAV and a maximum likelihood estimation scheme for determining the dominant parameters, such as inertia matrix, center of gravity (CoG) with respect to the IMU, and parameters related to the aerodynamics. To incorporate all information given by the IMU and the physical MAV model, we propose to use two process models in the optimization. We show the effectiveness of the method on simulated data, as well as on a real platform.},
author = {Burri, Michael and Nikolic, Janosch and Oleynikova, Helen and Achtelik, Markus W. and Siegwart, Roland},
doi = {10.1109/ICRA.2016.7487627},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {May},
pages = {4297--4303},
title = {{Maximum likelihood parameter identification for MAVs}},
volume = {2016-June},
year = {2016}
}
@article{Chuang,
author = {Chuang, Tzu-kuan and Teng, Chun-chih and Pillai, Sudeep and Hung, Chen-hao and Huang, Yi-wei and Kuo, Chang-yi and Lee, Teng-yok and Paull, Liam and Leonard, John and Wang, Hsueh-cheng},
file = {::},
title = {{Robust Deep Text Spotting for Resource-Constrained Mobile Robots}}
}
@inproceedings{Hartley2018,
abstract = {State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking.},
archivePrefix = {arXiv},
arxivId = {1712.05873},
author = {Hartley, Ross and Mangelson, Josh and Gan, Lu and Jadidi, Maani Ghaffari and Walls, Jeffrey M. and Eustice, Ryan M. and Grizzle, Jessy W.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8460748},
eprint = {1712.05873},
file = {:home/matias/Documents/Mendeley Desktop/Hartley et al/2018/Hartley et al. - 2018 - Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
pages = {4422--4429},
title = {{Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors}},
year = {2018}
}
@book{Stillwell2008,
abstract = {We study a family of 'classical' orthogonal polynomials which satisfy (apart from a three-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl type. These polynomials can be obtained from the little q-Jacobi polynomials in the limit q = -1. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for q = -1. {\textcopyright} 2011 IOP Publishing Ltd.},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Vinet, Luc and Zhedanov, Alexei},
booktitle = {Journal of Physics A: Mathematical and Theoretical},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {1011.1669},
file = {::},
isbn = {978-0-387-78214-0},
issn = {17518113},
keywords = {icle},
number = {8},
pmid = {25246403},
publisher = {Springer New York},
series = {Undergraduate Texts in Mathematics},
title = {{A 'missing' family of classical orthogonal polynomials}},
url = {http://link.springer.com/10.1007/978-0-387-78214-0},
volume = {44},
year = {2011}
}
@book{Corke2017,
abstract = {Robotic vision, the combination of robotics and computer vision, involves the application of computer algorithms to data acquired from sensors. The research community has developed a large body of such algorithms but for a newcomer to the field this can be quite daunting. For over 20 years the author has maintained two open-source MATLAB{\textregistered} Toolboxes, one for robotics and one for vision. They provide implementations of many important algorithms and allow users to work with real problems, not just trivial examples. This book makes the fundamental algorithms of robotics, vision and control accessible to all. It weaves together theory, algorithms and examples in a narrative that covers robotics and computer vision separately and together. Using the latest versions of the Toolboxes the author shows how complex problems can be decomposed and solved using just a few simple lines of code. The topics covered are guided by real problems observed by the author over many years as a practitioner of both robotics and computer vision. It is written in an accessible but informative style, easy to read and absorb, and includes over 1000 MATLAB and Simulink{\textregistered} examples and over 400 figures. The book is a real walk through the fundamentals of mobile robots, arm robots. then camera models, image processing, feature extraction and multi-view geometry and finally bringing it all together with an extensive discussion of visual servo systems. This second edition is completely revised, updated and extended with coverage of Lie groups, matrix exponentials and twists; inertial navigation; differential drive robots; lattice planners; pose-graph SLAM and map making; restructured material on arm-robot kinematics and dynamics; series-elastic actuators and operational-space control; Lab color spaces; light field cameras; structured light, bundle adjustment and visual odometry; and photometric visual servoing.},
author = {Corke, Peter},
booktitle = {Annals of Mathematics and Artificial Intelligence},
doi = {10.1007/978-3-319-54413-7},
isbn = {978-3-319-54413-7},
issn = {15737470},
number = {1-2},
pages = {693},
publisher = {Springer International Publishing},
series = {Springer Tracts in Advanced Robotics},
title = {{Robotics, Vision and Control - Fundamental Algorithms In MATLAB{\textregistered} Second, Completely Revised, Extended And Updated Edition}},
url = {https://www.springer.com/gp/book/9783319544120},
volume = {75},
year = {2017}
}
@article{Shahriari2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Nathan, Andrew J. and Scobell, Andrew},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {00157120},
journal = {Foreign Affairs},
keywords = {decision making,design of experiments,optimi-,response surface methodology,statistical learning,zation},
number = {5},
pages = {1--24},
pmid = {25246403},
title = {{How China sees America}},
url = {http://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf},
volume = {91},
year = {2012}
}
@article{Elseberg2013,
abstract = {Automated 3-dimensional modeling pipelines include 3D scanning, registration, data abstraction, and visualization. All steps in such a pipeline require the processing of a massive amount of 3D data, due to the ability of current 3D scanners to sample environments with a high density. The increasing sampling rates make it easy to acquire Billions of spatial data points. This paper presents algorithms and data structures for handling these data. We propose an efficient octree to store and compress 3D data without loss of precision. We demonstrate its usage for an exchange file format, fast point cloud visualization, sped-up 3D scan matching, and shape detection algorithms. We evaluate our approach using typical terrestrial laser scans. {\textcopyright} 2012 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Elseberg, Jan and Borrmann, Dorit and N{\"{u}}chter, Andreas},
doi = {10.1016/j.isprsjprs.2012.10.004},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Data compression,Frustum culling,Nearest neighbor search,Octree,RANSAC,Ray casting,Tree data structure},
pages = {76--88},
title = {{One billion points in the cloud - An octree for efficient processing of 3D laser scans}},
volume = {76},
year = {2013}
}
@article{Masuya2015,
abstract = {A novel technique of dead reckoning for high-rate feedback control of biped robots is proposed. A fast position estimation of a robot is achieved by fusing information only from internal sensors including joint angle encoders, inertial sensors and force sensors. It combines the kinematics computation and the double integral of acceleration in a complementary way in order to improve the accuracy. The kinematics computation takes the movement of supporting foot, particularly, rotation about a fixed point and rolling on the terrain into consideration. The weights on each information are adjusted automatically based on the reaction force from the ground as it is expected to reflect the certainty of the contact condition of each foot. The validity of the proposed method is verified through computer simulations.},
author = {Masuya, Ken and Sugihara, Tomomichi},
doi = {10.1080/01691864.2015.1011694},
file = {::},
issn = {15685535},
journal = {Advanced Robotics},
keywords = {biped robot,dead reckoning,internal sensor,self-localization},
number = {12},
pages = {785--799},
title = {{Dead reckoning for biped robots that suffers less from foot contact condition based on anchoring pivot estimation}},
url = {http://www.tandfonline.com/doi/full/10.1080/01691864.2015.1011694},
volume = {29},
year = {2015}
}
@phdthesis{Guerrero2011,
author = {Guerrero, Pablo},
keywords = {Electricidad,Probabilidades,Procesos de Gauss,Rob{\'{o}}tica},
pages = {150},
school = {Universidad de Chile},
title = {{Bayesian Handling of Uncertainty For Mobile Robots}},
url = {http://www.tesis.uchile.cl/handle/2250/102531},
year = {2011}
}
@article{Hartley2013,
abstract = {This paper is conceived as a tutorial on rotation averaging, summarizing the research that has been carried out in this area; it discusses methods for single-view and multiple-view rotation averaging, as well as providing proofs of convergence and convexity in many cases. However, at the same time it contains many new results, which were developed to fill gaps in knowledge, answering fundamental questions such as radius of convergence of the algorithms, and existence of local minima. These matters, or even proofs of correctness have in many cases not been considered in the Computer Vision literature. We consider three main problems: single rotation averaging, in which a single rotation is computed starting from several measurements; multiple-rotation averaging, in which absolute orientations are computed from several relative orientation measurements; and conjugate rotation averaging, which relates a pair of coordinate frames. This last is related to the hand-eye coordination problem and to multiple-camera calibration. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Hartley, Richard and Trumpf, Jochen and Dai, Yuchao and Li, Hongdong},
doi = {10.1007/s11263-012-0601-0},
isbn = {0005-1098},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Angular distance,Chordal distance,Geodesic distance,L1 mean,L2 mean,Quaternion distance,conjugate rotation},
number = {3},
pages = {267--305},
title = {{Rotation averaging}},
volume = {103},
year = {2013}
}
@article{Berczi2016,
abstract = {This paper presents a learned, place-dependent terrain-assessment classifier that improves over time. Whereas typical methods aim to assess all of the terrain in a given environment, we exploit the fact that many robotic navigation tasks are well-suited to visual-teach-and-repeat navigation where robot motion is restricted to previously driven paths. In such scenarios, we argue that general terrain assessment is not required, and we can instead solve the much easier problem of detecting changes along the path; we sacrifice the ability to generalize off the path in favour of improved performance on the path. Terrain along a pretaught path is compared to terrain seen at the same location during previous, human-supervised traverses, and any significant differences cause that location to be labelled as unsafe. By storing all of our previous experiences, we are able to continuously improve our estimates as we revisit the same locations multiple times. We tested our method on two datasets collected at the University of Toronto and show that we improve over existing place-independent (both learned and not) methods, enabling nearly full autonomy in challenging, varied terrain using only a stereo camera.},
author = {Berczi, Laszlo Peter and Barfoot, Timothy D.},
doi = {10.1109/IROS.2016.7759585},
file = {:home/matias/Documents/Mendeley Desktop/Berczi, Barfoot/2016/Berczi, Barfoot - 2016 - It ' s Like D ´ ej ` a Vu All Over Again Learning Place-Dependent Terrain Assessment for Visual Teach and R.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3973--3980},
publisher = {IEEE},
title = {{It's like d{\'{e}}j{\`{a}} vu all over again: Learning place-dependent terrain assessment for visual teach and repeat}},
volume = {2016-Novem},
year = {2016}
}
@article{Dame2013,
abstract = {In this paper we propose a new way to achieve a navigation task (visual path following) for a non-holonomic vehicle. We consider an image-based navigation process. We show that it is possible to navigate along a visual path without relying on the extraction, matching and tracking of geometric visual features such as keypoint. The new proposed approach relies directly on the information (entropy) contained in the image signal. We show that it is possible to build a control law directly from the maximization of the shared information between the current image and the next key image in the visual path. The shared information between those two images is obtained using mutual information that is known to be robust to illumination variations and occlusions. Moreover the generally complex task of features extraction and matching is avoided. Both simulations and experiments on a real vehicle are presented and show the possibilities and advantages offered by the proposed method. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Dame, Amaury and Marchand, Eric},
doi = {10.1016/j.robot.2012.11.004},
file = {:home/matias/Documents/Mendeley Desktop/Dame, Marchand/2013/Dame, Marchand - 2013 - Using mutual information for appearance-based visual path following.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Mutual information,Navigation,Visual servoing},
number = {3},
pages = {259--270},
title = {{Using mutual information for appearance-based visual path following}},
volume = {61},
year = {2013}
}
@article{Whelan2012,
abstract = {In this paper we present an extension to the KinectFusion algorithm that permits dense mesh-based mapping of extended scale environments in real-time. This is achieved through (i) altering the original algorithm such that the region of space being mapped by the KinectFusion algorithm can vary dynamically, (ii) extracting a dense point cloud from the regions that leave the KinectFusion volume due to this variation, and, (iii) incrementally adding the resulting points to a triangular mesh representation of the environment. The system is implemented as a set of hierarchical multi-threaded components which are capable of operating in real-time. The architecture facilitates the creation and integration of new modules with minimal impact on the performance on the dense volume tracking and surface reconstruction modules. We provide experimental results demonstrating the system's ability to map areas considerably beyond the scale of the original KinectFusion algorithm including a two story apartment and an extended sequence taken from a car at night. In order to overcome failure of the iterative closest point (ICP) based odometry in areas of low geometric features we have evaluated the Fast Odometry from Vision (FOVIS) system as an alternative. We provide a comparison between the two approaches where we show a trade off between the reduced drift of the visual odometry approach and the higher local mesh quality of the ICP-based approach. Finally we present ongoing work on incorporating full simultaneous localisation and mapping (SLAM) pose-graph optimisation. I.},
author = {Whelan, Thomas and Kaess, Michael and Fallon, Maurice},
journal = {RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras},
keywords = {H. Johannsson,J.J. Leonard and J.B. McDonald,M. Kaess,M.F. Fallon,T. Whelan},
pages = {7},
title = {{Kintinuous: Spatially extended kinectfusion}},
url = {http://18.7.29.232/handle/1721.1/71756},
year = {2012}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {::},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Huang2020a,
abstract = {Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies – ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training – a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective.},
author = {Huang, Wenlong and Mordatch, Igor and Pathak, Deepak},
file = {:home/matias/Documents/Mendeley Desktop/Huang, Mordatch, Pathak/2020/Huang, Mordatch, Pathak - 2020 - One Policy to Control Them All Emergent Centralized Control via Reusable Modular Policies.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
title = {{One Policy to Control Them All : Emergent Centralized Control via Reusable Modular Policies}},
year = {2020}
}
@inproceedings{Glover2012a,
abstract = {Appearance-based loop closure techniques, which leverage the high information content of visual images and can be used independently of pose, are now widely used in robotic applications. The current state-of-the-art in the field is Fast Appearance-Based Mapping (FAB-MAP) having been demonstrated in several seminal robotic mapping experiments. In this paper, we describe OpenFABMAP, a fully open source implementation of the original FAB-MAP algorithm. Beyond the benefits of full user access to the source code, OpenFABMAP provides a number of configurable options including rapid codebook training and interest point feature tuning. We demonstrate the performance of OpenFABMAP on a number of published datasets and demonstrate the advantages of quick algorithm customisation. We present results from OpenFABMAP's application in a highly varied range of robotics research scenarios. {\textcopyright} 2012 IEEE.},
author = {Glover, Arren and Maddern, William and Warren, Michael and Reid, Stephanie and Milford, Michael and Wyeth, Gordon},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224843},
file = {:home/matias/Documents/Mendeley Desktop/Glover et al/2012/Glover et al. - 2012 - OpenFABMAP An open source toolbox for appearance-based loop closure detection.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
pages = {4730--4735},
publisher = {IEEE},
title = {{OpenFABMAP: An open source toolbox for appearance-based loop closure detection}},
year = {2012}
}
@phdthesis{Greenheck2015,
abstract = {Abstract The fast paced development of micro-electromechanical systems (MEMS) technology in recent years has resulted in the availability of low cost gyroscopes and accelerometers in commercial markets. These sensors can be integrated into a single ...},
author = {Greenheck, Daniel R},
booktitle = {Masters, Marquette University},
pages = {113},
school = {Marquette University},
title = {{Design and Characterization of a Low Cost MEMS IMU Cluster for Precision Navigation}},
url = {http://epublications.marquette.edu/theses_open/325%5Cnhttp://epublications.marquette.edu/theses_open/325/},
year = {2015}
}
@inproceedings{Spaenlehauer2017,
abstract = {In monocular vision systems, lack of knowledge about metric distances caused by the inherent scale ambiguity can be a strong limitation for some applications. We offer a method for fusing inertial measurements with monocular odometry or tracking to estimate metric distances in inertial-monocular systems and to increase the rate of pose estimates. As we performed the fusion in a loosely-coupled manner, each input block can be easily replaced with one's preference, which makes our method quite flexible. We experimented our method using the ORB-SLAM algorithm for the monocular tracking input and Euler forward integration to process the inertial measurements. We chose sets of data recorded on UAVs to design a suitable system for flying robots.},
archivePrefix = {arXiv},
arxivId = {1707.07518},
author = {Spaenlehauer, Ariane and Fremont, Vincent and Sekercioglu, Y. Ahmet and Fantoni, Isabelle},
booktitle = {IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
doi = {10.1109/MFI.2017.8170419},
eprint = {1707.07518},
isbn = {9781509060641},
month = {nov},
pages = {137--143},
publisher = {IEEE},
title = {{A loosely-coupled approach for metric scale estimation in monocular vision-inertial systems}},
url = {http://arxiv.org/abs/1707.07518 http://ieeexplore.ieee.org/document/8170419/},
volume = {2017-Novem},
year = {2017}
}
@article{Lim2015,
abstract = {In this paper we present a new real-time image-based localization method for scenes that have been reconstructed offline using structure from motion. From input video, our method continuously computes six-degree-of-freedom camera pose estimates by efficiently tracking natural features and matching them to 3D points reconstructed by structure from motion. Our main contribution lies in efficiently interleaving a fast keypoint tracker that uses inexpensive binary feature descriptors with a new approach for direct 2D-to-3D matching. Our 2D-to-3D matching scheme avoids the need for online extraction of scale-invariant features. Instead, offline we construct an indexed database containing multiple DAISY descriptors per 3D point extracted at multiple scales. The key to the efficiency of our method is invoking DAISY descriptor extraction and matching sparingly during localization, and in distributing this computation over a temporal window of successive frames. This enables the system to run in real-time and achieve low per-frame latency over long durations. Our algorithm runs at over 30 Hz on a laptop and at 12 Hz on a low-power computer suitable for onboard computation on a mobile robot such as a micro-aerial vehicle. We have evaluated our method using ground truth and present results on several challenging indoor and outdoor sequences.},
author = {Lim, Hyon and Sinha, Sudipta N. and Cohen, Michael F. and Uyttendaele, Matt and Kim, H. Jin},
doi = {10.1177/0278364914561101},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {2D-to-3D feature matching,Image-based localization,pose estimation,structure from motion},
number = {4-5},
pages = {476--492},
title = {{Real-time monocular image-based 6-DoF localization}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364914561101},
volume = {34},
year = {2015}
}
@article{Czarnowski2020,
abstract = {The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.},
archivePrefix = {arXiv},
arxivId = {2001.05049},
author = {Czarnowski, Jan and Laidlow, Tristan and Clark, Ronald and Davison, Andrew J.},
doi = {10.1109/LRA.2020.2965415},
eprint = {2001.05049},
file = {:home/matias/Documents/Mendeley Desktop/Czarnowski et al/2020/Czarnowski et al. - 2020 - DeepFactors Real-Time Probabilistic Dense Monocular SLAM.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,SLAM,mapping},
number = {2},
pages = {721--728},
title = {{DeepFactors: Real-Time Probabilistic Dense Monocular SLAM}},
volume = {5},
year = {2020}
}
@article{Faessler2015a,
abstract = {The use of mobile robots in search-and-rescue and disaster-response missions has increased significantly in recent years. However, they are still remotely controlled by expert professionals on an actuator set-point level, and they would benefit, therefore, from any bit of autonomy added. This would allow them to execute high-level commands, such as "execute this trajectory" or "map this area." In this paper, we describe a vision-based quadrotor micro aerial vehicle that can autonomously execute a given trajectory and provide a live, dense three-dimensional (3D) map of an area. This map is presented to the operator while the quadrotor is mapping, so that there are no unnecessary delays in the mission. Our system does not rely on any external positioning system (e.g., GPS or motion capture systems) as sensing, computation, and control are performed fully onboard a smartphone processor. Since we use standard, off-the-shelf components from the hobbyist and smartphone markets, the total cost of our system is very low. Due to its low weight (below 450 g), it is also passively safe and can be deployed close to humans. We describe both the hardware and the software architecture of our system. We detail our visual odometry pipeline, the state estimation and control, and our live dense 3D mapping, with an overview of how all the modules work and how they have been integrated into the final system. We report the results of our experiments both indoors and outdoors. Our quadrotor was demonstrated over 100 times at multiple trade fairs, at public events, and to rescue professionals. We discuss the practical challenges and lessons learned. Code, datasets, and videos are publicly available to the robotics community.},
author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Mueggler, Elias and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1002/rob.21581},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {4},
pages = {431--450},
title = {{Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle}},
url = {http://doi.wiley.com/10.1002/rob.21581},
volume = {33},
year = {2016}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms-for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several "visual Turing tests" probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {:home/matias/Documents/Mendeley Desktop/Lake, Salakhutdinov, Tenenbaum/2015/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
isbn = {0036-8075},
issn = {10959203},
journal = {Science},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
volume = {350},
year = {2015}
}
@article{Miller2020,
abstract = {Robotic exploration of underground environments is a particularly challenging problem due to communication, endurance, and traversability constraints which necessitate high degrees of autonomy and agility. These challenges are further exacerbated by the need to minimize human intervention for practical applications. While legged robots have the ability to traverse extremely challenging terrain, they also engender new challenges for planning, estimation, and control. In this work, we describe a fully autonomous system for multi-robot mine exploration and mapping using legged quadrupeds, as well as a distributed database mesh networking system for reporting data. In addition, we show results from DARPA Subterranean Challenge (SubT) Tunnel Circuit demonstrating localization of artifacts after traversals of hundreds of meters. These experiments describe fully autonomous exploration of an unknown Global Navigation Satellite System (GNSS)-denied environment undertaken by legged robots.},
archivePrefix = {arXiv},
arxivId = {1909.09662},
author = {Miller, Ian D. and Cohen, Avraham and Kulkarni, Adarsh and Laney, James and Taylor, Camillo Jose and Kumar, Vijay and Cladera, Fernando and Cowley, Anthony and Shivakumar, Shreyas S. and Lee, Elijah S. and Jarin-Lipschitz, Laura and Bhat, Akhilesh and Rodrigues, Neil and Zhou, Alex},
doi = {10.1109/LRA.2020.2972872},
eprint = {1909.09662},
file = {:home/matias/Documents/Mendeley Desktop/Miller et al/2020/Miller et al. - 2020 - Mine Tunnel Exploration Using Multiple Quadrupedal Robots.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mining robotics,field robots,legged robots},
number = {2},
pages = {2840--2847},
title = {{Mine Tunnel Exploration Using Multiple Quadrupedal Robots}},
volume = {5},
year = {2020}
}
@book{Sutton2017,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S and Barto, Andrew G},
eprint = {1603.02199},
file = {:home/matias/Documents/Mendeley Desktop/Sutton, Barto/2017/Sutton, Barto - 2017 - Reinforcement Learning An Introduction(2).pdf:pdf},
isbn = {0262193981},
issn = {01406736},
pmid = {17044734},
publisher = {MIT Press},
title = {{Reinforcement Learning: An Introduction}},
year = {2017}
}
@techreport{Blanco2014,
abstract = {An arbitrary rigid transformation in SE(3) can be separated into two parts, namely, a translation and a rigid rotation. This technical report reviews, under a unifying viewpoint, three common alternatives to representing the rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal rotation matrices from SO(3) and quaternions. It will be described: (i) the equivalence between these representations and the formulas for transforming one to each other (in all cases considering the translational and rotational parts as a whole), (ii) how to compose poses with poses and poses with points in each representation and (iii) how the uncertainty of the poses (when modeled as Gaussian distributions) is affected by these transformations and compositions. Some brief notes are also given about the Jacobians required to implement least-squares optimization on manifolds, an very promising approach in recent engineering literature. The text reflects which MRPT C++ library 1 functions implement each of the described algorithms. All the implementations have been thoroughly validated by means of unit testing and numerical estimation of the Jacobians.},
author = {Blanco, Jl},
booktitle = {University of Malaga, Tech. Rep},
file = {::},
institution = {University of Malaga},
keywords = {()},
number = {3},
title = {{A tutorial on se (3) transformation parameterizations and on-manifold optimization}},
url = {http://mapir.isa.uma.es/$\sim$jlblanco/papers/jlblanco2010geometry3D_techrep.pdf},
year = {2010}
}
@techreport{Dellaert2016,
abstract = {Mathematical theory used in GTSAM repository},
author = {Dellaert, Frank},
institution = {Georgia Institute of Technology},
pages = {1--24},
title = {{Derivatives and Differentials Theory}},
url = {https://github.com/borglab/gtsam/blob/develop/doc/math.pdf},
year = {2013}
}
@article{MacTavish2015,
abstract = {Camera-based localization techniques must be robust to correspondence errors, i.e., when visual features (landmarks)are matched incorrectly. The two primary techniques to address this issue are RANSAC and robust M-estimation-each more appropriate for different applications. This paper investigates the use of different robust cost functions for M-estimation to deal with correspondence outliers, and assesses their performance under varying degrees of data corruption. Experimental results show that using an aggressive red ascending cost function (e.g., Dynamic Covariance Scaling (DCS) or Geman-McClure (G-M)) best improves accuracy by excluding outliers almost entirely. Additionally, adjusting an error-scaling parameter for the robust cost function over the course of the optimization improves convergence with poor initial conditions.},
author = {MacTavish, Kirk and Barfoot, Timothy D.},
doi = {10.1109/CRV.2015.52},
file = {:home/matias/Documents/Mendeley Desktop/MacTavish, Barfoot/2015/MacTavish, Barfoot - 2015 - At all Costs A Comparison of Robust Cost Functions for Camera Correspondence Outliers.pdf:pdf},
isbn = {9781479919864},
journal = {Proceedings -2015 12th Conference on Computer and Robot Vision, CRV 2015},
keywords = {Features,Outlier Rejection,Visual Odometry},
pages = {62--69},
title = {{At all Costs: A Comparison of Robust Cost Functions for Camera Correspondence Outliers}},
year = {2015}
}
@article{MacTavish2018,
abstract = {Visual navigation is a key enabling technology for autonomous mobile vehicles. The ability to provide large-scale, long-term navigation using low-cost, low-power vision sensors is appealing for industrial applications. A crucial requirement for long-term navigation systems is the ability to localize in environments whose appearance is constantly changing over time—due to lighting, weather, seasons, and physical changes. This paper presents a multiexperience localization (MEL) system that uses a powerful map representation—storing every visual experience in layers—that does not make assumptions about underlying appearance modalities and generators. Our localization system provides real-time performance by selecting online, a subset of experiences against which to localize. We achieve this task through a novel experience-triage algorithm based on collaborative filtering, which selects experiences relevant to the live view, outperforming competing techniques. Based on classical memory-based recommender systems, this technique also enables landmark-level recommendations, is entirely online, and requires no training data. We demonstrate the capabilities of the MEL system in the context of long-term autonomous path following in unstructured outdoor environments with a challenging 100-day field experiment through day, night, snow, spring, and summer. We furthermore provide offline analysis comparing our system to several state-of-the-art alternatives. We show that the combination of the novel methods presented in this paper enable full use of incredibly rich multiexperience maps, opening the door to robust long-term visual localization.},
author = {MacTavish, Kirk and Paton, Michael and Barfoot, Timothy D.},
doi = {10.1002/rob.21838},
file = {:home/matias/Documents/Mendeley Desktop/MacTavish, Paton, Barfoot/2018/MacTavish, Paton, Barfoot - 2018 - Selective memory Recalling relevant experience for long-term visual localization.pdf:pdf},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {mapping,position estimation,terrestrial robotics},
number = {8},
pages = {1265--1292},
title = {{Selective memory: Recalling relevant experience for long-term visual localization}},
volume = {35},
year = {2018}
}
@article{Burri2016b,
abstract = {This paper presents visual-inertial datasets collected on-board a micro aerial vehicle. The datasets contain synchronized stereo images, IMU measurements and accurate ground truth. The first batch of datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data. It was collected in an industrial environment and contains millimeter accurate position ground truth from a laser tracking system. The second batch of datasets is aimed at precise 3D environment reconstruction and was recorded in a room equipped with a motion capture system. The datasets contain 6D pose ground truth and a detailed 3D scan of the environment. Eleven datasets are provided in total, ranging from slow flights under good visual conditions to dynamic flights with motion blur and poor illumination, enabling researchers to thoroughly test and evaluate their algorithms. All datasets contain raw sensor measurements, spatio-temporally aligned sensor data and ground truth, extrinsic and intrinsic calibrations and datasets for custom calibrations.},
author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W. and Siegwart, Roland},
doi = {10.1177/0278364915620033},
file = {:home/matias/Documents/Mendeley Desktop/Burri et al/2016/Burri et al. - 2016 - The EuRoC micro aerial vehicle datasets.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Dataset,MAV,ground truth,visual-inertial},
number = {10},
pages = {1157--1163},
title = {{The EuRoC micro aerial vehicle datasets}},
volume = {35},
year = {2016}
}
@article{Ponce2010,
abstract = {This paper addresses the problem of estimating the epipolar geometry from point correspondences between two images taken by uncalibrated perspective cameras. It is shown that Jepson's and Heeger's linear subspace technique for infinitesimal motion estimation can be generalized to the finite motion case by choosing an appropriate basis for projective space. This yields a linear method for weak calibration. The proposed algorithm has been implemented and tested on both real and synthetic images, and it is compared to other linear and non-linear approaches to weak calibration.},
author = {Ponce, Jean and Genc, Yakup},
doi = {10.1109/cvpr.1996.517160},
isbn = {0-8186-7258-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {776--781},
title = {{Epipolar geometry and linear subspace methods: a new approach to weak calibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=517160},
year = {1996}
}
@article{Merwe2004,
abstract = {Core to integrated navigation systems is the concept of fusing noisy observations from GPS, Inertial Measurement Units (IMU), and other available sensors. The current industry standard and most widely used algorithm for this purpose is the extended Kalman filter (EKF) [6]. The EKF combines the sensor measurements with predictions coming from a model of vehicle motion (either dynamic or kinematic), in order to generate an estimate of the current navigational state (position, velocity, and attitude). This paper points out the inherent shortcomings in using the EKF and presents, as an alternative, a family of improved derivativeless nonlinear Kalman filters called sigma-point Kalman filters (SPKF). We demonstrate the improved state estimation performance of the SPKF by applying it to the problem of loosely coupled GPS/INS integration. A novel method to account for latency in the GPS updates is also developed for the SPKF (such latency compensation is typically inaccurate or not practical with the EKF). A UAV (rotor-craft) test platform is used to demonstrate the results. Performance metrics indicate an approximate 30% error reduction in both attitude and position estimates relative to the baseline EKF implementation.},
author = {{Van Der Merwe}, Rudolph and Wan, Eric A.},
doi = {10.1.1.9.5753},
journal = {Proceedings of the Annual Meeting - Institute of Navigation},
pages = {641--654},
title = {{Sigma-point Kalman filters for integrated navigation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.5753&rep=rep1&type=pdf},
year = {2004}
}
@article{Khan2019,
abstract = {Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. As a result, we can obtain a GP kernel and a nonlinear feature map simply by training the DNN. Surprisingly, the resulting kernel is the neural tangent kernel which has desirable theoretical properties for infinitely-wide DNNs. We show feature maps obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.},
archivePrefix = {arXiv},
arxivId = {1906.01930},
author = {Khan, Mohammad Emtiyaz and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
eprint = {1906.01930},
file = {:home/matias/Documents/Mendeley Desktop/Khan et al/2019/Khan et al. - 2019 - Approximate inference turns deep networks into Gaussian processes.pdf:pdf},
journal = {arXiv},
number = {VI},
title = {{Approximate inference turns deep networks into Gaussian processes}},
year = {2019}
}
@article{Campusano2017,
abstract = {Typically, development of robot behavior entails writing the code, deploying it on a simulator or robot and running it in a test setting. If this feedback reveals errors, the programmer mentally needs to map the error in behavior back to the source code that caused it before being able to fix it. This process suffers from a long cognitive distance between the code and the resulting behavior, which slows down development and can make experimentation with different behaviors prohibitively expensive. In contrast, Live Programming tightens the feedback loop, minimizing the cognitive distance. As a result, programmers benefit from an immediate connection with the program that they are making thanks to an immediate, ‘live' feedback on program behavior. This allows for extremely rapid creation, or variation, of robot behavior and for dramatically increased debugging speed. To enable such Live Robot Programming, in this article we discuss LRP; our language that provides for live programming of nested state machines. We detail the design of the language and show its features, give an overview of the interpreter and how it enables the liveness properties of the language, and illustrate its independence from specific robot APIs.},
author = {Campusano, Miguel and Fabry, Johan},
doi = {10.1016/j.scico.2016.06.002},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Live Programming,Nested state machines,Robot},
month = {jan},
pages = {1--19},
title = {{Live Robot Programming: The language, its implementation, and robot API independence}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167642316300697},
volume = {133},
year = {2017}
}
@article{Whelan2016,
abstract = {We present a novel approach to real-time dense visual simultaneous localisation and mapping. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution, while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables (a) more realistic augmented reality rendering; (b) a richer understanding of the scene beyond pure geometry and; (c) more accurate and robust photometric tracking.},
author = {Whelan, Thomas and Salas-Moreno, Renato F. and Glocker, Ben and Davison, Andrew J. and Leutenegger, Stefan},
doi = {10.1177/0278364916669237},
file = {::},
isbn = {0278-3649},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {GPU,RGB-D,SLAM,Surfel fusion,camera pose estimation,dense methods,large scale,light sources,real-time,reflections,specular},
number = {14},
pages = {1697--1716},
title = {{ElasticFusion: Real-time dense SLAM and light source estimation}},
volume = {35},
year = {2016}
}
@article{Kim2008,
abstract = {An imaging sensor made of multiple light-weight non-overlapping cameras is an effective sensor for a small unmanned aerial vehicle that has strong payload limitation. This paper presents a method for motion estimation by assuming that such a multi-camera system is a spherical imaging system (that is, the cameras share a single optical center). We derive analytically and empirically a condition for a multi-camera system to be modeled as a spherical camera. Interestingly, not only does the spherical assumption simplify the algorithms and calibration procedure, but also motion estimation based on that assumption becomes more accurate. {\textcopyright}2008 IEEE.},
author = {Kim, Jun Sik and Hwangbo, Myung and Kanade, Takeo},
doi = {10.1109/ROBOT.2008.4543678},
file = {:home/matias/Documents/Mendeley Desktop/Kim, Hwangbo, Kanade/2008/Kim, Hwangbo, Kanade - 2008 - Motion estimation using multiple non-overlapping cameras for small unmanned aerial vehicles.pdf:pdf},
isbn = {9781424416479},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Aerial Robotics,Computer Vision,Motion and Path Planning},
pages = {3076--3081},
title = {{Motion estimation using multiple non-overlapping cameras for small unmanned aerial vehicles}},
year = {2008}
}
@article{Cummins2011,
abstract = {We describe a new formulation of appearance-only SLAM suitable for very large scale place recognition. The system navigates in the space of appearance, assigning each new observation to either a new or a previously visited location, without reference to metric position. The system is demonstrated performing reliable online appearance mapping and loop-closure detection over a 1000 km trajectory, with mean filter update times of 14 ms. The scalability of the system is achieved by defining a sparse approximation to the FAB-MAP model suitable for implementation using an inverted index. Our formulation of the problem is fully probabilistic and naturally incorporates robustness against perceptual aliasing. We also demonstrate that the approach substantially outperforms the standard term-frequency inverse-document-frequency (tf-idf) ranking measure. The 1000 km data set comprising almost a terabyte of omni-directional and stereo imagery is available for use, and we hope that it will serve as a benchmark for future systems. {\textcopyright} The Author(s) 2011.},
author = {Cummins, Mark and Newman, Paul},
doi = {10.1177/0278364910385483},
file = {:home/matias/Documents/Mendeley Desktop/Cummins, Newman/2011/Cummins, Newman - 2011 - Appearance-only SLAM at large scale with FAB-MAP 2.0.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {FAB-MAP,Robotics,SLAM,appearance-based navigation},
number = {9},
pages = {1100--1123},
title = {{Appearance-only SLAM at large scale with FAB-MAP 2.0}},
volume = {30},
year = {2011}
}
@inproceedings{Lim2014,
abstract = {Real-time approach for monocular visual simultaneous localization and mapping (SLAM) within a large-scale environment is proposed. From a monocular video sequence, the proposed method continuously computes the current 6-DOF camera pose and 3D landmarks position. The proposed method successfully builds consistent maps from challenging outdoor sequences using a monocular camera as the only sensor, while existing approaches have utilized additional structural information such as camera height from the ground. By using a binary descriptor and metric-topological mapping, the system demonstrates real-time performance on a large-scale outdoor environment without utilizing GPUs or reducing input image size. The effectiveness of the proposed method is demonstrated on various challenging video sequences including the KITTI dataset and indoor video captured on a micro aerial vehicle.},
author = {Lim, Hyon and Lim, Jongwoo and Kim, H. Jin},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6907055},
isbn = {978-1-4799-3685-4},
issn = {10504729},
month = {may},
pages = {1532--1539},
publisher = {IEEE},
title = {{Real-time 6-DOF monocular visual SLAM in a large-scale environment}},
url = {http://ieeexplore.ieee.org/document/6907055/},
year = {2014}
}
@article{Howard2004,
abstract = {This paper introduces a new method for representing two-dimensional maps, and shows how this representation may be applied to concurrent localization and mapping problems involving multiple robots. We introduce the notion of a manifold map; this representation takes maps out of the plane and onto a two-dimensional surface embedded in a higher-dimensional space. Compared with standard planar maps, the key advantage of the manifold representation is self-consistency: manifold maps do not suffer from the 'cross over' problem that planar maps commonly exhibit in environments containing loops. This self-consistency facilitates a number of important autonomous capabilities, including robust retro-traverse, lazy loop closure, active loop closure using robot rendezvous, and, ultimately, autonomous exploration. By way of validation, this paper also includes experimental results obtained using teams of two to four robots in environments ranging in size from 400 m2 to 900 m 2.},
author = {Howard, Andrew},
doi = {10.1109/robot.2004.1308933},
file = {:home/matias/Documents/Mendeley Desktop/Howard/2004/Howard - 2004 - Multi-robot mapping using manifold representations.pdf:pdf},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {4},
pages = {4198--4203},
title = {{Multi-robot mapping using manifold representations}},
volume = {2004},
year = {2004}
}
@article{Loncomilla2012,
abstract = {In current visual SLAM methods, point-like landmarks (As in Filliat and Meyer (Cogn Syst Res 4(4):243-282, 2003), we use this expression to denote a landmark generated by a point or an object considered as punctual.) are used for representation on maps. As the observation of each point-like landmark gives only angular information about a bearing camera, a covariance matrix between point-like landmarks must be estimated in order to converge with a global scale estimation. However, as the computational complexity of covariance matrices scales in a quadratic way with the number of landmarks, the maximum number of landmarks that is possible to use is normally limited to a few hundred. In this paper, a visual SLAM system based on the use of what are called rigid-body 3D landmarks is proposed. A rigid-body 3D landmark represents the 6D pose of a rigid body in space (position and orientation), and its observation gives full-pose information about a bearing camera. Each rigid-body 3D landmark is created from a set of N point-like landmarks by collapsing 3N state components into seven state components plus a set of parameters that describe the shape of the landmark. Rigid-body 3D landmarks are represented and estimated using so-called point-quaternions, which are introduced here. By using rigid-body 3D landmarks, the computational time of an EKF-SLAM system can be reduced up to 5.5%, as the number of landmarks increases. The proposed visual SLAM system is validated in simulated and real video sequences (outdoor). The proposed methodology can be extended to any SLAM system based on the use of point-like landmarks, including those generated by laser measurement. {\textcopyright} 2011 Springer Science+Business Media B.V.},
author = {Loncomilla, Patricio and {Del Solar}, Javier Ruiz},
doi = {10.1007/s10846-011-9601-5},
file = {::},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {3D Mapping,6D SLAM,Localization,Model reduction,MonoSLAM,Robotics,SLAM,Visual SLAM},
number = {1-2},
pages = {125--149},
title = {{Visual SLAM based on rigid-body 3D landmarks}},
volume = {66},
year = {2012}
}
@article{Shade2011,
abstract = {This paper is about the autonomous acquisiti of detailed 3D maps of a-priori unknown environments using stereo camera - it is about choosing where to go. Our approa hinges upon a boundary value constrained partial differenti equation (PDE) - the solution of which provides a scalar fie guaranteed to have no local minima. This scalar field is trivial transformed into a vector field in which following lines of m flow causes provably complete exploration of the environme in full 6 degrees of freedom (6-DOF). We use a SLAM syste to infer the position of a stereo pair in real time and fus stereo depth maps to generate the boundary conditions whi drive exploration. Our exploration algorithm is parameter fr is as applicable to 3D laser data as it is to stereo, is real tim and is guaranteed to deliver complete exploration. We sho empirically that it performs better than oft-used frontier bas approaches and demonstrate our system working with real an simulated data. {\textcopyright} 2011 IEEE.},
author = {Shade, Robbie and Newman, Paul},
doi = {10.1109/ICRA.2011.5980121},
file = {:home/matias/Documents/Mendeley Desktop/Shade, Newman/2011/Shade, Newman - 2011 - Choosing where to go Complete 3D exploration with Stereo.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2806--2811},
publisher = {IEEE},
title = {{Choosing where to go: Complete 3D exploration with Stereo}},
year = {2011}
}
@phdthesis{Xinjilefu2014a,
author = {Xinjilefu, X},
pages = {152},
school = {Carnegie Mellon University},
title = {{State Estimation for Humanoid Robots}},
year = {2015}
}
@article{Strasdat2012a,
abstract = {While the most accurate solution to off-line structure from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform batch optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM - also called visual SLAM (simultaneous localisation and mapping) - have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform a rigorous analysis of the relative advantages of filtering and sparse bundle adjustment for sequential visual SLAM. In a series of Monte Carlo experiments we investigate the accuracy and cost of visual SLAM. We measure accuracy in terms of entropy reduction as well as root mean square error (RMSE), and analyse the efficiency of bundle adjustment versus filtering using combined cost/accuracy measures. In our analysis, we consider both SLAM using a stereo rig and monocular SLAM as well as various different scenes and motion patterns. For all these scenarios, we conclude that keyframe bundle adjustment outperforms filtering, since it gives the most accuracy per unit of computing time. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Strasdat, Hauke and Montiel, J. M.M. and Davison, Andrew J.},
doi = {10.1016/j.imavis.2012.02.009},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Bundle adjustment,EKF,Information filter,Monocular vision,SLAM,Stereo vision,Structure from motion},
number = {2},
pages = {65--77},
pmid = {5509636},
title = {{Visual SLAM: Why filter?}},
volume = {30},
year = {2012}
}
@article{Carpino2012,
abstract = {The introduction of intrinsic compliance in the actuation system of assistive robots improves safety and dynamical adaptability. Furthermore, in the case of wearable robots for gait assistance, the exploitation of conservative compliant elements as energy buffers can mimic the intrinsic dynamical properties of legs during locomotion. However, commercially available compliant components do not generally allow to meet the desired requirements in terms of admissible peak load, as typically required by gait assistance, while guaranteeing low stiffness and a compact and lightweight design. This paper presents a novel compact monolithic torsional spring to be used as the basic component of a modular compliant system for series elastic actuators. The spring, whose design was refined through an iterative FEA-based optimization process, has an external diameter of 85 mm, a thickness of 3 mm and a weight of 61.5 g. The spring, characterized using a custom dynamometric test bed, shows a linear torque versus angle characteristic. The compliant element has a stiffness of 98 N m/rad and it is capable of withstanding a maximum torque of 7. 68 N m. A good agreement between simulated and experimental data were observed, with a maximum resultant error of 6. By arranging a number of identical springs in series or in parallel, it is possible to render different torque versus angle characteristics, in order to match the specific applications requirements. {\textcopyright} 2012 American Society of Mechanical Engineers.},
archivePrefix = {arXiv},
arxivId = {5},
author = {Carpino, Giorgio and Accoto, Dino and Sergi, Fabrizio and Tagliamonte, Nevio Luigi and Guglielmelli, Eugenio},
doi = {10.1115/1.4007695},
eprint = {5},
file = {::},
isbn = {0-7803-7729-X},
issn = {10500472},
journal = {Journal of Mechanical Design, Transactions of the ASME},
keywords = {pHRI,series elastic actuator,torsional spring,wearable robot},
number = {12},
pages = {121002},
pmid = {525827},
title = {{A novel compact torsional spring for series elastic actuators for assistive wearable robots}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4007695},
volume = {134},
year = {2012}
}
@article{Mazuran2015,
abstract = {In the last years, many researchers started to consider teach-and-repeat approaches for reliable autonomous navigation. The paradigm, in all its proposed forms, is deeply rooted in the idea that the robot should autonomously follow a route that has been demonstrated by a human during a teach phase. However, human demonstrations are often inefficient in terms of execution time or may cause premature wear of the robot components due to jittery behavior or strong accelerations. In this paper, we propose the concept of teach, optimize and repeat, which introduces a trajectory optimization step between the teach and repeat phases. To address this problem, we further propose LexTOR, a constrained trajectory optimization method for teach and repeat problems, where the constraints are defined according to user preferences. At its core, LexTOR optimizes both the execution time and the trajectory smoothness in a lexicographic sense. The experiments show that LexTOR is very effective, both qualitatively and quantitatively, in terms of execution time, smoothness, accuracy and bound satisfaction.},
author = {Mazuran, Mladen and Sprunk, Christoph and Burgard, Wolfram and Tipaldi, Gian Diego},
doi = {10.1109/ICRA.2015.7139577},
file = {:home/matias/Documents/Mendeley Desktop/Mazuran et al/2015/Mazuran et al. - 2015 - LexTOR Lexicographic teach optimize and repeat based on user preferences.pdf:pdf},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {2780--2786},
title = {{LexTOR: Lexicographic teach optimize and repeat based on user preferences}},
volume = {2015-June},
year = {2015}
}
@article{Mukadam2018,
abstract = {We introduce a novel formulation of motion planning, for continuous-time trajectories, as probabilistic inference. We first show how smooth continuous-time trajectories can be represented by a small number of states using sparse Gaussian process (GP) models. We next develop an efficient gradient-based optimization algorithm that exploits this sparsity and GP interpolation. We call this algorithm the Gaussian Process Motion Planner (GPMP). We then detail how motion planning problems can be formulated as probabilistic inference on a factor graph. This forms the basis for GPMP2, a very efficient algorithm that combines GP representations of trajectories with fast, structure-exploiting inference via numerical optimization. Finally, we extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan when conditions change. We benchmark our algorithms against several sampling-based and trajectory optimization-based motion planning algorithms on planning problems in multiple environments. Our evaluation reveals that GPMP2 is several times faster than previous algorithms while retaining robustness. We also benchmark iGPMP2 on replanning problems, and show that it can find successful solutions in a fraction of the time required by GPMP2 to replan from scratch.},
archivePrefix = {arXiv},
arxivId = {1707.07383},
author = {Mukadam, Mustafa and Dong, Jing and Yan, Xinyan and Dellaert, Frank and Boots, Byron},
doi = {10.1177/0278364918790369},
eprint = {1707.07383},
file = {:home/matias/Documents/Mendeley Desktop/Mukadam et al/2018/Mukadam et al. - 2018 - Continuous-time Gaussian process motion planning via probabilistic inference.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Gaussian processes,Motion planning,factor graphs,probabilistic inference,trajectory optimization},
number = {11},
pages = {1319--1340},
title = {{Continuous-time Gaussian process motion planning via probabilistic inference}},
volume = {37},
year = {2018}
}
@article{Botvinick2020,
abstract = {The emergence of powerful artificial intelligence (AI) is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning in tasks such as image classification. However, there is another area of recent AI work that has so far received less attention from neuroscientists but that may have profound neuroscientific implications: deep reinforcement learning (RL). Deep RL offers a comprehensive framework for studying the interplay among learning, representation, and decision making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research. Recent advances in artificial intelligence have unified the fields of reinforcement learning and deep learning. The result, deep reinforcement learning, has far-reaching implications for neuroscience. Botvinick et al. introduce deep reinforcement learning and identify diverse opportunities it creates for brain research.},
archivePrefix = {arXiv},
arxivId = {2007.03750},
author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and Kurth-Nelson, Zeb},
doi = {10.1016/j.neuron.2020.06.014},
eprint = {2007.03750},
file = {:home/matias/Documents/Mendeley Desktop/Botvinick et al/2020/Botvinick et al. - 2020 - Deep Reinforcement Learning and Its Neuroscientific Implications.pdf:pdf},
issn = {10974199},
journal = {Neuron},
pages = {1--22},
pmid = {32663439},
title = {{Deep Reinforcement Learning and Its Neuroscientific Implications}},
url = {http://arxiv.org/abs/2007.03750},
year = {2020}
}
@article{Tom,
abstract = {This article extends previous work, presenting a novel polyurethane based compliant spring system designed to be attached to a conventional robotics servo motor, turning it into a series elastic actuator (SEA). The new system is composed by only two mechanical parts: a torsional polyurethane spring and a round aluminum support for link attachment. The polyurethane spring, had its design derived from a iterative FEM-based optimization process. We present also some system identification and practical results using a PID controller for robust position holding.},
author = {Martins, Leandro Tom{\'{e}} and Tatsch, Christopher and Maciel, Eduardo Henrique and {Bayan Henriques}, Renato Ventura and Gerndt, Reinhard and da Guerra, Rodrigo Silva},
doi = {10.1007/978-3-319-29339-4_29},
file = {::},
isbn = {9783319293387},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Passive compliance,Series elastic actuator},
number = {1},
pages = {347--355},
title = {{Polyurethane-based modular series elastic upgrade to a robotics actuator}},
volume = {9513},
year = {2015}
}
@article{Shih1996,
author = {Shih, SW and Hung, YP and Lin, WS},
pages = {1--48},
title = {{Error analysis on closed-form solutions for kinematic calibration}},
url = {http://www.researchgate.net/publication/2434042_Error_Analysis_on_Closed-Form_Solutions_for_Kinematic_Calibration/file/32bfe510bf43eb3a6f.pdf},
year = {1996}
}
@inproceedings{Elseberg2012,
abstract = {The terrestrial acquisition of 3D point clouds by laser range finders has recently moved to mobile platforms. Measuring the environment while simultaneously moving the vehicle demands a high level of accuracy from positioning systems such as the IMU, GPS and odometry. We present a novel semi-rigid SLAM algorithm that corrects the global position of the vehicle at every point in time, while simultaneously improving the quality and accuracy of the entire acquired map. Using the algorithm the temporary failure of positioning systems or the lack thereof can be compensated for. We demonstrate the capabilities of our approach on a wide variety of systems and data sets. {\textcopyright} 2012 IEEE.},
author = {Elseberg, Jan and Borrmann, Dorit and Nuchter, Andreas},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385509},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {1865--1870},
publisher = {IEEE},
title = {{6DOF semi-rigid SLAM for mobile scanning}},
url = {http://ieeexplore.ieee.org/document/6385509/},
year = {2012}
}
@inproceedings{Leutenegger2011,
abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the SIFT and SURF algorithms which exhibit great performance under a variety of image transformations, with SURF in particular considered as the most computationally efficient amongst the high-performance methods to date. In this paper we propose BRISK 1, a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood. {\textcopyright} 2011 IEEE.},
author = {Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland Y.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126542},
isbn = {9781457711015},
month = {nov},
pages = {2548--2555},
publisher = {IEEE},
title = {{BRISK: Binary Robust invariant scalable keypoints}},
url = {http://ieeexplore.ieee.org/document/6126542/},
year = {2011}
}
@article{Hassabis2017,
abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields. Hassabis et al. review how neuroscience has informed research in artificial intelligence. They argue that a better understanding of biological brains will play a vital role in building intelligent machines.},
archivePrefix = {arXiv},
arxivId = {1404.7282},
author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
doi = {10.1016/j.neuron.2017.06.011},
eprint = {1404.7282},
file = {::},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
keywords = {artificial intelligence,brain,cognition,learning,neural network},
number = {2},
pages = {245--258},
pmid = {28728020},
publisher = {Elsevier Inc.},
title = {{Neuroscience-Inspired Artificial Intelligence}},
url = {http://dx.doi.org/10.1016/j.neuron.2017.06.011},
volume = {95},
year = {2017}
}
@article{Ling2016,
abstract = {In this work, we address the problem of aggressive flight of a quadrotor aerial vehicle using cameras and IMUs as the only sensing modalities. We present a fully integrated quadrotor system and demonstrate through online experiment the capability of autonomous flight with linear velocities up to 4.2 m/s, linear accelerations up to 9.6 m/s2, and angular velocities up to 245.1 degree/s. Central to our approach is a dense visual-inertial state estimator for reliable tracking of aggressive motions. An uncertainty-aware direct dense visual tracking module provides camera pose tracking that takes inverse depth uncertainty into account and is resistant to motion blur. Measurements from IMU pre-integration and multi-constrained dense visual tracking are fused probabilistically using an optimization-based sensor fusion framework. Extensive statistical analysis and comparison are presented to verify the performance of the proposed approach. We also release our code as open-source ROS packages.},
author = {Ling, Yonggen and Liu, Tianbo and Shen, Shaojie},
doi = {10.1109/ICRA.2016.7487286},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1499--1506},
title = {{Aggressive quadrotor flight using dense visual-inertial fusion}},
volume = {2016-June},
year = {2016}
}
@article{Peretroukhin2018,
abstract = {We present a novel method to fuse the power of deep networks with the computational efficiency of geometric and probabilistic localization algorithms. In contrast to other methods that completely replace a classical visual estimator with a deep network, we propose an approach that uses a convolutional neural network to learn difficult-to-model corrections to the estimator from ground-truth training data. To this end, we derive a novel loss function for learning SE(3) corrections based on a matrix Lie groups approach, with a natural formulation for balancing translation and rotation errors. We use this loss to train a deep pose correction network (DPC-Net) that predicts corrections for a particular estimator, sensor and environment. Using the KITTI odometry dataset, we demonstrate significant improvements to the accuracy of a computationally-efficient sparse stereo visual odometry pipeline, that render it as accurate as a modern computationally-intensive dense estimator. Further, we show how DPC-Net can be used to mitigate the effect of poorly calibrated lens distortion parameters.},
archivePrefix = {arXiv},
arxivId = {1709.03128},
author = {Peretroukhin, Valentin and Kelly, Jonathan},
doi = {10.1109/LRA.2017.2778765},
eprint = {1709.03128},
file = {:home/matias/Documents/Mendeley Desktop/Peretroukhin, Kelly/2018/Peretroukhin, Kelly - 2018 - DPC-Net Deep Pose Correction for Visual Localization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep Learning in Robotics and Automation,localization},
number = {3},
pages = {2424--2431},
title = {{DPC-Net: Deep Pose Correction for Visual Localization}},
volume = {3},
year = {2018}
}
@article{Deng2013,
abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme. In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
author = {Learning, Deep},
doi = {10.1136/bmj.319.7209.0a},
file = {::},
isbn = {9781405161251},
issn = {0959-8138},
journal = {Foundations and Trends in Signal Processing},
number = {7209},
pages = {197----387},
pmid = {10463930},
title = {{GPU-A Full Hardware Guide to Deep Learning}},
volume = {319},
year = {1999}
}
@article{Nister2004,
abstract = {An efficient algorithmic solution to the classical five-point relative pose problem is presented. The problem is to find the possible solutions for relative camera pose between two calibrated views given five corresponding points. The algorithm consists of computing the coefficients of a tenth degree polynomial in closed form and, subsequently, finding its roots. It is the first algorithm well-suited for numerical implementation that also corresponds to the inherent complexity of the problem. We investigate the numerical precision of the algorithm. We also study its performance under noise in minimal as well as overdetermined cases. The performance is compared to that of the well-known 8 and 7-point methods and a 6-point scheme. The algorithm is used in a robust hypothesize-and-test framework to estimate structure and motion in real-time with low delay. The real-time system uses solely visual input and has been demonstrated at major conferences.},
author = {Nist{\'{e}}r, David},
doi = {10.1109/TPAMI.2004.17},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera calibration,Ego-motion estimation,Imaging geometry,Motion,Relative orientation,Scene reconstruction,Structure from motion},
month = {jun},
number = {6},
pages = {756--770},
pmid = {18579936},
title = {{An efficient solution to the five-point relative pose problem}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1288525},
volume = {26},
year = {2004}
}
@inproceedings{Xinjilefu2015,
abstract = {We introduce a center of mass estimator, and its application in full body inverse dynamics control, fall detection and fall prevention for humanoid robots. We use the Linear Inverted Pendulum Model dynamics with an offset to predict the center of mass motion. This offset can be interpreted as a modelling error on the center of mass position, or an external force exerted on the center of mass of the robot, or a combination of both. The center of mass estimator is implemented on our Atlas humanoid robot. During the DARPA Robotics Challenge Finals, it successfully saved our robot from falling on two occasions, and made us the only competitive team without a fall or human physical intervention among all humanoid teams.},
author = {Xinjilefu, X. and Feng, Siyuan and Atkeson, Christopher G.},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2015.7363533},
file = {::},
isbn = {9781479968855},
issn = {21640580},
keywords = {Acceleration,Dynamics,Foot,Kinematics,Legged locomotion,Robot sensing systems},
month = {nov},
pages = {67--73},
publisher = {IEEE},
title = {{Center of mass estimator for humanoids and its application in modelling error compensation, fall detection and prevention}},
url = {http://ieeexplore.ieee.org/document/7363533/},
volume = {2015-Decem},
year = {2015}
}
@article{Brooks1997,
abstract = {A procedure is described for self-calibration of a moving camera from instantaneous optical flow. Under certain assumptions this procedure allows the egomotion and some intrinsic parameters of the camera to be determined solely from the instantaneous positions and velocities of a set of image features. The proposed method relies on the use of a differential epipolar equation that relates optical flow to the egomotion and the internal geometry of the camera. A detailed derivation of this equation is presented. This aspect of the work may be seen as a recasting into an analytical framework of the pivotal research of Vi{\'{e}}ville and Faugeras [Proceedings of the Fifth International Conference on Computer Vision (IEEE Computer Society Press, Los Alamitos, Calif., 1995), pp. 750-756]. The information about the camera's egomotion and internal geometry enters the differential epipolar equation via two matrices. It emerges that the optical flow determines the composite ratio of some of the entries of the two matrices. It is shown that a camera with unknown focal length undergoing arbitrary motion can be self-calibrated by means of closed-form expressions in the composite ratio. The corresponding formulas specify five egomotion parameters as well as the focal length and its derivative. A procedure is presented for reconstructing the viewed scene, up to a scale factor, from the derived self-calibration data and the optical flow data. Experimental results are given to demonstrate the correctness of the approach. {\textcopyright} 1997 Optical Society of America.},
author = {Brooks, Michael J. and Chojnacki, Wojciech and Baumela, Luis},
doi = {10.1364/josaa.14.002670},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A},
keywords = {and phrases,ego-motion,epipolar equation,intrinsic parameters,self-calibration},
number = {10},
pages = {2670},
title = {{Determining the egomotion of an uncalibrated camera from instantaneous optical flow}},
volume = {14},
year = {1997}
}
@inproceedings{Bertsekas1996,
abstract = {In this paper we propose and analyze nonlinear least squares methods, which process the data incrementally, one data block at a time. Such methods are well suited for large data sets and real time operation, and have received much attention in the context of neural network training problems. We focus on the Extended Kalman Filter, which may be viewed as an incremental version of the Gauss-Newton method. We provide a nonstochastic analysis of its convergence properties, and we discuss variants aimed at accelerating its convergence.},
author = {Bertsekas, Dimitri P.},
booktitle = {Proceedings of the IEEE Conference on Decision and Control},
doi = {10.1109/cdc.1994.411166},
isbn = {0-7803-1968-0},
issn = {01912216},
keywords = {65K10,90C30,Kalman filter AMS subject classifications 93-11,least squares,optimization},
number = {3},
pages = {1211--1214},
publisher = {IEEE},
title = {{Incremental least squares methods and the extended Kalman filter}},
url = {http://ieeexplore.ieee.org/document/411166/},
volume = {2},
year = {1994}
}
@phdthesis{Churchill2012a,
abstract = {For robotic systems to realise lifelong autonomy they must be able to navigate accurately in changing environments. In this thesis we describe, implement and validate a new approach to the problem of long-term navigation. To begin, we present our stereo visual odometry system which provides highly ac-curate pose estimation. Our approach combines several techniques found in existing implementations and a recently published image descriptor that simplifies the solu-tion architecture. The performance and versatility of our system is demonstrated through testing on multiple datasets. Equipped with our visual odometry system, we describe a new approach to the problem of lifelong navigation. We learn a model whose complexity varies naturally in accordance with the variation of scene appearance. As the robot repeatedly traverses its workspace, it accumulates distinct visual experiences that, in concert, implicitly represent the scene variation -each experience captures a visual mode. When operating in a previously visited area, we continually try to localise in these previous experiences while simultaneously running the visual odometry. Failure to localise in a sufficient number of prior experiences indicates an insufficient model of the workspace and instigates the laying down of the live image sequence as a new distinct experience. In this way, over time we can capture the typical temporally varying appearance of an environment and the number of experiences required tends to a constant. Although we focus on vision as a primary sensor, the ideas we present here are equally applicable to other sensor modalities. We demonstrate our approach working on a road vehicle operating over a three month period at different times of day, in different weather and lighting conditions. ii},
author = {Churchill, Winston and Newman, Paul},
file = {:home/matias/Documents/Mendeley Desktop/Churchill, Newman/2012/Churchill, Newman - 2012 - Experience Based Navigation Theory, Practice and Implementation.pdf:pdf},
number = {October},
school = {University of Oxford},
title = {{Experience Based Navigation: Theory, Practice and Implementation}},
url = {http://www.robots.ox.ac.uk/$\sim$mobile/Theses/WinstonThesis.pdf},
year = {2012}
}
@phdthesis{Newcombe2012,
abstract = {Visual SLAM systems aim to estimate the motion of a moving camera together with the geometric structure and appearance of the world being observed. To the extent that this is possible using only an image stream, the core problem that must be solved by any practical visual SLAM system is that of obtaining correspondence throughout the images captured. Modern visual SLAM pipelines commonly obtain correspondence by using sparse feature matching techniques and construct maps using a composition of point, line or other simple geometric primitives. The resulting sparse feature map representations provide sparsely furnished, incomplete reconstructions of the observed scene. Related techniques from multiple view stereo (MVS) achieve high quality dense recon- struction by obtaining dense correspondences over calibrated image sequences. Despite the usefulness of the resulting dense models, these techniques have been of limited use in visual SLAM systems. The computational complexity of estimating dense surface geome- try has been a practical barrier to its use in real-time SLAM. Furthermore, MVS algorithms have typically required a fixed length, calibrated image sequence to be available throughout the optimisation—a condition fundamentally at odds with the online nature of SLAM. With the availability of massively-parallel commodity computing hardware, we demon- strate new algorithms that achieve high quality incremental dense reconstruction within online visual SLAM. The result is a live dense reconstruction (LDR) of scenes that makes possible numerous applications that can utilise online surface modelling, for instance: plan- ning robot interactions with unknown objects, augmented reality with characters that in- teract with the scene, or providing enhanced data for object recognition. The core of this thesis goes beyond LDR to demonstrate fully dense visual SLAM. We re- place the sparse feature map representation with an incrementally updated, non-parametric, dense surface model. By enabling real-time dense depth map estimation through novel short baseline MVS, we can continuously update the scene model and further leverage its predictive capabilities to achieve robust camera pose estimation with direct whole image alignment. We demonstrate the capabilities of dense visual SLAM using a single mov- ing passive camera, and also when real-time surface measurements are provided by a commodity depth camera. The results demonstrate state-of-the-art, pick-up-and-play 3D reconstruction and camera tracking systems useful in many real world scenarios.},
author = {Newcombe, Richard A},
doi = {10.1109/IROS.2013.6696650},
keywords = {SLAM,computer vision,convex optimisation,dense reconstruction,multiple view stereo,mvs,robot vision,stereo},
number = {December},
pages = {324},
school = {Imperial College London},
title = {{Dense Visual SLAM}},
year = {2012}
}
@article{Burri2017,
abstract = {With the growing availability of agile and powerful micro aerial vehicles (MAVs), accurate modeling is becoming more important. Especially for highly dynamic flights, model-based estimation and control combined with a good simulation framework is key. While detailed models are available in the literature, measuring the model parameters can be a time-consuming task and requires access to special equipment or facilities. In this paper, we propose a principled approach to accurately estimate physical parameters based on a maximum likelihood (ML) estimation scheme. Unlike many current methods, we make direct use of both raw inertial measurement unit measurements and the rotor speeds of the MAV. We also estimate the spatial-temporal alignment to a modular pose sensor. The proposed ML-based approach finds the parameters that best explain the sensor readings and also provides an estimate of their uncertainty. Although we derive the proposed method for use with an MAV, the approach is kept general and can be extended to other sensors or flying platforms. Extensive evaluation on simulated data and on real-world experimental data demonstrates that the approach yields accurate estimates and exhibits a large region of convergence. Furthermore, we show that the estimation can be performed using only on-board sensing, requiring no external infrastructure.},
author = {Burri, Michael and Bloesch, Michael and Taylor, Zachary and Siegwart, Roland and Nieto, Juan},
doi = {10.1002/rob.21729},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {5--22},
title = {{A framework for maximum likelihood parameter identification applied on MAVs}},
volume = {35},
year = {2018}
}
@article{Corke2013,
abstract = {In outdoor environments shadows are common. These typically strong visual features cause considerable change in the appearance of a place, and therefore confound vision-based localisation approaches. In this paper we describe how to convert a colour image of the scene to a greyscale invariant image where pixel values are a function of underlying material property not lighting. We summarise the theory of shadow invariant images and discuss the modelling and calibration issues which are important for non-ideal off-the-shelf colour cameras. We evaluate the technique with a commonly used robotic camera and an autonomous car operating in an outdoor environment, and show that it can outperform the use of ordinary greyscale images for the task of visual localisation. {\textcopyright} 2013 IEEE.},
author = {Corke, Peter and Paul, Rohan and Churchill, Winston and Newman, Paul},
doi = {10.1109/IROS.2013.6696648},
file = {:home/matias/Documents/Mendeley Desktop/Corke et al/2013/Corke et al. - 2013 - Dealing with shadows Capturing intrinsic scene appearance for image-based outdoor localisation.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2085--2092},
publisher = {IEEE},
title = {{Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation}},
year = {2013}
}
@article{Horn1987,
abstract = {Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 × 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.},
author = {Horn, Berthold K. P.},
doi = {10.1364/josaa.4.000629},
isbn = {1084-7529},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A},
number = {4},
pages = {629},
title = {{Closed-form solution of absolute orientation using unit quaternions}},
url = {https://www.osapublishing.org/abstract.cfm?URI=josaa-4-4-629},
volume = {4},
year = {1987}
}
@article{Wirbel2014,
abstract = {In this article, we present our work to provide a navigation and localization system on a constrained humanoid platform, the NAO robot, without modifying the robot sensors. First, we tried to implement a simple and light version of classic monocular Simultaneous Localization and Mapping (SLAM) algorithms, while adapting to the CPU and camera quality, which turned out to be insufficient on the platform for the moment. From our work on keypoints tracking, we identified that some keypoints can be still accurately tracked at little cost, and used them to build a visual compass. This compass was then used to correct the robot walk because it makes it possible to control the robot orientation accurately.},
author = {Wirbel, Emilie and Bonnabel, Silv{\`{e}}re and {De La Fortelle}, Arnaud and Moutarde, Fabien},
doi = {10.1515/jisys-2013-0079},
issn = {03341860},
journal = {Journal of Intelligent Systems},
keywords = {Humanoid,SLAM,Vision},
number = {2},
pages = {113--132},
title = {{Humanoid robot navigation: Getting localization information from vision}},
volume = {23},
year = {2014}
}
@article{Berczi2017,
abstract = {Assessing terrain ahead of a robot when repeating previously driven safe paths can be accomplished by looking for geometric changes (e.g., due to the appearance of humans or other obstacles). Previous work has shown that the incorporation of data-driven learning and place-dependence are useful aspects of making terrain classification viable in challenging terrain. This paper presents a learning, place-dependent (LPD) terrain classifier that uses a probabilistic model of the terrain to improve detection of small obstacles in uncluttered terrain while avoiding false positives in more challenging environments. Specifically, a Gaussian mixture model is used to account for multi-height terrain cells that arise in heavily vegetated areas (where both a ground plane and overhanging vegetation can occupy the same cell). A variational Bayesian technique is used to automatically determine the number of components required for each cell using a Dirichlet prior on mixing proportions and a Normal-Inverse-Wishart prior on the means and covariances of the components. The probabilistic nature of the model allows for the detection of much smaller obstacles in regions that exhibit low variance in the terrain surface, whilst still avoiding false positives in regions where the terrain is highly cluttered (e.g., vegetation). The algorithm is tested on almost 10 km of autonomous traverse and is shown to be able to classify a wider range of obstacles than two baseline change-detection algorithms based on absolute geometric differences.},
author = {Berczi, Laszlo Peter and Barfoot, Timothy D.},
doi = {10.1109/IROS.2017.8206243},
file = {:home/matias/Documents/Mendeley Desktop/Berczi, Barfoot/2017/Berczi, Barfoot - 2017 - Looking high and low Learning place-dependent Gaussian mixture height models for terrain assessment.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Learning from Demonstration,Robot Safety,Visual-Ba},
pages = {3918--3925},
title = {{Looking high and low: Learning place-dependent Gaussian mixture height models for terrain assessment}},
volume = {2017-Septe},
year = {2017}
}
@inproceedings{Pan2019,
abstract = {Imitation learning is a popular approach for training visual navigation policies. However, collecting expert demonstrations for legged robots is challenging as these robots can be hard to control, move slowly, and cannot operate continuously for a long time. Here, we propose a zero-shot imitation learning approach for training a visual navigation policy on legged robots from human (third-person perspective) demonstrations, enabling high-quality navigation and cost-effective data collection. However, imitation learning from third-person demonstrations raises unique challenges. First, these demonstrations are captured from different camera perspectives, which we address via a feature disentanglement network (FDN) that extracts perspective-invariant state features. Second, as transition dynamics vary across systems, we label missing actions by either building an inverse model of the robot's dynamics in the feature space and applying it to the human demonstrations or developing a Graphic User Interface(GUI) to label human demonstrations. To train a navigation policy we use a model-based imitation learning approach with FDN and labeled human demonstrations. We show that our framework can learn an effective policy for a legged robot, Laikago, from human demonstrations in both simulated and real-world environments. Our approach is zero-shot as the robot never navigates the same paths during training as those at testing time. We justify our framework by performing a comparative study.},
archivePrefix = {arXiv},
arxivId = {1909.12971},
author = {Pan, Xinlei and Zhang, Tingnan and Ichter, Brian and Faust, Aleksandra and Tan, Jie and Ha, Sehoon},
booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA40945.2020.9196602},
eprint = {1909.12971},
file = {:home/matias/Documents/Mendeley Desktop/Pan et al/2020/Pan et al. - 2020 - Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation.pdf:pdf},
isbn = {978-1-7281-7395-5},
month = {may},
pages = {679--685},
publisher = {IEEE},
title = {{Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation}},
url = {http://arxiv.org/abs/1909.12971 https://ieeexplore.ieee.org/document/9196602/},
year = {2020}
}
@article{Vanhoucke2011,
abstract = {Recent advances in deep learning have made the use of large, deep neural net- works with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improve- ment over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.},
author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mz},
issn = {9781450329569},
journal = {Proc. Deep Learning and {\ldots}},
pages = {1--8},
title = {{Improving the speed of neural networks on CPUs}},
url = {http://research.google.com/pubs/archive/37631.pdf},
year = {2011}
}
@article{Alismail2017,
abstract = {Feature descriptors are powerful tools for photometrically and geometrically invariant image matching. To date, however, their use has been tied to sparse interest point detection, which is susceptible to noise under adverse imaging conditions. In this letter, we propose to use binary feature descriptors in a direct tracking framework without relying on sparse interest points. This novel combination of feature descriptors and direct tracking is shown to achieve robust and efficient visual odometry with applications to poorly lit subterranean environments.},
author = {Alismail, Hatem and Kaess, Michael and Browning, Brett and Lucey, Simon},
doi = {10.1109/LRA.2016.2635686},
file = {:home/matias/Documents/Mendeley Desktop/Alismail et al/2017/Alismail et al. - 2017 - Direct Visual Odometry in Low Light Using Binary Descriptors.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Low light vision,SLAM,mapping,robust visual odometry,visual tracking},
number = {2},
pages = {444--451},
publisher = {IEEE},
title = {{Direct Visual Odometry in Low Light Using Binary Descriptors}},
volume = {2},
year = {2017}
}
@article{Newcombe2015,
abstract = {We present the first dense SLAM system capable of reconstructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sensors. Our DynamicFusion approach reconstructs scene geometry whilst simultaneously estimating a dense volumetric 6D motion field that warps the estimated geometry into a live frame. Like KinectFusion, our system produces increasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes.},
author = {Newcombe, Richard A. and Fox, Dieter and Seitz, Steven M.},
doi = {10.1109/CVPR.2015.7298631},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {343--352},
title = {{DynamicFusion: Reconstruction and tracking of non-rigid scenes in real-time}},
volume = {07-12-June},
year = {2015}
}
@article{Rofer2014,
abstract = {In RoboCup, source code releases after a competition are one important part of the competition's overall progress. In particular teams in the Standard Platform League can strongly benefit from other's software, as everybody shares the same robot platform. Therefore several releases by different teams exist. In this paper, we describe the past code releases of our team B-Human, particularly focusing on its underlying software architecture, which is set in relation to concepts of the currently popular Robot Operating System (ROS), as well as on its impact on the league's progress. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {R{\"{o}}fer, Thomas and Laue, Tim},
doi = {10.1007/978-3-662-44468-9_61},
file = {::},
isbn = {9783662444672},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {648--655},
title = {{On B-human's code releases in the standard platform league - Software architecture and impact}},
volume = {8371 LNAI},
year = {2014}
}
@phdthesis{Furgale2011,
abstract = {Mars represents one of the most important targets for space exploration in the next 10 to 30 years, particularly because of evidence of liquid water in the planet's past. Current environmental conditions dictate that any existing water reserves will be in the form of ice; finding and sampling these ice deposits would further the study of the planet's climate history, further the search for evidence of life, and facilitate in-situ resource utilization during future manned exploration missions. This thesis presents a suite of algorithms to help enable a robotic ice-prospecting mission to Mars. Starting from visual odometry--the estimation of a rover's motion using a stereo camera as the primary sensor--we develop the following extensions: (i) a coupled surface/subsurface modelling system that provides novel data products to scientists working remotely, (ii) an autonomous retrotraverse system that allows a rover to return to previously visited places along a route for sampling, or to return a sample to an ascent vehicle, and (iii) the extension of the appearance-based visual odometry pipeline to an actively illuminated light detection and ranging sensor that provides data similar to a stereo camera but is not reliant on consistent ambient lighting, thereby enabling appearance-based vision techniques to be used in environments that are not conducive to passive cameras, such as underground mines or permanently shadowed craters on the moon. All algorithms are evaluated on real data collected using our field robot at the University of Toronto Institute for Aerospace Studies, or at a planetary analogue site on Devon Island, in the Canadian High Arctic.},
author = {Furgale, Paul Timothy},
booktitle = {ProQuest Dissertations and Theses},
isbn = {9780494781852},
issn = {0494781858},
keywords = {0538:Aerospace engineering,0771:Robotics,Aerospace engineering,Applied sciences,Planetary surfaces,Robotic missions,Robotics,Space exploration,Visual odometry},
pages = {157},
school = {University of Toronto},
title = {{Extensions to the Visual Odometry Pipeline for the Exploration of Planetary Surfaces}},
url = {http://ezproxy.net.ucf.edu/login?url=http://search.proquest.com/docview/924428359?accountid=10003%5Cnhttp://sfx.fcla.edu/ucf?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations+&+theses&sid=ProQ:ProQuest+Dissertations+&+T},
volume = {NR78185},
year = {2011}
}
@inproceedings{McManus2012,
abstract = {Visual Teach and Repeat (VT&R) has proven to be an effective method to allow a vehicle to autonomously repeat any previously driven route without the need for a global positioning system. One of the major challenges for a method that relies on visual input to recognize previously visited places is lighting change, as this can make the appearance of a scene look drastically different. For this reason, passive sensors, such as cameras, are not ideal for outdoor environments with inconsistent/inadequate light. However, camera-based systems have been very successful for localization and mapping in outdoor, unstructured terrain, which can be largely attributed to the use of sparse, appearance-based computer vision techniques. Thus, in an effort to achieve lighting invariance and to continue to exploit the heritage of the appearance-based vision techniques traditionally used with cameras, this paper presents the first VT&R system that uses appearance-based techniques with laser scanners for motion estimation. The system has been field tested in a planetary analogue environment for an entire diurnal cycle, covering more than 11km with an autonomy rate of 99.7% of the distance traveled. {\textcopyright} 2012 IEEE.},
author = {McManus, Colin and Furgale, Paul and Stenning, Braden and Barfoot, Timothy D.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224654},
file = {:home/matias/Documents/Mendeley Desktop/McManus et al/2012/McManus et al. - 2012 - Visual Teach and Repeat using appearance-based lidar.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
pages = {389--396},
title = {{Visual Teach and Repeat using appearance-based lidar}},
year = {2012}
}
@article{Charbonneau2016,
abstract = {This paper proposes control laws ensuring the stabilization of a time-varying desired joint trajectory and joint limit avoidance in the case of fully-actuated manipulators. The key idea is to perform a parametrization of the feasible joint space in terms of exogenous states. It follows that the control of these states allows for joint limit avoidance. One of the main outcomes of this paper is that position terms in control laws are replaced by parametrized terms. Stability and convergence of time-varying reference trajectories obtained with the proposed method are demonstrated to be in the sense of Lyapunov. The introduced control laws are verified by carrying out experiments on two degrees-of-freedom of the torque-controlled humanoid robot iCub.},
archivePrefix = {arXiv},
arxivId = {1608.06767},
author = {Charbonneau, Marie and Nori, Francesco and Pucci, Daniele},
doi = {10.1109/HUMANOIDS.2016.7803379},
eprint = {1608.06767},
file = {::},
isbn = {9781509047185},
issn = {21640580},
journal = {IEEE-RAS International Conference on Humanoid Robots},
pages = {899--904},
title = {{On-line joint limit avoidance for torque controlled robots by joint space parametrization}},
year = {2016}
}
@article{Corke2007a,
abstract = {This paper demonstrates some interesting connections between the hitherto disparate fields of mobile robot navigation and image-based visual servoing. A planar formulation of the well-known image-based visual servoing method leads to a bearing-only navigation system that requires no explicit localization and directly yields desired velocity. The well known benefits of image-based visual servoing such as robustness apply also to the planar case. Simulation results are presented.},
author = {Corke, Peter},
doi = {10.1007/3-540-36460-9_24},
file = {:home/matias/Documents/Mendeley Desktop/Corke/2007/Corke - 2007 - Mobile Robot Navigation As A Planar Visual Servoing Problem.pdf:pdf},
journal = {Robotics Research},
pages = {361--372},
title = {{Mobile Robot Navigation As A Planar Visual Servoing Problem}},
year = {2007}
}
@article{Corke2007,
abstract = {In this paper we present a tutorial introduction to two important senses for biological and robotic systems inertial and visual perception. We discuss the fundamentals of these two sensing modalities from a biological and an engineering perspective. Digital camera chips and micro-machined accelerometers and gyroscopes are now commodities, and when combined with today's available computing can provide robust estimates of self-motion as well 3D scene structure, without external infrastructure. We discuss the complementarity of these sensors, describe some fundamental approaches to fusing their outputs and survey the field. {\textcopyright} 2007 SAGE Publications.},
author = {Corke, Peter and Lobo, Jorge and Dias, Jorge},
doi = {10.1177/0278364907079279},
isbn = {0278364907079},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Inertial sensing,Sensor fusion,Vision},
month = {jun},
number = {6},
pages = {519--535},
title = {{An introduction to inertial and visual sensing}},
url = {http://journals.sagepub.com/doi/10.1177/0278364907079279},
volume = {26},
year = {2007}
}
@article{Warren2019a,
abstract = {Redundant navigation systems are critical for safe operation of UAVs in high-risk environments. Since most commercial UAVs almost wholly rely on GPS, jamming, interference, and multi-pathing are real concerns that usually limit their operations to low-risk environments and visual line-of-sight. This letter presents a vision-based route-following system for the autonomous, safe return of UAVs under primary navigation failure such as GPS jamming. Using a Visual Teach and Repeat framework to build a visual map of the environment during an outbound flight, we show the autonomous return of the UAV by visually localizing the live view to this map when a simulated GPS failure occurs, controlling the vehicle to follow the safe outbound path back to the launch point. Using gimbal-stabilized stereo vision and inertial sensing alone, without reliance on external infrastructure, Visual Odometry and localization are achieved at altitudes of 5-25 m and flight speeds up to 55 km/h. We examine the performance of the visual localization algorithm under a variety of conditions and also demonstrate closed-loop autonomy along a complicated 450 m path.},
archivePrefix = {arXiv},
arxivId = {1809.05757},
author = {Warren, Michael and Greeff, Melissa and Patel, Bhavit and Collier, Jack and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/LRA.2018.2883408},
eprint = {1809.05757},
file = {:home/matias/Documents/Mendeley Desktop/Warren et al/Unknown/Warren et al. - Unknown - There ' s No Place Like Home Visual Teach and Repeat for Emergency Return of Multirotor UAVs During GPS Fai.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial systems: Perception and autonomy,sensor-based control,visual-based navigation},
number = {1},
pages = {161--168},
title = {{There's no place like home: Visual teach and repeat for emergency return of multirotor UAVs during GPS failure}},
volume = {4},
year = {2019}
}
@article{Linegar2016a,
abstract = {Robust localisation is a key requirement for autonomous vehicles. However, in order to achieve widespread adoption of this technology, we also require this func-tion to be performed using low-cost hardware. Cameras are appealing due to their information-rich image content and low cost; however, camera-based localisation is difficult because of the problem of appearance change. For example, in outdoor en-vironments the appearance of the world can change dramatically and unpredictably with variations in lighting, weather, season and scene structure. We require au-tonomous vehicles to be robust under these challenging environmental conditions. This thesis presents Dub4, a vision-only localisation system for autonomous ve-hicles. The system is founded on the concept of experiences, where an " experience " is a visual memory which models the world under particular conditions. By allow-ing the system to build up and curate a map of these experiences, we are able to handle cyclic appearance change (lighting, weather and season) as well as adapt to slow structural change. We present a probabilistic framework for predicting which experiences are most likely to match successfully with the live image at run-time, conditioned on the robot's prior use of the map. In addition, we describe an un-supervised algorithm for detecting and modelling higher-level visual features in the environment for localisation. These features are trained on a per-experience basis and are robust to extreme changes in appearance, for example between rain and sun, or day and night. The system is tested on over 1500 km of data, from urban and off-road envi-ronments, through sun, rain, snow, harsh lighting, at different times of the day and night, and through all seasons. In addition to this extensive offline testing, Dub4 has served as the primary localisation source on a number of autonomous vehicles, in-cluding the Oxford University's RobotCar, the 2016 Shell Eco-Marathon, the LUTZ PathFinder Project in Milton Keynes, and the GATEway Project in Greenwich, London.},
author = {Linegar, Chris and Newman, Paul},
file = {:home/matias/Documents/Mendeley Desktop/Linegar, Newman/2016/Linegar, Newman - 2016 - Vision-Only Localisation Under Extreme Appearance Change.pdf:pdf},
number = {August},
title = {{Vision-Only Localisation Under Extreme Appearance Change}},
url = {http://www.robots.ox.ac.uk/$\sim$mobile/Theses/linegar_thesis_2016.pdf},
year = {2016}
}
@incollection{Kitano1998a,
abstract = {The Robot World-Cup Soccer (RoboCup) is an attempt to foster AI and intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined. The first RoboCup competition will be held at the Fifteenth International Joint Conference on Artificial Intelligence in Nagoya, Japan. A robot team must actually perform a soccer game, incorporating various technologies, including design principles of autonomous agents, multiagent collaboration, strategy acquisition, real-time reasoning, robotics, and sensor fusion. RoboCup is a task for a team of multiple fast-moving robots under a dynamic environment. Although RoboCup's final target is a world cup with real robots, RoboCup offers a software platform for research on the software aspects of RoboCup. This article describes technical challenges involved in RoboCup, rules, and the simulation environment. Copyright {\textcopyright} 1997, American Association for Artificial Intelligence.},
author = {Kitano, Hiroaki and Asada, Minoru and Kuniyoshi, Yasuo and Noda, Itsuki and Osawa, Eiichi and Matsubara, Hitoshi},
booktitle = {AI Magazine},
doi = {10.1007/3-540-64473-3_46},
file = {::},
isbn = {9783540644736},
issn = {07384602},
number = {1},
pages = {73--85},
title = {{RoboCup: A challenge problem for AI}},
url = {http://link.springer.com/10.1007/3-540-64473-3_46},
volume = {18},
year = {1997}
}
@inproceedings{Chen2015,
abstract = {This paper presents a data-driven matching cost for stereo matching. A novel deep visual correspondence embedding model is trained via Convolutional Neural Network on a large set of stereo images with ground truth disparities. This deep embedding model leverages appearance data to learn visual similarity relationships between corresponding image patches, and explicitly maps intensity values into an embedding feature space to measure pixel dissimilarities. Experimental results on KITTI and Middlebury data sets demonstrate the effectiveness of our model. First, we prove that the new measure of pixel dissimilarity outperforms traditional matching costs. Furthermore, when integrated with a global stereo framework, our method ranks top 3 among all two-frame algorithms on the KITTI benchmark. Finally, cross-validation results show that our model is able to make correct predictions for unseen data which are outside of its labeled training set.},
author = {Chen, Zhuoyuan and Sun, Xun and Wang, Liang and Yu, Yinan and Huang, Chang},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.117},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
number = {c},
pages = {972--980},
publisher = {IEEE},
title = {{A deep visual correspondence embedding model for stereo matching costs}},
url = {http://ieeexplore.ieee.org/document/7410474/},
volume = {2015 Inter},
year = {2015}
}
@article{Yu2018,
abstract = {Accurate and robust object state estimation enables successful object manipulation. Visual sensing is widely used to estimate object poses. However, in a cluttered scene or in a tight workspace, the robot's end-effector often occludes the object from the visual sensor. The robot then loses visual feedback and must fall back on open-loop execution. In this paper, we integrate both tactile and visual input using a framework for solving the SLAM problem, incremental smoothing and mapping (iSAM), to provide a fast and flexible solution. Visual sensing provides global pose information but is noisy in general, whereas contact sensing is local, but its measurements are more accurate relative to the end-effector. By combining them, we aim to exploit their advantages and overcome their limitations. We explore the technique in the context of a pusher-slider system. We adapt iSAM's measurement cost and motion cost to the pushing scenario, and use an instrumented setup to evaluate the estimation quality with different object shapes, on different surface materials, and under different contact modes.},
archivePrefix = {arXiv},
arxivId = {1709.09694},
author = {Yu, Kuan Ting and Rodriguez, Alberto},
doi = {10.1109/ICRA.2018.8463183},
eprint = {1709.09694},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {7778--7785},
title = {{Realtime State Estimation with Tactile and Visual Sensing. Application to Planar Manipulation}},
year = {2018}
}
@article{Ostafew2015,
abstract = {Robust control maintains stability and performance for a fixed amount of model uncertainty but can be conservative since the model is not updated online. Learning-based control, on the other hand, uses data to improve the model over time but is not typically guaranteed to be robust throughout the process. This paper proposes a novel combination of both ideas: A robust Min-Max Learning-Based Nonlinear Model Predictive Control (MM-LB-NMPC) algorithm. Based on an existing LB-NMPC algorithm, we present an efficient and robust extension, altering the NMPC performance objective to optimize for the worst-case scenario. The algorithm uses a simple a priori vehicle model and a learned disturbance model. Disturbances are modelled as a Gaussian Process (GP) based on experience collected during previous trials as a function of system state, input, and other relevant variables. Nominal state sequences are predicted using an Unscented Transform and worst-case scenarios are defined as sequences bounding the 3$\sigma$ confidence region. Localization for the controller is provided by an on-board, vision-based mapping and navigation system enabling operation in large-scale, GPS-denied environments. The paper presents experimental results from testing on a 50 kg skid-steered robot executing a path-tracking task. The results show reductions in maximum lateral and heading path-tracking errors by up to 30% and a clear transition from robust control when the model uncertainty is high to optimal control when model uncertainty is reduced.},
author = {Ostafew, Chris J. and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/ICRA.2015.7139033},
file = {:home/matias/Documents/Mendeley Desktop/Ostafew, Schoellig, Barfoot/2015/Ostafew, Schoellig, Barfoot - 2015 - Conservative to confident Treating uncertainty robustly within Learning-Based Control.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {421--427},
publisher = {IEEE},
title = {{Conservative to confident: Treating uncertainty robustly within Learning-Based Control}},
volume = {2015-June},
year = {2015}
}
@article{Chen2008,
abstract = {CHOLMOD is a set of routines for factorizing sparse symmetric positive definite matrices of the form A or AAT, updating/downdating a sparse Cholesky factorization, solving linear systems, updating/downdating the solution to the triangular system Lx = b, and many other sparse matrix functions for both symmetric and unsymmetric matrices. Its supernodal Cholesky factorization relies on LAPACK and the Level-3 BLAS, and obtains a substantial fraction of the peak performance of the BLAS. Both real and complex matrices are supported. CHOLMOD is written in ANSI/ISO C, with both C and MATLABTM interfaces. It appears in MATLAB 7.2 as x = A\b when A is sparse symmetric positive definite, as well as in several other sparse matrix functions. {\textcopyright} 2008 ACM.},
author = {Chen, Yanqing and Davis, Timothy A. and Hager, William W. and Rajamanickam, Sivasankaran},
doi = {10.1145/1391989.1391995},
issn = {00983500},
journal = {ACM Transactions on Mathematical Software},
keywords = {Cholesky factorization,Linear equations,Sparse matrices},
number = {3},
pages = {1--14},
title = {{Algorithm 887: CHOLMOD, supernodal sparse cholesky factorization and update/downdate}},
volume = {35},
year = {2008}
}
@article{Cieslewski2019,
abstract = {A wide range of computer vision algorithms rely on identifying sparse interest points in images and establishing correspondences between them. However, only a subset of the initially identified interest points results in true correspondences (inliers). In this paper, we seek a detector that finds the minimum number of points that are likely to result in an application-dependent 'sufficient' number of inliers k. To quantify this goal, we introduce the 'k-succinctness' metric. Extracting a minimum number of interest points is attractive for many applications, because it can reduce computational load, memory, and data transmission. Alongside succinctness, we introduce an unsupervised training methodology for interest point detectors that is based on predicting the probability of a given pixel being an inlier. In comparison to previous learned detectors, our method requires the least amount of data pre-processing. Our detector and other state-of-the-art detectors are extensively evaluated with respect to succinctness on popular public datasets covering both indoor and outdoor scenes, and both wide and narrow baselines. In certain cases, our detector is able to obtain an equivalent amount of inliers with as little as 60% of the amount of points of other detectors. The code and trained networks are provided at https://github.com/uzh-rpg/sips2-open.},
archivePrefix = {arXiv},
arxivId = {1805.01358},
author = {Cieslewski, Titus and Derpanis, Konstantinos G. and Scaramuzza, Davide},
doi = {10.1109/3DV.2019.00072},
eprint = {1805.01358},
file = {:home/matias/Documents/Mendeley Desktop/Cieslewski, Derpanis, Scaramuzza/2019/Cieslewski, Derpanis, Scaramuzza - 2019 - SIPs Succinct Interest Points from Unsupervised Inlierness Probability Learning.pdf:pdf},
isbn = {9781728131313},
journal = {Proceedings - 2019 International Conference on 3D Vision, 3DV 2019},
keywords = {Interest points,detection,features,keypoint,unsupervised},
pages = {604--613},
title = {{SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning}},
year = {2019}
}
@article{Stenning2013,
abstract = {Growing a network of reusable paths is a novel approach to navigation that allows a mobile robot to autonomously seek distant goals in unmapped, GPS-denied environments, which may make it particularly well-suited to rovers used for planetary exploration. A network of reusable paths is an extension to visual-teach-and-repeat systems; instead of a simple chain of poses, there is an arbitrary network. This allows the robot to return to any pose it has previously visited, and it lets a robot plan to reuse previous paths. This paradigm results in closer goal acquisition (through reduced localization error) and a more robust approach to exploration with a mobile robot. It also allows a rover to return a sample to an ascent vehicle with a single command. We show that our network-of-reusable-paths approach is a physical embodiment of the popular rapidly exploring random tree (RRT) planner. Simulation results are presented along with the results from two different robotic test systems. These test systems drove over 14 km in planetary analog environments. {\textcopyright} 2013 Wiley Periodicals, Inc.},
author = {Stenning, Braden E. and McManus, Colin and Barfoot, Timothy D.},
doi = {10.1002/rob.21474},
file = {:home/matias/Documents/Mendeley Desktop/Stenning, Mcmanus, Barfoot/2013/Stenning, Mcmanus, Barfoot - 2013 - Planning using a Network of Reusable Paths A Physical Embodiment of a Rapidly Exploring Random Tree.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {nov},
number = {6},
pages = {916--950},
title = {{Planning using a network of reusable paths: A physical embodiment of a rapidly exploring random tree}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/rob.21514/abstract http://doi.wiley.com/10.1002/rob.21474},
volume = {30},
year = {2013}
}
@article{Grunnet-Jepsen2018,
author = {Grunnet-Jepsen, Anders and Winer, Paul and Takagi, Aki and Sweetser, John and Zhao, Kevin and Khuong, Tri and Nie, Dan and Woodfill, John},
doi = {10.19729/j.cnki.1673-5188.2018.03.002},
file = {:home/matias/Documents/Mendeley Desktop/Grunnet-Jepsen et al/2018/Grunnet-Jepsen et al. - 2018 - Using the Intel {\textregistered} RealSense TM Depth cameras D4xx in Multi-Camera Configurations.pdf:pdf},
title = {{Using the Intel {\textregistered} RealSense TM Depth cameras D4xx in Multi-Camera Configurations}},
url = {https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/RealSense_Multiple_Camera_WhitePaper.pdf%0Ahttps://www.digikey.com/product-detail/en/jst-sales-america-inc/SHR-09V-S/455-1399-ND/759888},
year = {2018}
}
@book{Bishop2006,
abstract = {Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field, and together they have undergone substantial development over the past ten years. In particular, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic models. Also, the practical applicability of Bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational Bayes and expectation propa- gation. Similarly, new models based on kernels have had significant impact on both algorithms and applications. This new textbook reflects these recent developments while providing a compre- hensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners, and assumes no previous knowledge of pattern recognition or ma- chine learning concepts. Knowledge of multivariate calculus and basic linear algebra is required, and some familiarity with probabilities would be helpful though not es- sential as the book includes a self-contained introduction to basic probability theory.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M.},
doi = {10.5555/1162264},
eprint = {0-387-31073-8},
file = {:home/matias/Documents/Mendeley Desktop/Bishop/2006/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {00063568},
pages = {749},
pmid = {18292226},
publisher = {Springer Science+Business Media Inc.},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
year = {2006}
}
@article{Black1996,
author = {Black, Michael J and Rangarajan, Anand},
file = {:home/matias/Documents/Mendeley Desktop/Black, Rangarajan/1995/Black, Rangarajan - 1995 - On the Unification of Line Processes , Outlier Rejection , and Robust Statistics with Applications in Early V.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {1},
pages = {57--91},
title = {{On the Unification of Line Processes , Outlier Rejection , and Robust Statistics with Applications in Early Vision Mailing Address : Please address correspondence to :}},
volume = {19},
year = {1996}
}
@article{Gadd2016,
abstract = {This paper is about underpinning long-term operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on realworld forays with maps which require much less storage.},
author = {Gadd, Matthew and Newman, Paul},
doi = {10.1109/IROS.2016.7759843},
file = {:home/matias/Documents/Mendeley Desktop/Gadd, Newman/2016/Gadd, Newman - 2016 - Checkout my map Version control for fleetwide visual localisation.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {5729--5736},
publisher = {IEEE},
title = {{Checkout my map: Version control for fleetwide visual localisation}},
volume = {2016-Novem},
year = {2016}
}
@article{Redmon2016,
abstract = {Text mining draw more and more attention recently, it has been applied on different domains including web mining, opinion mining, and sentiment analysis. Text pre-processing is an important stage in text mining. The major obstacle in text mining is the very high dimensionality and the large size of text data. Natural language processing and morphological tools can be employed to reduce dimensionality and size of text data. In addition, there are many term weighting schemes available in the literature that may be used to enhance text representation as feature vector. In this paper, we study the impact of text pre-processing and different term weighting schemes on Arabic text classification. In addition, develop new combinations of term weighting schemes to be applied on Arabic text for classification purposes.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1142/9789812771728_0012},
eprint = {1612.08242},
isbn = {1879-2057 (Electronic)\n0001-4575 (Linking)},
issn = {0146-4833},
journal = {arXiv},
month = {dec},
pages = {187--213},
pmid = {23021419},
title = {{Sequence Classification Using Decision Trees}},
url = {http://arxiv.org/abs/1612.08242 http://www.worldscientific.com/doi/abs/10.1142/9789812771728_0012},
year = {2007}
}
@article{Khatib2014,
abstract = {The International Symposium on Experimental Robotics (ISER) is a series\nof bi-annual meetings which are organized in a rotating fashion around\nNorth America, Europe and Asia/Oceania. The goal of ISER is to provide\na forum for research in robotics that focuses on novelty of theoretical\ncontributions validated by experimental results. The meetings are\nconceived to bring together, in a small group setting, researchers\nfrom around the world who are in the forefront of experimental robotics\nresearch. \n\nThis unique reference presents the latest advances across the various\nfields of robotics, with ideas that are not only conceived conceptually\nbut also explored experimentally. It collects robotics contributions\non the current developments and new directions in the field of experimental\nrobotics, which are based on the papers presented at the 12 th ISER\nheld on December 18-21, 2010 in New Delhi and Agra, India.This present\ntwelfth edition of Experimental Robotics edited by Oussama Khatib,\nVijay Kumar and Gaurav Sukhatme offers in its eight-chapter volume\na collection of a broad range of topics in field and human-centered\nrobotics.},
author = {Khatib, Oussama and Kumar, Vijay and Sukhatme, Gaurav},
doi = {10.1007/978-3-642-28572-1},
isbn = {9783642285714},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
title = {{Experimental Robotics: The 12th International Symposium on Experimental Robotics ABC}},
volume = {79},
year = {2014}
}
@techreport{Grisetti2011,
abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state-of-the-art approaches for the specific problems. {\textcopyright} 2011 IEEE.},
author = {K{\"{u}}mmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5979949},
file = {::},
isbn = {9781612843865},
issn = {10504729},
pages = {3607--3613},
title = {{G2o: A general framework for graph optimization}},
year = {2011}
}
@article{Dong2017,
author = {Dong, Jing},
file = {::},
title = {{Gaussian Processes as Continuous-time Trajectory Representations : Applications in SLAM and Motion Planning Discrete time SLAM Downsides :}},
year = {2017}
}
@article{Malherbe2017,
abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional Holder like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
archivePrefix = {arXiv},
arxivId = {1703.02628},
author = {Malherbe, C{\'{e}}dric and Vayatis, Nicolas},
eprint = {1703.02628},
isbn = {9781510855144},
issn = {1938-7228},
journal = {34th International Conference on Machine Learning, ICML 2017},
keywords = {convergence rate,global optimization,lipschitz constant,statistical analysis},
number = {1972},
pages = {3592--3601},
title = {{Global optimization of Lipschitz functions}},
url = {http://arxiv.org/abs/1703.02628},
volume = {5},
year = {2017}
}
@article{BostonDynamics2006,
author = {{Boston Dynamics}},
file = {::;::},
pages = {64},
title = {{LittleDog User Manual}},
year = {2006}
}
@article{Oertel2020,
abstract = {In this paper, we propose to augment image-based place recognition with structural cues. Specifically, these structural cues are obtained using structure-from-motion, such that no additional sensors are needed for place recognition. This is achieved by augmenting the 2D convolutional neural network (CNN) typically used for image-based place recognition with a 3D CNN that takes as input a voxel grid derived from the structure-from-motion point cloud. We evaluate different methods for fusing the 2D and 3D features and obtain best performance with global average pooling and simple concatenation. On the Oxford RobotCar dataset, the resulting descriptor exhibits superior recognition performance compared to descriptors extracted from only one of the input modalities, including state-of-the-art image-based descriptors. Especially at low descriptor dimensionalities, we outperform state-of-the-art descriptors by up to 90%.},
archivePrefix = {arXiv},
arxivId = {2003.00278},
author = {Oertel, Amadeus and Cieslewski, Titus and Scaramuzza, Davide},
doi = {10.1109/lra.2020.3009077},
eprint = {2003.00278},
file = {:home/matias/Documents/Mendeley Desktop/Oertel, Cieslewski, Scaramuzza/2020/Oertel, Cieslewski, Scaramuzza - 2020 - Augmenting Visual Place Recognition With Structural Cues.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
number = {4},
pages = {5534--5541},
title = {{Augmenting Visual Place Recognition With Structural Cues}},
volume = {5},
year = {2020}
}
@article{Parisotto2018,
abstract = {The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment.},
archivePrefix = {arXiv},
arxivId = {1802.06857},
author = {Parisotto, Emilio and Chaplot, Devendra Singh and Zhang, Jian and Salakhutdinov, Ruslan},
doi = {10.1109/CVPRW.2018.00061},
eprint = {1802.06857},
file = {::},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
month = {feb},
pages = {350--359},
title = {{Global pose estimation with an attention-based recurrent network}},
url = {http://arxiv.org/abs/1802.06857},
volume = {2018-June},
year = {2018}
}
@article{Duerer1977,
author = {Duerer, Albrecht},
file = {::},
pages = {1--15},
title = {{Early developments 1}},
year = {1977}
}
@book{Skiena2017,
abstract = {This engaging and clearly written textbook/reference provides a must-have introduction to the rapidly emerging interdisciplinary field of data science. It focuses on the principles fundamental to becoming a good data scientist and the key skills needed to build systems for collecting, analyzing, and interpreting data. The Data Science Design Manual is a source of practical insights that highlights what really matters in analyzing data, and provides an intuitive understanding of how these core concepts can be used. The book does not emphasize any particular programming language or suite of data-analysis tools, focusing instead on high-level discussion of important design principles. This easy-to-read text ideally serves the needs of undergraduate and early graduate students embarking on an "Introduction to Data Science" course. It reveals how this discipline sits at the intersection of statistics, computer science, and machine learning, with a distinct heft and character of its own. Practitioners in these and related fields will find this book perfect for self-study as well. Additional learning tools: Contains "War Stories," offering perspectives on how data science applies in the real world Includes "Homework Problems," providing a wide range of exercises and projects for self-study Provides a complete set of lecture slides and online video lectures at www.data-manual.com Provides "Take-Home Lessons," emphasizing the big-picture concepts to learn from each chapter Recommends exciting "Kaggle Challenges" from the online platform Kaggle Highlights "False Starts," revealing the subtle reasons why certain approaches fail Offers examples taken from the data science television show "The Quant Shop" (www.quant-shop.com). What is Data Science? -- Mathematical Preliminaries -- Data Munging -- Scores and Rankings -- Statistical Analysis -- Visualizing Data -- Mathematical Models -- Linear Algebra -- Linear and Logistic Regression -- Distance and Network Methods -- Machine Learning -- Big Data: Achieving Scale.},
author = {Skiena, Steven S.},
booktitle = {Springer},
doi = {10.1007/978-3-319-55444-0},
file = {:home/matias/Documents/Mendeley Desktop/Skiena/2017/Skiena - 2017 - The data science design manual.pdf:pdf},
isbn = {978-3-319-55443-3},
pages = {1--445},
title = {{The data science design manual}},
url = {http://link.springer.com/10.1007/978-3-319-55444-0},
year = {2017}
}
@article{Niv2009,
abstract = {A wealth of research focuses on the decision-making processes that animals and humans employ when selecting actions in the face of reward and punishment. Initially such work stemmed from psychological investigations of conditioned behavior, and explanations of these in terms of computational models. Increasingly, analysis at the computational level has drawn on ideas from reinforcement learning, which provide a normative framework within which decision-making can be analyzed. More recently, the fruits of these extensive lines of research have made contact with investigations into the neural basis of decision making. Converging evidence now links reinforcement learning to specific neural substrates, assigning them precise computational roles. Specifically, electrophysiological recordings in behaving animals and functional imaging of human decision-making have revealed in the brain the existence of a key reinforcement learning signal, the temporal difference reward prediction error. Here, we first introduce the formal reinforcement learning framework. We then review the multiple lines of evidence linking reinforcement learning to the function of dopaminergic neurons in the mammalian midbrain and to more recent data from human imaging experiments. We further extend the discussion to aspects of learning not associated with phasic dopamine signals, such as learning of goal-directed responding that may not be dopamine-dependent, and learning about the vigor (or rate) with which actions should be performed that has been linked to tonic aspects of dopaminergic signaling. We end with a brief discussion of some of the limitations of the reinforcement learning framework, highlighting questions for future research. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Niv, Yael},
doi = {10.1016/j.jmp.2008.12.005},
file = {:home/matias/Documents/Mendeley Desktop/Niv/2009/Niv - 2009 - Reinforcement learning in the brain.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
number = {3},
pages = {139--154},
publisher = {Elsevier Inc.},
title = {{Reinforcement learning in the brain}},
url = {http://dx.doi.org/10.1016/j.jmp.2008.12.005},
volume = {53},
year = {2009}
}
@article{Ratliff2015,
abstract = {What is it that makes movement around obstacles hard? The answer seems clear: obstacles contort the geometry of the workspace and make it difficult to leverage what we consider easy and intuitive straight-line Cartesian geometry. But is Cartesian motion actually easy? It's certainly well-understood and has numerous applications. But beneath the details of linear algebra and pseudoinverses, lies a non-trivial Riemannian metric driving the solution. Cartesian motion is easy only because the pseudoinverse, our powerhouse tool, correctly represents how Euclidean workspace geometry pulls back into the configuration space. In light of that observation, it reasons that motion through a field of obstacles could be just as easy as long as we correctly account for how those obstacles warp the geometry of the space. This paper explores extending our geometric model of the robot beyond the notion of a Cartesian workspace space to fully model and leverage how geometry changes in the presence of obstacles. Intuitively, impenetrable obstacles form topological holes and geodesics curve around them accordingly. We formalize this intuition and develop a general motion optimization framework called Riemannian Motion Optimization (RieMO) to efficiently find motions using our geometric models. Our experiments demonstrate that, for many problems, obstacle avoidance can be much more natural when placed within the right geometric context.},
author = {Ratliff, Nathan and Toussaint, Marc and Schaal, Stefan},
doi = {10.1109/ICRA.2015.7139778},
file = {:home/matias/Documents/Mendeley Desktop/Ratliff, Toussaint, Schaal/2015/Ratliff, Toussaint, Schaal - 2015 - Understanding the geometry of workspace obstacles in Motion Optimization.pdf:pdf},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {4202--4209},
title = {{Understanding the geometry of workspace obstacles in Motion Optimization}},
volume = {2015-June},
year = {2015}
}
@article{Merel2017,
abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to produce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller.},
archivePrefix = {arXiv},
arxivId = {1707.02201},
author = {Merel, Josh and Tassa, Yuval and TB, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
eprint = {1707.02201},
file = {::},
journal = {arXiv},
title = {{Learning human behaviors from motion capture by adversarial imitation}},
url = {http://arxiv.org/abs/1707.02201},
year = {2017}
}
@inproceedings{Gomez2018,
abstract = {We propose a Visual-SLAM based localization and navigation system for service robots. Our system is built on top of the ORB-SLAM monocular system but extended by the inclusion of wheel odometry in the estimation procedures. As a case study, the proposed system is validated using the Pepper robot, whose short-range LIDARs and RGB-D camera do not allow the robot to self-localize in large environments. The localization system is tested in navigation tasks using Pepper in two different environments: a medium-size laboratory, and a large-size hall.},
archivePrefix = {arXiv},
arxivId = {1811.08414},
author = {G{\~{A}}³mez, Cristopher and Mattamala, Mat{\~{A}}­as and Resink, Tim and Ruiz-del-Solar, Javier},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-27544-0_3},
eprint = {1811.08414},
isbn = {9783030275433},
issn = {16113349},
pages = {32--44},
title = {{Visual SLAM-Based Localization and Navigation for Service Robots: The Pepper Case}},
url = {http://link.springer.com/10.1007/978-3-030-27544-0_3},
volume = {11374 LNAI},
year = {2019}
}
@article{Huang2020,
abstract = {We introduce a novel lidar-monocular visual odometry approach using point and line features. Compared to previous point-only based lidar-visual odometry, our approach leverages more environment structure information by introducing both point and line features into pose estimation. We provide a robust method for point and line depth extraction, and formulate the extracted depth as prior factors for point-line bundle adjustment. This method greatly reduces the features' 3D ambiguity and thus improves the pose estimation accuracy. Besides, we also provide a purely visual motion tracking method and a novel scale correction scheme, leading to an efficient lidar-monocular visual odometry system with high accuracy. The evaluations on the public KITTI odometry benchmark show that our technique achieves more accurate pose estimation than the state-of-the-art approaches, and is sometimes even better than those leveraging semantic information.},
author = {Huang, Shi-sheng and Ma, Ze-yu and Mu, Tai-jiang and Fu, Hongbo and Hu, Shi-min},
file = {:home/matias/Documents/Mendeley Desktop/Huang et al/2020/Huang et al. - 2020 - Lidar-Monocular Visual Odometry using Point and Line Features.pdf:pdf},
isbn = {9781728173955},
pages = {1091--1097},
title = {{Lidar-Monocular Visual Odometry using Point and Line Features}},
year = {2020}
}
@article{Forster2017a,
abstract = {Direct methods for visual odometry (VO) have gained popularity for their capability to exploit information from all intensity gradients in the image. However, low computational speed as well as missing guarantees for optimality and consistency are limiting factors of direct methods, in which established feature-based methods succeed instead. Based on these considerations, we propose a semidirect VO (SVO) that uses direct methods to track and triangulate pixels that are characterized by high image gradients, but relies on proven feature-based methods for joint optimization of structure and motion. Together with a robust probabilistic depth estimation algorithm, this enables us to efficiently track pixels lying on weak corners and edges in environments with little or high-frequency texture. We further demonstrate that the algorithm can easily be extended to multiple cameras, to track edges, to include motion priors, and to enable the use of very large field of view cameras, such as fisheye and catadioptric ones. Experimental evaluation on benchmark datasets shows that the algorithm is significantly faster than the state of the art while achieving highly competitive accuracy.},
author = {Forster, Christian and Zhang, Zichao and Gassner, Michael and Werlberger, Manuel and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2623335},
file = {:home/matias/Documents/Mendeley Desktop/Forster et al/2017/Forster et al. - 2017 - SVO Semidirect Visual Odometry for Monocular and Multicamera Systems.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Robot vision,simultaneous localization and mapping (SLAM)},
number = {2},
pages = {249--265},
publisher = {IEEE},
title = {{SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems}},
volume = {33},
year = {2017}
}
@book{Hastie2009,
abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {The Elements of Statistical Learning},
doi = {10.1007/b94608},
eprint = {arXiv:1011.1669v3},
file = {:home/matias/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman/2009/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
number = {2},
pages = {83--85},
pmid = {15512507},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf http://link.springer.com/10.1007/b94608},
volume = {27},
year = {2009}
}
@article{Churchill2015,
abstract = {This paper is about building maps which not only contain the traditional information useful for localising - such as point features - but also embeds a spatial model of expected localiser performance. This often overlooked second-order information provides vital context when it comes to map use and planning. Our motivation here is to improve the performance of the popular Teach and Repeat paradigm [1] which has been shown to enable truly large-scale field operation. When using the taught route for localisation, it is often assumed the robot is following exactly, or is sufficiently close to, the original path, enabling successful localisation. However, what happens if it is not possible, or not desirable to exactly follow the mapped path? How far off the beaten track can the robot travel before it gets lost? We present an approach for assessing this localisation area around a taught route, which we refer to as the localisation envelope. Using a combination of physical sampling and a Gaussian Process model, we are able to accurately predict the localisation performance at unseen points.},
author = {Churchill, Winston and Tong, Chi Hay and Gurəu, Corina and Posner, Ingmar and Newman, Paul},
doi = {10.1109/ICRA.2015.7139783},
file = {:home/matias/Documents/Mendeley Desktop/Churchill et al/2015/Churchill et al. - 2015 - Know your limits Embedding localiser performance models in teach and repeat maps.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {4238--4244},
title = {{Know your limits: Embedding localiser performance models in teach and repeat maps}},
volume = {2015-June},
year = {2015}
}
@incollection{Zach2014,
abstract = {In this work we address robust estimation in the bundle adjustment procedure. Typically, bundle adjustment is not solved via a generic optimization algorithm, but usually cast as a nonlinear least-squares problem instance. In order to handle gross outliers in bundle adjustment the least-squares formulation must be robustified. We investigate several approaches to make least-squares objectives robust while retaining the least-squares nature to use existing efficient solvers. In particular, we highlight a method based on lifting a robust cost function into a higher dimensional representation, and show how the lifted formulation is efficiently implemented in a Gauss-Newton framework. In our experiments the proposed lifting-based approach almost always yields the best (i.e. lowest) objectives. {\textcopyright} 2014 Springer International Publishing.},
author = {Zach, Christopher},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_50},
isbn = {978-3-319-10602-1; 978-3-319-10601-4},
issn = {16113349},
keywords = {Bundle adjustment,nonlinear least-squares optimization,robust cost function},
number = {PART 5},
pages = {772--787},
title = {{Robust bundle adjustment revisited}},
url = {http://link.springer.com/10.1007/978-3-319-10602-1_50},
volume = {8693 LNCS},
year = {2014}
}
@article{Milford2012,
abstract = {Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these "local best matches". This approach removes the need for global matching performance by the vision front-end - instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100% precision with recall rates of up to 60%. {\textcopyright} 2012 IEEE.},
author = {Milford, Michael J. and Wyeth, Gordon F.},
doi = {10.1109/ICRA.2012.6224623},
file = {:home/matias/Documents/Mendeley Desktop/Milford, Wyeth/2012/Milford, Wyeth - 2012 - SeqSLAM Visual route-based navigation for sunny summer days and stormy winter nights.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1643--1649},
title = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights}},
year = {2012}
}
@article{Kokkinos,
abstract = {In this work we train in an end-to-end manner a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture. Such a network can act like a 'swiss knife' for vision tasks; we call it an "UberNet" to indicate its overarching nature. The main contribution of this work consists in handling challenges that emerge when scaling up to many tasks. We introduce techniques that facilitate (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. This allows us to train in an end-to-end manner a unified CNN architecture that jointly handles (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all tasks in 0.7 seconds on a GPU. Our system will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1609.02132},
author = {Kokkinos, Iasonas},
doi = {10.1109/CVPR.2017.579},
eprint = {1609.02132},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5454--5463},
title = {{UberNet: Training a universal convolutional neural network for Low-, Mid-, and high-level vision using diverse datasets and limited memory}},
volume = {2017-Janua},
year = {2017}
}
@article{Hirose,
abstract = {Humans can robustly follow a visual trajectory defined by a sequence of images (i.e. a video) regardless of substantial changes in the environment or the presence of obstacles. We aim at endowing similar visual navigation capabilities to mobile robots solely equipped with a RGB fisheye camera. We propose a novel probabilistic visual navigation system that learns to follow a sequence of images with bidirectional visual predictions conditioned on possible navigation velocities. By predicting bidirectionally (from start towards goal and vice versa) our method extends its predictive horizon enabling the robot to go around unseen large obstacles that are not visible in the video trajectory. Learning how to react to obstacles and potential risks in the visual field is achieved by imitating human teleoperators. Since the human teleoperation commands are diverse, we propose a probabilistic representation of trajectories that we can sample to find the safest path. Integrated into our navigation system, we present a novel localization approach that infers the current location of the robot based on the virtual predicted trajectories required to reach different images in the visual trajectory. We evaluate our navigation system quantitatively and qualitatively in multiple simulated and real environments and compare to state-of-the-art baselines.Our approach outperforms the most recent visual navigation methods with a large margin with regard to goal arrival rate, subgoal coverage rate, and success weighted by path length (SPL). Our method also generalizes to new robot embodiments never used during training.},
archivePrefix = {arXiv},
arxivId = {2003.09224},
author = {Hirose, Noriaki and Taguchi, Shun and Xia, Fei and Martin-Martin, Roberto and Tahara, Yasumasa and Ishigaki, Masanori and Savarese, Silvio},
eprint = {2003.09224},
file = {:home/matias/Documents/Mendeley Desktop/Hirose et al/2020/Hirose et al. - 2020 - Probabilistic Visual Navigation with Bidirectional Image Prediction.pdf:pdf},
title = {{Probabilistic Visual Navigation with Bidirectional Image Prediction}},
url = {http://arxiv.org/abs/2003.09224},
year = {2020}
}
@article{Handa2012,
abstract = {Higher frame-rates promise better tracking of rapid motion, but advanced real-time vision systems rarely exceed the standard 10-60Hz range, arguing that the computation required would be too great. Actually, increasing frame-rate is mitigated by reduced computational cost per frame in trackers which take advantage of prediction. Additionally, when we consider the physics of image formation, high frame-rate implies that the upper bound on shutter time is reduced, leading to less motion blur but more noise. So, putting these factors together, how are application-dependent performance requirements of accuracy, robustness and computational cost optimised as frame-rate varies? Using 3D camera tracking as our test problem, and analysing a fundamental dense whole image alignment approach, we open up a route to a systematic investigation via the careful synthesis of photorealistic video using ray-tracing of a detailed 3D scene, experimentally obtained photometric response and noise models, and rapid camera motions. Our multi-frame-rate, multi-resolution, multi-light-level dataset is based on tens of thousands of hours of CPU rendering time. Our experiments lead to quantitative conclusions about frame-rate selection and highlight the crucial role of full consideration of physical image formation in pushing tracking performance. {\textcopyright} 2012 Springer-Verlag.},
author = {Handa, Ankur and Newcombe, Richard A. and Angeli, Adrien and Davison, Andrew J.},
doi = {10.1007/978-3-642-33786-4_17},
isbn = {9783642337857},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 7},
pages = {222--235},
title = {{Real-time camera tracking: When is high frame-rate best?}},
volume = {7578 LNCS},
year = {2012}
}
@article{Badrinarayanan2015,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,decoder,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
month = {nov},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://arxiv.org/abs/1505.0729%5Cnhttp://mi.eng.cam.ac.uk/projects/segnet/ http://arxiv.org/abs/1505.00729 http://arxiv.org/abs/1511.00561},
volume = {39},
year = {2017}
}
@inproceedings{Piperakis2016,
abstract = {This article presents a novel state estimation scheme for humanoid robot locomotion using an Extended Kalman Filter (EKF) for fusing encoder, inertial and Foot Sensitive Resistor (FSR) measurements. The filter's model is based on the non-linear Zero Moment Point (ZMP) dynamics and thus, coupling the dynamic behavior in the frontal and the lateral plane. Furthermore, it provides state estimates for variables that are commonly used by walking pattern generators and posture balance controllers, such as the Center of Mass (CoM) and the linear time-varying Divergent Component of Motion (DCM) position and velocity, in the 3-D space. Modeling errors are taken into account as external forces acting on the robot in the acceleration level. In addition, an observability analysis for the non-linear system dynamics and the linearized discrete-time EKF dynamics is presented. Subsequently, by utilizing ground-truth data obtained from a vicon motion capture system with a NAO humanoid robot, we demonstrate the effectiveness and robustness of the proposed scheme contrasted to the linear filters, even in the case where disturbances are introduced to the system. Finally, the proposed approach is implemented and employed for feedback to a real-time posture controller, rendering a NAO robot able to walk on an outdoors inclined pavement.},
author = {Piperakis, Stylianos and Trahanias, Panos},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2016.7803278},
isbn = {9781509047185},
issn = {21640580},
keywords = {Locomotion,Multimodal perception},
month = {nov},
number = {2},
pages = {202--209},
publisher = {IEEE},
title = {{Non-linear ZMP based state estimation for humanoid robot locomotion}},
url = {http://ieeexplore.ieee.org/document/7803278/},
year = {2016}
}
@article{Warren2018,
abstract = {Operating in rough, unstructured terrain is an essential requirement for any truly field-deployable ground robot. Search-and-rescue, border patrol and agricultural work all require operation in environments with little established infrastructure for easy navigation. This presents challenges for sensor-based navigation such as vision, where erratic motion and feature-poor environments test feature tracking and hinder the performance of repeat matching of point features. For vision-based route-following methods such as Visual Teach and Repeat (VTR), maintaining similar visual perspective of salient point features is critical for reliable odometry and accurate localisation over long periods. In this paper, we investigate a potential solution to these challenges by integrating a gimbaled camera with VTR on a Grizzly Robotic Utility Vehicle (RUV) for testing at high speeds and in visually challenging environments. We examine the benefits and drawbacks of using an actively gimbaled camera to attenuate image motion and control viewpoint. We compare the use of a gimbaled camera to our traditional fixed stereo configuration and demonstrate cases of improved performance in Visual Odometry (VO), localisation and path following in several sets of outdoor experiments.},
author = {Warren, Michael and Schoellig, Angela P. and Barfoot, Timothy D.},
doi = {10.1109/ICRA.2018.8460961},
file = {:home/matias/Documents/Mendeley Desktop/Warren, Schoellig, Barfoot/2018/Warren, Schoellig, Barfoot - 2018 - Level-Headed Evaluating Gimbal-Stabilised Visual Teach and Repeat for Improved Localisation Perform.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {7239--7246},
publisher = {IEEE},
title = {{Level-Headed: Evaluating Gimbal-Stabilised Visual Teach and Repeat for Improved Localisation Performance}},
year = {2018}
}
@article{Tron2014,
abstract = {In the last few years there has been a growing interest in optimization methods for averaging pose measurements between a set of cameras or objects (obtained, for instance, using epipolar geometry or pose estimation). Alas, existing approaches do not take into consideration that measurements might have different uncertainties (i.e., the noise might not be isotropically distributed), or that they might be incomplete (e.g., they might be known only up to a rotation around a fixed axis). We propose a Riemannian optimization framework which addresses these cases by using covariance matrices, and test it on synthetic and real data. {\textcopyright} 2014 Springer International Publishing.},
author = {Tron, Roberto and Daniilidis, Kostas},
doi = {10.1007/978-3-319-10602-1_52},
file = {::;::},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Anisotropic filtering,Error propagation,Incomplete measurements,Pose averaging,Riemannian geometry},
number = {PART 5},
pages = {804--819},
title = {{Statistical pose averaging with non-isotropic and incomplete relative measurements}},
volume = {8693 LNCS},
year = {2014}
}
@article{Omitted,
abstract = {Feature descriptors, such as SIFT and ORB, are well-known for their robustness to illumination changes, which has made them popular for feature-based VSLAM\@. However, in degraded imaging conditions such as low light, low texture, blur and specular reflections, feature extraction is often unreliable. In contrast, direct VSLAM methods which estimate the camera pose by minimizing the photometric error using raw pixel intensities are often more robust to low textured environments and blur. Nonetheless, at the core of direct VSLAM is the reliance on a consistent photometric appearance across images, otherwise known as the brightness constancy assumption. Unfortunately, brightness constancy seldom holds in real world applications. In this work, we overcome brightness constancy by incorporating feature descriptors into a direct visual odometry framework. This combination results in an efficient algorithm that combines the strength of both feature-based algorithms and direct methods. Namely, we achieve robustness to arbitrary photometric variations while operating in low-textured and poorly lit environments. Our approach utilizes an efficient binary descriptor, which we call Bit-Planes, and show how it can be used in the gradient-based optimization required by direct methods. Moreover, we show that the squared Euclidean distance between Bit-Planes is equivalent to the Hamming distance. Hence, the descriptor may be used in least squares optimization without sacrificing its photometric invariance. Finally, we present empirical results that demonstrate the robustness of the approach in poorly lit underground environments.},
archivePrefix = {arXiv},
arxivId = {1604.00990},
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
eprint = {1604.00990},
file = {::},
title = {{Direct Visual Odometry using Bit-Planes}},
url = {http://arxiv.org/abs/1604.00990},
year = {2016}
}
@techreport{Toussaint2019,
abstract = {Talked about what Machine Learning is - Reviewed some of the key problems machine learning solves - Reviewed some of the key algorithms - Looked at an example - Any Questions?.},
author = {Pentakalos, Odysseas},
booktitle = {Cmg Impact 2019},
doi = {10.4018/978-1-7998-0414-7.ch003},
file = {:home/matias/Documents/Mendeley Desktop/Pentakalos/2019/Pentakalos - 2019 - Introduction to machine learning.pdf:pdf},
issn = {09521976},
title = {{Introduction to machine learning}},
year = {2019}
}
@article{Vysotska2019,
abstract = {Visual place recognition is a challenging task, especially in outdoor environments as the scenes naturally change their appearance. In this letter, we propose a method for visual place recognition that is able to deal with seasonal changes, different weather condition as well as illumination changes. Our approach localizes the robot in a map, which is represented by multiple image sequences collected in the past at different points in time. Our approach is also able to localize a vehicle in a map generated from Google Street View images. Due to the deployment of an efficient hashing-based image retrieval strategy for finding potential matches in combination with informed search in a data association graph, our approach robustly localizes a robot and quickly relocalizes if it is getting lost. Our experiments suggest that our algorithm is an effective matching approach to align the currently obtained images with multiple trajectories for online operation.},
author = {Vysotska, Olga and Stachniss, Cyrill},
doi = {10.1109/LRA.2019.2897160},
file = {:home/matias/Documents/Mendeley Desktop/Vysotska, Stachniss/2019/Vysotska, Stachniss - 2019 - Effective visual place recognition using multi-sequence maps.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Localization,visual place recognition},
number = {2},
pages = {1730--1736},
publisher = {IEEE},
title = {{Effective visual place recognition using multi-sequence maps}},
volume = {4},
year = {2019}
}
@article{Booij2007,
abstract = {Vision systems are used more and more in 'personal' robots interacting with humans, since semantic information about objects and places can be derived from the rich sensory information. Visual information is also used for building appearance based topological maps, which can be used for localization. In this paper we describe a system capable of using this appearance based topological map for navigation. The system is made robust by using the epipolar geometry and a planar floor constraint in computing the necessary heading information. Using this method the robot is able to drive robustly in a large environment. We tested the method on real data under varying environment conditions and compared performance with a human-controlled robot. {\textcopyright} 2007 IEEE.},
author = {Booij, O. and Terwijn, B. and Zivkovic, Z. and Kr{\"{o}}se, B.},
doi = {10.1109/ROBOT.2007.364081},
file = {:home/matias/Documents/Mendeley Desktop/Booij et al/2007/Booij et al. - 2007 - Navigation using an appearance based topological map.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {April},
pages = {3927--3932},
title = {{Navigation using an appearance based topological map}},
year = {2007}
}
@article{Fabry2014,
author = {Fabry, Johan and Campusano, Miguel},
number = {Dcc},
title = {{Live Programming the Lego Mindstorms}},
year = {2014}
}
@article{Geva-Sagiv2015,
abstract = {Spatial orientation and navigation rely on the acquisition of several types of sensory information. This information is then transformed into a neural code for space in the hippocampal formation through the activity of place cells, grid cells and head-direction cells. These spatial representations, in turn, are thought to guide long-range navigation. But how the representations encoded by these different cell types are integrated in the brain to form a neural 'map and compass' is largely unknown. Here, we discuss this problem in the context of spatial navigation by bats and rats. We review the experimental findings and theoretical models that provide insight into the mechanisms that link sensory systems to spatial representations and to large-scale natural navigation.},
author = {Geva-Sagiv, Maya and Las, Liora and Yovel, Yossi and Ulanovsky, Nachum},
doi = {10.1038/nrn3888},
file = {:home/matias/Documents/Mendeley Desktop/Geva-Sagiv et al/2015/Geva-Sagiv et al. - 2015 - Spatial cognition in bats and rats From sensory acquisition to multiscale maps and navigation.pdf:pdf},
issn = {14710048},
journal = {Nature Reviews Neuroscience},
number = {2},
pages = {94--108},
pmid = {25601780},
title = {{Spatial cognition in bats and rats: From sensory acquisition to multiscale maps and navigation}},
volume = {16},
year = {2015}
}
@article{Niv2011,
abstract = {diss},
author = {Niv, Yael},
file = {:home/matias/Documents/Mendeley Desktop/Niv/2011/Niv - 2011 - Reinforcement learning in the industry.pdf:pdf},
journal = {Learning},
pages = {1--6},
title = {{Reinforcement learning in the industry}},
year = {2011}
}
@inproceedings{Furgale2010a,
abstract = {Visual teach-and-repeat navigation enables long-range rover autonomy without solving the simultaneous localization and mapping problem or requiring an accurate global reconstruction. During a learning phase, the rover is piloted along a route, logging images. After post-processing, the rover is able to repeat the route in either direction any number of times. This paper describes and evaluates the localization algorithm at the core of a teach-and-repeat system that has been tested on over 32 kilometers of autonomous driving in an urban environment and at a planetary analog site in the High Arctic. We show how a stereo visual odometry pipeline can be extended to become a mapping and localization system, then evaluate the performance of the algorithm with respect to accuracy, robustness to path-tracking error, and the effects of lighting. {\textcopyright}2010 IEEE.},
author = {Furgale, Paul and Barfoot, Tim},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509133},
file = {:home/matias/Documents/Mendeley Desktop/Furgale, Barfoot/2010/Furgale, Barfoot - 2010 - Stereo mapping and localization for long-range path following on rough terrain.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
pages = {4410--4416},
title = {{Stereo mapping and localization for long-range path following on rough terrain}},
year = {2010}
}
@article{George2014,
abstract = {In this paper, we put forward a method for humanoid robot indoor localization and navigation, using 2D bar codes, running on embedded hardware. Our approach is based on camera image processing and the detection of 2D bar codes stuck to the walls. The particularities of our approach are: it works without odometry, fast localization (≈1s) that allows for kidnapping and falls, and it does not require the manual edition of a map (thus no extra computer is needed). Results of an experimental study conducted with three different NAO robots in our office show that the proposed system is operational and usable in domestic environments. The NAO robots are able to navigate through a corridor of 5m in under 86s on average. Moreover using our system the robots manage to map and navigate a complex environment with multiple rooms that includes doors and furniture. Taken together our results demonstrate that our navigation system could become a standard feature for our robots.},
author = {George, Laurent and Mazel, Alexandre},
doi = {10.1109/HUMANOIDS.2013.7029995},
isbn = {9781479926176},
issn = {21640580},
journal = {IEEE-RAS International Conference on Humanoid Robots},
keywords = {Aldebaran NAO Navigation},
number = {February},
pages = {329--335},
title = {{Humanoid robot indoor navigation based on 2D bar codes: Application to the NAO robot}},
volume = {2015-Febru},
year = {2015}
}
@article{Barnes2018,
abstract = {We present a self-supervised approach to ignoring 'distractors' in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.},
archivePrefix = {arXiv},
arxivId = {1711.06623},
author = {Barnes, Dan and Maddern, Will and Pascoe, Geoffrey and Posner, Ingmar},
doi = {10.1109/ICRA.2018.8460564},
eprint = {1711.06623},
file = {:home/matias/Documents/Mendeley Desktop/Barnes et al/Unknown/Barnes et al. - Unknown - Driven to Distraction Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Envir.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1894--1900},
title = {{Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments}},
year = {2018}
}
@article{Oishi2019,
abstract = {This paper presents a new approach to view-based localization and navigation in outdoor environments, which are indispensable functions for mobile robots. Several approaches have been proposed for autonomous navigation. GPS-based systems are widely used especially in the case of automobiles, however, they can be unreliable or non-operational near tall buildings. Localization with a precise 3D digital map of the target environment also enables mobile robots equipped with range sensors to estimate accurate poses, but maintaining a large-scale outdoor map is often costly. We have therefore developed a novel view-based localization method SeqSLAM++ by extending the conventional SeqSLAM in order not only to robustly estimate the robot position comparing image sequences but also to cope with changes in a robot's heading and speed as well as view changes using wide-angle images and a Markov localization scheme. According to the direction to move provided by the SeqSLAM++, the local-level path planner navigates the robot by setting subgoals repeatedly considering the structure of the surrounding environment using a 3D LiDAR. The entire navigation system has been implemented in the ROS framework, and the effectiveness and accuracy of the proposed method was evaluated through off-line/on-line navigation experiments.},
author = {Oishi, Shuji and Inoue, Yohei and Miura, Jun and Tanaka, Shota},
doi = {10.1016/j.robot.2018.10.014},
file = {:home/matias/Documents/Mendeley Desktop/Oishi et al/2019/Oishi et al. - 2019 - SeqSLAM View-based robot localization and navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Mobile robot,Navigation,SeqSLAM,View-based localization},
pages = {13--21},
publisher = {Elsevier B.V.},
title = {{SeqSLAM++: View-based robot localization and navigation}},
url = {https://doi.org/10.1016/j.robot.2018.10.014},
volume = {112},
year = {2019}
}
@article{Halodova2019,
abstract = {In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
author = {Halodova, Lucie and Dvorrakova, Eliska and Majer, Filip and Vintr, Tomas and Mozos, Oscar Martinez and Dayoub, Feras and Krajnik, Tomas},
doi = {10.1109/IROS40897.2019.8967994},
file = {:home/matias/Documents/Mendeley Desktop/Halodov{\'{a}} et al/2019/Halodov{\'{a}} et al. - 2019 - Predictive and adaptive maps for long-term visual navigation in changing environments Predictive and adaptive.pdf:pdf},
isbn = {9781728140049},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {January 2020},
pages = {7033--7039},
title = {{Predictive and adaptive maps for long-term visual navigation in changing environments}},
year = {2019}
}
@article{Ronsse2016,
abstract = {Gaze stabilization is a fundamental function for humanoid robots. Stabilizing the image being perceived facilitates the processing and thus the interpretation of visual data. In parallel, fixation should also guarantee that the visual target remains centered in the image. Several approaches exist to address the problem of gaze stabilization: closed-loop algorithms processing the visual data or inferring head movements from kinematic measurements, and feed-forward algorithms anticipating head movements from the lower-body commands. In this contribution, we develop a feed-forward controller addressing both image stabilization and target fixation into a unified framework. The addition of a virtual linkage between the robot eye and the visual target offers to elegantly rephrase the gaze control problem as the classical control of a redundant serial robot manipulator. Furthermore, a novel method to estimate the self-induced optical flow based on the robot kinematics-extended with this virtual linkage-is developed. It is then possible to solve the redundancy (i.e. guaranteeing target fixation) through a minimization of the optical flow (i.e. Achieving image stabilization). This method is validated in simulation with a model of the head of the ARMAR IV humanoid. It is shown that the proposed controller offers to accurately estimate and minimize the optical flow, while keeping the visual target exactly in the center of the image.},
author = {Habra, Timothee and Ronsse, Renaud},
doi = {10.1109/BIOROB.2016.7523616},
file = {::},
isbn = {9781509032877},
issn = {21551774},
journal = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
pages = {163--169},
title = {{Gaze stabilization of a humanoid robot based on virtual linkage}},
volume = {2016-July},
year = {2016}
}
@article{DeJong2020,
abstract = {End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.},
archivePrefix = {arXiv},
arxivId = {2004.09317},
author = {de Jong, D. B. and Paredes-Vall{\'{e}}s, F. and de Croon, G. C. H. E.},
eprint = {2004.09317},
file = {:home/matias/Documents/Mendeley Desktop/de Jong, Paredes-Vall{\'{e}}s, de Croon/2020/de Jong, Paredes-Vall{\'{e}}s, de Croon - 2020 - How Do Neural Networks Estimate Optical Flow A Neuropsychology-Inspired Study.pdf:pdf},
title = {{How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired Study}},
url = {http://arxiv.org/abs/2004.09317},
year = {2020}
}
@inproceedings{Strasdat2010a,
abstract = {State of the art visual SLAM systems have recently been presented which are capable of accurate, large-scale and real-time performance, but most of these require stereo vision. Important application areas in robotics and beyond open up if similar performance can be demonstrated using monocular vision, since a single camera will always be cheaper, more compact and easier to calibrate than a multi-camera rig. With high quality estimation, a single camera moving through a static scene of course effectively provides its own stereo geometry via frames distributed over time. However, a classic issue with monocular visual SLAM is that due to the purely projective nature of a single camera, motion estimates and map structure can only be recovered up to scale. Without the known inter-camera distance of a stereo rig to serve as an anchor, the scale of locally constructed map portions and the corresponding motion estimates is therefore liable to drift over time. In this paper we describe a new near real-time visual SLAM system which adopts the continuous keyframe optimisation approach of the best current stereo systems, but accounts for the additional challenges presented by monocular input. In particular, we present a new pose-graph optimisation technique which allows for the efficient correction of rotation, translation and scale drift at loop closures. Especially, we describe the Lie group of similarity transformations and its relation to the corresponding Lie algebra. We also present in detail the system's new image processing front-end which is able accurately to track hundreds of features per frame, and a filter-based approach for feature initialisation within keyframe-based SLAM. Our approach is proven via large-scale simulation and real-world experiments where a camera completes large looped trajectories.},
author = {Strasdat, Hauke and Montiel, J. M.M. and Davison, Andrew J.},
booktitle = {Robotics: Science and Systems},
doi = {10.1.1.165.7975},
file = {:home/matias/Documents/Mendeley Desktop/Strasdat, Montiel, Davison/2011/Strasdat, Montiel, Davison - 2011 - Scale drift-aware large scale monocular SLAM.pdf:pdf},
isbn = {9780262516815},
issn = {2330765X},
pages = {73--80},
title = {{Scale drift-aware large scale monocular SLAM}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.7975&rep=rep1&type=pdf},
volume = {6},
year = {2011}
}
@article{Stentz2015,
abstract = {We have developed the CHIMP (CMU Highly Intelligent Mobile Platform) robot as a platform for executing complex tasks in dangerous, degraded, human-engineered environments. CHIMP has a near-human form factor, work-envelope, strength, and dexterity to work effectively in these environments. It avoids the need for complex control by maintaining static rather than dynamic stability. Utilizing various sensors embedded in the robot's head, CHIMP generates full three-dimensional representations of its environment and transmits these models to a human operator to achieve latency-free situational awareness. This awareness is used to visualize the robot within its environment and preview candidate free-space motions. Operators using CHIMP are able to select between task, workspace, and joint space control modes to trade between speed and generality. Thus, they are able to perform remote tasks quickly, confidently, and reliably, due to the overall design of the robot and software. CHIMP's hardware was designed, built, and tested over 15 months leading up to the DARPA Robotics Challenge. The software was developed in parallel using surrogate hardware and simulation tools. Over a six-week span prior to the DRC Trials, the software was ported to the robot, the system was debugged, and the tasks were practiced continuously. Given the aggressive schedule leading to the DRC Trials, development of CHIMP focused primarily on manipulation tasks. Nonetheless, our team finished 3rd out of 16.With an upcoming year to develop new software for CHIMP, we look forward to improving the robot's capability and increasing its speed to compete in the DRC Finals.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Stentz, Anthony and Herman, Herman and Kelly, Alonzo and Meyhofer, Eric and Haynes, G. Clark and Stager, David and Zajac, Brian and Bagnell, J. Andrew and Brindza, Jordan and Dellin, Christopher and George, Michael and Gonzalez-Mora, Jose and Hyde, Sean and Jones, Morgan and Laverne, Michel and Likhachev, Maxim and Lister, Levi and Powers, Matt and Ramos, Oscar and Ray, Justin and Rice, David and Scheifflee, Justin and Sidki, Raumi and Srinivasa, Siddhartha and Strabala, Kyle and Tardif, Jean Philippe and Valois, Jean Sebastien and {Vande Weghe}, J. Michael and Wagner, Michael and Wellington, Carl},
doi = {10.1002/rob.21569},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {209--228},
pmid = {22164016},
title = {{CHIMP, the CMU highly intelligent mobile platform}},
url = {http://doi.wiley.com/10.1002/rob.21569},
volume = {32},
year = {2015}
}
@inproceedings{Xinjilefu2012,
abstract = {This paper compares two approaches to designing Kalman Filters for walking systems. The first design uses Linear Inverted Pendulum Model (LIPM) dynamics, and the other design uses a more complete Planar dynamics. The filter based on the simpler LIPM design is more robust to modeling error. The more complex design estimates center of mass height and joint velocities, and tracks horizontal center of mass translation more accurately. We also investigate different ways of handling contact states and using force sensing in state estimation. In the LIPM filter, force sensing is used to determine contact states and tune filter parameters. In the Planar filter, force sensing is used to select the proper measurement equation. {\textcopyright} 2012 IEEE.},
author = {Xinjilefu and Atkeson, Christopher G.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386070},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {3693--3699},
publisher = {IEEE},
title = {{State estimation of a walking humanoid robot}},
year = {2012}
}
@article{Chaplot2018,
abstract = {Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose “Active Neural Localizer”, a fully differentiable neural network that learns to localize accurately and efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to localize accurately while minimizing the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.},
archivePrefix = {arXiv},
arxivId = {1801.08214},
author = {Chaplot, Devendra Singh and Parisotto, Emilio and Salakhutdinov, Ruslan},
eprint = {1801.08214},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {jan},
pages = {1--15},
title = {{Active neural localization}},
url = {https://drive.google.com/file/d/1MWBMXG8nrrH7FyWcO6jmSVILqvA3U6mL/view http://arxiv.org/abs/1801.08214},
year = {2018}
}
@article{Kunstner2019,
abstract = {Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher-unlike the Fisher-does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.},
archivePrefix = {arXiv},
arxivId = {1905.12558},
author = {Kunstner, Frederik and Balles, Lukas and Hennig, Philipp},
eprint = {1905.12558},
file = {:home/matias/Documents/Mendeley Desktop/Kunstner, Balles, Hennig/2019/Kunstner, Balles, Hennig - 2019 - Limitations of the empirical fisher approximation for natural gradient descent.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {3},
pages = {1--20},
title = {{Limitations of the empirical fisher approximation for natural gradient descent}},
volume = {32},
year = {2019}
}
@article{El-Sheimy2008,
abstract = {It is well known that inertial navigation systems can provide high-accuracy position, velocity, and attitude information over short time periods. However, their accuracy rapidly degrades with time. The requirements for an accurate estimation of navigation information necessitate the modeling of the sensors' error components. Several variance techniques have been devised for stochastic modeling of the error of inertial sensors. They are basically very similar and primarily differ in that various signal processings, by way of weighting functions, window functions, etc., are incorporated into the analysis algorithms in order to achieve a particular desired result for improving the model characterizations. The simplest is the Allan variance. The Allan variance is a method of representing the root means square (RMS) randomdrift error as a function of averaging time. It is simple to compute and relatively simple to interpret and understand. The Allan variance method can be used to determine the characteristics of the underlying random processes that give rise to the data noise. This technique can be used to characterize various types of error terms in the inertial-sensor data by performing certain operations on the entire length of data. In this paper, the Allan variance technique will be used in analyzing and modeling the error of the inertial sensors used in different grades of the inertial measurement units. By performing a simple operation on the entire length of data, a characteristic curve is obtained whose inspection provides a systematic characterization of various random errors contained in the inertial-sensor output data. Being a directly measurable quantity, the Allan variance can provide information on the types and magnitude of the various error terms. This paper covers both the theoretical basis for the Allan variance for modeling the inertial sensors' error terms and its implementation in modeling different grades of inertial sensors. {\textcopyright} 2008 IEEE.},
author = {El-Sheimy, Naser and Hou, Haiying and Niu, Xiaoji},
doi = {10.1109/TIM.2007.908635},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Allan variance,Error analysis,Gyroscopes,Inertial navigation,Inertial sensors},
month = {jan},
number = {1},
pages = {140--149},
title = {{Analysis and modeling of inertial sensors using allan variance}},
url = {http://ieeexplore.ieee.org/document/4404126/},
volume = {57},
year = {2008}
}
@article{Galashov2019,
abstract = {Many real world tasks exhibit rich structure that is repeated across different parts of the state space or in time. In this work we study the possibility of leveraging such repeated structure to speed up and regularize learning. We start from the KL regularized expected reward objective which introduces an additional component, a default policy. Instead of relying on a fixed default policy, we learn it from data. But crucially, we restrict the amount of information the default policy receives, forcing it to learn reusable behaviours that help the policy learn faster. We formalize this strategy and discuss connections to information bottleneck approaches and to the variational EM algorithm. We present empirical results in both discrete and continuous action domains and demonstrate that, for certain tasks, learning a default policy alongside the policy can significantly speed up and improve learning.},
archivePrefix = {arXiv},
arxivId = {1905.01240},
author = {Galashov, Alexandre and Jayakumar, Siddhant M. and Hasenclever, Leonard and Tirumala, Dhruva and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech M. and {Whye Teh}, Yee and Pascanu, Razvan and Heess, Nicolas},
eprint = {1905.01240},
file = {:home/matias/Documents/Mendeley Desktop/Galashov et al/2019/Galashov et al. - 2019 - Information asymmetry in KL-regularized RL.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--25},
title = {{Information asymmetry in KL-regularized RL}},
year = {2019}
}
@article{Fraundorfer2012,
abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
author = {Fraundorfer, Friedrich and Scaramuzza, Davide},
doi = {10.1109/MRA.2012.2182810},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
month = {jun},
number = {2},
pages = {78--90},
pmid = {6153423},
title = {{Visual odometry: Part II: Matching, robustness, optimization, and applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6096039 http://www.ncbi.nlm.nih.gov/pubmed/6153423 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6153423},
volume = {19},
year = {2012}
}
@article{Clark2017,
abstract = {In this paper we present an on-manifold sequence-to-sequence learning approach to motion estimation using visual and inertial sensors. It is to the best of our knowledge the first end-to-end trainable method for visual-inertial odometry which performs fusion of the data at an intermediate feature-representation level. Our method has numerous advantages over traditional approaches. Specifically, it eliminates the need for tedious manual synchronization of the camera and IMU as well as eliminating the need for manual calibration between the IMU and camera. A further advantage is that our model naturally and elegantly incorporates domain specific information which significantly mitigates drift. We show that our approach is competitive with state-of-the-art traditional methods when accurate calibration data is available and can be trained to outperform them in the presence of calibration and synchronization errors.},
archivePrefix = {arXiv},
arxivId = {1701.08376},
author = {Clark, Ronald and Wang, Sen and Wen, Hongkai and Markham, Andrew and Trigoni, Niki},
eprint = {1701.08376},
file = {::},
journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
pages = {3995--4001},
title = {{VINet: Visual-inertial odometry as a sequence-to-sequence learning problem}},
year = {2017}
}
@article{Muglikar2020,
abstract = {In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multicamera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.},
archivePrefix = {arXiv},
arxivId = {2003.02247},
author = {Muglikar, Manasi and Zhang, Zichao and Scaramuzza, Davide},
eprint = {2003.02247},
file = {:home/matias/Documents/Mendeley Desktop/Muglikar, Zhang, Scaramuzza/2020/Muglikar, Zhang, Scaramuzza - 2020 - Voxel Map for Visual SLAM.pdf:pdf},
title = {{Voxel Map for Visual SLAM}},
url = {http://arxiv.org/abs/2003.02247},
year = {2020}
}
@article{Fallon2015b,
abstract = {For humanoid robots to fulfill their mobility potential they must demonstrate reliable and efficient locomotion over rugged and irregular terrain. In this paper we present the perception and planning algorithms which have allowed a humanoid robot to use only passive stereo imagery (as opposed to actuating a laser range sensor) to safely plan footsteps to continuously walk over rough and uneven surfaces without stopping. The perception system continuously integrates stereo imagery to build a consistent 3D model of the terrain which is then used by our footstep planner which reasons about obstacle avoidance, kinematic reachability and foot rotation through mixed-integer quadratic optimization to plan the required step positions. We illustrate that our stereo imagery fusion approach can measure the walking terrain with sufficient accuracy that it matches the quality of terrain estimates from LIDAR. To our knowledge this is the first such demonstration of the use of computer vision to carry out general purpose terrain estimation on a locomoting robot - and additionally to do so in continuous motion. A particular integration challenge was ensuring that these two computationally intensive systems operate with minimal latency (below 1 second) to allow re-planning while walking. The results of extensive experimentation and quantitative analysis are also presented. Our results indicate that a laser range sensor is not necessary to achieve locomotion in these challenging situations.},
author = {Fallon, Maurice F. and Marion, Pat and Deits, Robin and Whelan, Thomas and Antone, Matthew and McDonald, John and Tedrake, Russ},
doi = {10.1109/HUMANOIDS.2015.7363465},
isbn = {9781479968855},
issn = {21640580},
journal = {IEEE-RAS International Conference on Humanoid Robots},
keywords = {Laser radar,Legged locomotion,Planning,Robot kinematics,Robot sensing systems,Three-dimensional displays},
pages = {881--888},
title = {{Continuous humanoid locomotion over uneven terrain using stereo fusion}},
volume = {2015-Decem},
year = {2015}
}
@article{Strasdat2010,
abstract = {While the most accurate solution to off-line structure from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform global optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM - also called monocular SLAM (Simultaneous Localisation and Mapping) - have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform the first rigorous analysis of the relative advantages of filtering and sparse optimisation for sequential monocular SLAM. A series of experiments in simulation as well using a real image SLAM system were performed by means of covariance propagation and Monte Carlo methods, and comparisons made using a combined cost/accuracy measure. With some well-discussed reservations, we conclude that while filtering may have a niche in systems with low processing resources, in most modern applications keyframe optimisation gives the most accuracy per unit of computing time. {\textcopyright}2010 IEEE.},
author = {Strasdat, Hauke and Montiel, J. M.M. and Davison, Andrew J.},
doi = {10.1109/ROBOT.2010.5509636},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2657--2664},
pmid = {5509636},
title = {{Real-time monocular SLAM: Why filter?}},
year = {2010}
}
@book{Harari2011,
abstract = {__},
author = {{Luque Agraz}, Diana},
booktitle = {Estudios Sociales. Revista de Alimentaci{\'{o}}n Contempor{\'{a}}nea y Desarrollo Regional},
doi = {10.24836/es.v28i51.502},
isbn = {9873752137},
issn = {0188-4557},
number = {51},
pages = {495},
publisher = {Debate},
title = {{De animales a dioses. Breve historia de la humanidad Yuval Noah Harari}},
volume = {28},
year = {2018}
}
@article{Lotter2017,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
file = {:home/matias/Documents/Mendeley Desktop/Lotter, Kreiman, Cox/2017/Lotter, Kreiman, Cox - 2017 - Deep predictive coding networks for video prediction and unsupervised learning.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
pages = {1--18},
title = {{Deep predictive coding networks for video prediction and unsupervised learning}},
year = {2017}
}
@article{Yoshida2006,
abstract = {The starting of the business was a development of a robotic machine for construction site workers, utilizing advanced technologies of computer, sensor and mechanism. The author wanted to realize a modern construction sites less 3K (Dangerous, Dirty, Heavy, in Japanese; Kiken, Kitanai, Kitsui), developing and introducing robots workable continuously for 24 hours a day. He developed fireproofing spray robots (SSR-1 to 3) to set human workers out of dirty and dusty environment. In Japan, many prototype robots were developed after his introduction. He did not continue developments of construction robots, but took his way to the applications of robotics under hazardous and challenging environments, and finally has reached space robots on the moon. Continuing space robotics, he is now shearing his effort to the robotics on the ground and space. For the ground use, he is planning to realize an intelligent buildings and habitation environment that is more comfortable and secured by robots. His ideal robots are not sophisticated like a humanoid and not for entertainment, but just a useful one. His targets are just machines workable in human territory with minimum components, minimum intelligence, minimum sensors and some capability of communication. His major works on robotics through 30 years are detailed and the lessons he learned are showed.},
author = {Yoshida, Tetsuji},
doi = {10.22260/isarc2006/0037},
file = {::},
isbn = {4990271718},
journal = {2006 Proceedings of the 23rd International Symposium on Robotics and Automation in Construction, ISARC 2006},
keywords = {Construction robot,History,Research and development,Space robot},
pages = {188--193},
title = {{A short history of construction robots research & development in a Japanese company}},
url = {http://www.iaarc.org/publications/fulltext/isarc2006-00037_200605311353.pdf},
volume = {5},
year = {2006}
}
@inproceedings{Piperakis2017,
author = {Piperakis, Stylianos and Trahanias, Panos},
booktitle = {Dynamic Walking},
number = {2},
title = {{Cascade Non-Linear State Estimation for Humanoid Robot Locomotion}},
year = {2017}
}
@inproceedings{Bowling2007,
abstract = {This article presents a method for including impact forces in the analysis of legged robot dynamic performance. This involves examining how well the legged system uses ground contact to produce acceleration of its body when the feet experience impact with the ground; these abilities are referred to as its force and acceleration capabilities and they determine the robot's agility. These capabilities are bounded by actuator torque limits and friction forces. The proposed analysis describes the effect of an impact event on dynamic performance in terms of the coefficient of friction required to prevent slipping of the feet, and the amount of control authority/acceleration which is achievable, at the termination of the impact event. The method is illustrated using a hexapod as an example. {\textcopyright}2007 IEEE.},
author = {Bowling, Alan},
booktitle = {IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM},
doi = {10.1109/AIM.2007.4412406},
file = {::},
isbn = {1424412641},
keywords = {Acceleration,Agility,Dynamic performance,Impact,Impulse,Leg,Locomotion,Mobility},
pages = {1--8},
publisher = {IEEE},
title = {{Impact forces and mobility in legged robot locomotion}},
url = {http://ieeexplore.ieee.org/document/4412406/},
year = {2007}
}
@article{Jepson1991,
abstract = {The image motion field for an observer moving through a static environment depends on the observer's translational and rotational velocities along with the distances to surface points. Given such a motion field as input we present a new algorithm for computing the observer's motion and the depth structure of the scene. The approach is a further development of subspace methods. This class of methods involve splitting the equations describing the motion field into separate equations for the observer's translational direction, the rotational velocity, and the relative depths. The resulting equations can then be solved successively, beginning with the equations for the translational direction. Here we show how this first step can be simplified considerably. The consequence is that the observer's velocity and the relative depths to points in the scene can all be recovered by successively solving three linear problems.},
author = {Jepson, Allan D. and Heeger, David J.},
doi = {10.1109/wvm.1991.212779},
isbn = {0818621532},
journal = {Proceedings of the IEEE Workshop on Visual Motion},
number = {1},
pages = {124--131},
title = {{Fast subspace algorithm for recovering rigid motion}},
year = {1991}
}
@article{Dellaert2006,
abstract = {Solving the SLAM (simultaneous localization and mapping) problem is one way to enable a robot to explore, map, and navigate in a previously unknown environment. Smoothing approaches have been investigated as a viable alternative to extended Kalman filter (EKF)-based solutions to the problem. In particular, approaches have been looked at that factorize either the associated information matrix or the measurement Jacobian into square root form. Such techniques have several significant advantages over the EKF: they are faster yet exact; they can be used in either batch or incremental mode; are better equipped to deal with non-linear process and measurement models; and yield the entire robot trajectory, at lower cost for a large class of SLAM problems. In addition, in an indirect but dramatic way, column ordering heuristics automatically exploit the locality inherent in the geographic nature of the SLAM problem. This paper presents the theory underlying these methods, along with an interpretation of factorization in terms of the graphical model associated with the SLAM problem. Both simulation results and actual SLAM experiments in large-scale environments are presented that underscore the potential of these methods as an alternative to EKF-based approaches.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Dellaert, Frank and Kaess, Michael},
doi = {10.1177/0278364906072768},
eprint = {there is not},
file = {:home/matias/Documents/Mendeley Desktop/Dellaert, Kaess/2006/Dellaert, Kaess - 2006 - Square root SAM Simultaneous localization and mapping via square root information smoothing.pdf:pdf},
isbn = {0278364906072},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Graphical models,Mobile robots,SLAM},
number = {12},
pages = {1181--1203},
pmid = {1638022},
title = {{Square root SAM: Simultaneous localization and mapping via square root information smoothing}},
volume = {25},
year = {2006}
}
@article{Besl1992,
author = {Besl, P.J. and McKay, Neil D.},
doi = {10.1109/34.121791},
file = {:home/matias/Documents/Mendeley Desktop/Besl, McKay/1992/Besl, McKay - 1992 - A method for registration of 3-D shapes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {feb},
number = {2},
pages = {239--256},
title = {{A method for registration of 3-D shapes}},
url = {http://ieeexplore.ieee.org/document/121791/},
volume = {14},
year = {1992}
}
@article{Kitano1997,
abstract = {Location: Web Keywords: Robot soccer senior projects Comments: A web link to the RoboCup initiative.},
author = {Kitano, Hiroaki and Asada, Minoru and Kuniyoshi, Yasuo and Noda, Itsuki and Osawa, Eiichi},
doi = {10.1145/267658.267738},
isbn = {0897918770},
journal = {AGENTS 97 Proceedings of the first international conference on Autonomous agents},
keywords = {robocup,robot soccer},
pages = {340--347},
title = {{RoboCup}},
url = {http://dx.doi.org/http://doi.acm.org/10.1145/267658.267738},
year = {1997}
}
@article{Klein2009,
abstract = {Camera phones are a promising platform for hand-held augmented reality. As their computational resources grow, they are becoming increasingly suitable for visual tracking tasks. At the same time, they still offer considerable challenges: Their cameras offer a narrow field-of-view not best suitable for robust tracking; images are often received at less than 15Hz; long exposure times result in significant motion blur; and finally, a rolling shutter causes severe smearing effects. This paper describes an attempt to implement a keyframe-based SLAMsystem on a camera phone (specifically, the Apple iPhone 3G). We describe a series of adaptations to the Parallel Tracking and Mapping system to mitigate the impact of the device's imaging deficiencies. Early results demonstrate a system capable of generating and augmenting small maps, albeit with reduced accuracy and robustness compared to SLAM on a PC. {\textcopyright}2009 IEEE.},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2009.5336495},
isbn = {9781424453900},
issn = {15534014},
journal = {Science and Technology Proceedings - IEEE 2009 International Symposium on Mixed and Augmented Reality, ISMAR 2009},
pages = {83--86},
pmid = {18717641},
title = {{Parallel tracking and mapping on a camera phone}},
year = {2009}
}
@article{Wang2008,
abstract = {Error propagation on the Euclidean motion group arises in a number of areas such as in dead reckoning errors in mobile robot navigation and joint errors that accumulate from the base to the distal end of kinematic chains such as manipulators and biological macromolecules. We address error propagation in rigid-body poses in a coordinate-free way. In this paper we show how errors propagated by convolution on the Euclidean motion group, SE133, can be approximated to second order using the theory of Lie algebras and Lie groups. We then show how errors that are small (but not so small that linearization is valid) can be propagated by a recursive formula derived here. This formula takes into account errors to second order, whereas prior efforts only considered the first-order case. Our formulation is non-parametric in the sense that it will work for probability density functions of any form (not only Gaussians). Numerical tests demonstrate the accuracy of this second-order theory in the context of a manipulator arm and a flexible needle with bevel tip. {\textcopyright} 2008 SAGE Publications.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Yunfeng and Chirikjian, Gregory S.},
doi = {10.1177/0278364908097583},
eprint = {NIHMS150003},
file = {:home/matias/Documents/Mendeley Desktop/Wang, Chirikjian/2008/Wang, Chirikjian - 2008 - Nonparametric second-order theory of error propagation on motion groups.pdf:pdf},
isbn = {9783540684046},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Euclidean group,Recursive error propagation,Spatial uncertainty.},
month = {nov},
number = {11-12},
pages = {1258--1273},
pmid = {20333324},
publisher = {Springer Berlin Heidelberg},
title = {{Nonparametric second-order theory of error propagation on motion groups}},
url = {http://link.springer.com/10.1007/978-3-540-68405-3_10 http://journals.sagepub.com/doi/10.1177/0278364908097583},
volume = {27},
year = {2008}
}
@techreport{Diepold2008,
abstract = {Bundle adjustment is a minimization method frequently used to refine the structure and motion parameters of a moving camera. In this work, we present a Newton-based approach to enhance the accuracy of the estimated motion parameters in the bundle adjustment framework. The key issue is to parameterize the motion variables of a camera on the manifold of the Euclidean motion by using the underlying Lie group structure of the motion representation. We then reformulate the bundle adjustment cost function and derive the gradient and the Hessian formulation on the manifold to minimize the cost function in such a manner that the minimum is the desired estimate of the motion. This results in a more compact derivation of the Hessian which allows us to use its complete form in the minimization process. Compared to the Levenberg-Marquardt scheme, the proposed algorithm is shown to provide more accurate results while having a comparable complexity although the latter uses an approximate form of the Hessian. The experimental results we performed on simulated and real image sets are evidence that demonstrate our claims.},
author = {Diepold, K},
file = {::},
institution = {Technische Universit{\"{a}}t M{\"{u}}unchen},
pages = {1--18},
title = {{Pose estimation of a camera using Newton optimization on the manifold}},
url = {https://mediatum.ub.tum.de/doc/655931/655931.pdf},
year = {2008}
}
@article{MacTavish2014,
abstract = {Many state-of-the-art approaches to visual place recognition consider matches to each previously visited frame individually. This strategy is at least linear in computational complexity with respect to the number of previous image frames. In the context of long-term autonomy, this is not feasible as the map grows intractably large; the algorithm should operate with sub-linear complexity to handle extremely large scales. This paper takes the first steps in adapting the Fast Appearance-Based Mapping (FAB-MAP) algorithm to operate with sub-linear complexity, removing the reliance on the arbitrary image frame place discretization. In our naive preliminary approach, we achieve reasonable performance on several public datasets, grouping up to 256 frames. These hierarchical frame groupings are an enabling factor for a tree search with logarithmic complexity. The success of this naive method encourages further investigation into a purpose-designed hierarchical representation , accompanied by a logarthmic search algorithm. Additionally, active imaging sensors such as lidar provide data that are robust to external lighting changes. This paper presents novel initial place recognition results on lidar intensity images, and shows that image groups increase performance for these low-quality images. Lidar data are acquired in a continuous scan, making it difficult to use traditional image-based place recognition. The proposed hierarchical place recognition does not depend on a single-image discretization, and is potentially compatible with unstructured lidar without major modification.},
author = {MacTavish, Kirk and Barfoot, Timothy D},
file = {:home/matias/Documents/Mendeley Desktop/MacTavish, Barfoot/2014/MacTavish, Barfoot - 2014 - Towards Hierarchical Place Recognition for Long-Term Autonomy.pdf:pdf},
journal = {Workshop on ICRA},
title = {{Towards Hierarchical Place Recognition for Long-Term Autonomy}},
url = {http://asrl.utias.utoronto.ca/$\sim$tdb/sbib/mactavish_icra14.pdf},
year = {2014}
}
@article{Furgale2011a,
abstract = {Mars represents one of the most important targets for space exploration in the next 10 to 30 years, particularly because of evidence of liquid water in the planet's past. Current environmental conditions dictate that any existing water reserves will be in the form of ice; finding and sampling these ice deposits would further the study of the planet's climate history, further the search for evidence of life, and facilitate in-situ resource utilization during future manned exploration missions. This thesis presents a suite of algorithms to help enable a robotic ice-prospecting mission to Mars. Starting from visual odometry--the estimation of a rover's motion using a stereo camera as the primary sensor--we develop the following extensions: (i) a coupled surface/subsurface modelling system that provides novel data products to scientists working remotely, (ii) an autonomous retrotraverse system that allows a rover to return to previously visited places along a route for sampling, or to return a sample to an ascent vehicle, and (iii) the extension of the appearance-based visual odometry pipeline to an actively illuminated light detection and ranging sensor that provides data similar to a stereo camera but is not reliant on consistent ambient lighting, thereby enabling appearance-based vision techniques to be used in environments that are not conducive to passive cameras, such as underground mines or permanently shadowed craters on the moon. All algorithms are evaluated on real data collected using our field robot at the University of Toronto Institute for Aerospace Studies, or at a planetary analogue site on Devon Island, in the Canadian High Arctic.},
author = {Furgale, Paul Timothy},
file = {:home/matias/Documents/Mendeley Desktop/Furgale, Furgale/2011/Furgale, Furgale - 2011 - Extensions to the Visual Odometry Pipeline for the by Extensions to the Visual Odometry Pipeline for the Explo.pdf:pdf},
isbn = {9780494781852},
issn = {0494781858},
journal = {ProQuest Dissertations and Theses},
keywords = {0538:Aerospace engineering,0771:Robotics,Aerospace engineering,Applied sciences,Planetary surfaces,Robotic missions,Robotics,Space exploration,Visual odometry},
pages = {157},
title = {{Extensions to the Visual Odometry Pipeline for the Exploration of Planetary Surfaces}},
url = {http://ezproxy.net.ucf.edu/login?url=http://search.proquest.com/docview/924428359?accountid=10003%5Cnhttp://sfx.fcla.edu/ucf?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations+&+theses&sid=ProQ:ProQuest+Dissertations+&+T},
volume = {NR78185},
year = {2011}
}
@inproceedings{Camurri2015,
abstract = {We present a real-time SLAM system that combines an improved version of the Iterative Closest Point (ICP) and inertial dead reckoning to localize our dynamic quadrupedal machine in a local map. Despite the strong and fast motions induced by our 80 kg hydraulic legged robot, the SLAM system is robust enough to keep the position error below 5% within the local map that surrounds the robot. The 3D map of the terrain, computed at the camera frame rate is suitable for vision based planned locomotion. The inertial measurements are used before and after the ICP registration, to provide a good initial guess, to correct the output and to detect registration failures which can potentially corrupt the map. The performance in terms of time and accuracy are also doubled by preprocessing the point clouds with a background subtraction prior to performing the ICP alignment. Our local mapping approach, in spite of having a global frame of reference fixed onto the ground, aligns the current map to the body frame, and allows us to push the drift away from the most recent camera scan. The system has been tested on our robot by performing a trot around obstacles and validated against a motion capture system.},
author = {Camurri, Marco and Bazeille, Stephane and Caldwell, Darwin G. and Semini, Claudio},
booktitle = {IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
doi = {10.1109/MFI.2015.7295818},
isbn = {9781479977727},
month = {sep},
pages = {259--264},
publisher = {IEEE},
title = {{Real-time depth and inertial fusion for local SLAM on dynamic legged robots}},
url = {http://ieeexplore.ieee.org/document/7295818/},
volume = {2015-Octob},
year = {2015}
}
@article{Furgale2010,
abstract = {This paper describes a system built to enable long-range rover autonomy using a stereo camera as the only sensor. During a learning phase, the system builds a manifold map of overlapping submaps as it is piloted along a route. Themap is then used for localization as the rover repeats the route autonomously. The use of local submaps allows the rover to faithfully repeat long routes without the need for an accurate global reconstruction. Path following over nonplanar terrain is handled by performing localization in three dimensions and then projecting this down to a local ground plane associated with the current submap to perform path tracking. We have tested this system in an urban area and in a planetary analog setting in the Canadian High Arctic. More than 32 km was covered-99.6% autonomously-with autonomous runs ranging from 45 m to 3.2 km, all without the use of the global positioning system (GPS). Because it enables long-range autonomous behavior in a single command cycle, visual teach and repeat is well suited to planetary applications, such as Mars sample return, in which no GPS is available. {\textcopyright} 2010 Wiley Periodicals, Inc.},
author = {Furgale, Paul and Barfoot, Timothy D.},
doi = {10.1002/rob.20342},
file = {:home/matias/Documents/Mendeley Desktop/Furgale, Barfoot/2010/Furgale, Barfoot - 2010 - Visual teach and repeat for long-range rover autonomy.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {5},
pages = {534--560},
title = {{Visual teach and repeat for long-range rover autonomy}},
volume = {27},
year = {2010}
}
@article{Rastegari2016,
abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16% in top-1 accuracy.},
archivePrefix = {arXiv},
arxivId = {1603.05279},
author = {Gevers, Theo and Smeulders, Arnold},
doi = {10.1007/978-3-319-46493-0},
eprint = {1603.05279},
file = {::},
isbn = {9783319488806},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {binary convolution,binary deep learning,binary neural networks,convolutional neural network,deep learning},
pages = {V},
title = {{Foreword}},
url = {http://arxiv.org/abs/1603.05279},
volume = {9914 LNCS},
year = {2016}
}
@incollection{Bay2006,
abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
address = {Berlin, Heidelberg},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11744023_32},
isbn = {3540338322},
issn = {03029743},
pages = {404--417},
publisher = {Springer Berlin Heidelberg},
title = {{SURF: Speeded up robust features}},
url = {http://dx.doi.org/10.1007/11744023_32 http://link.springer.com/10.1007/11744023_32},
volume = {3951 LNCS},
year = {2006}
}
@article{Es2015,
abstract = {Early work in the field of SLAM asserted that globally metrically consistent maps expressed in a single coordinate frame were necessary for autonomous operation. It has been shown previously that chain-structured and tree-structured optometric maps provide sufficient information for accurate path following. This paper extends this concept to arbitrarily connected graph structures with loop closures. We show that globally inconsistent maps may be treated as a set of locally defined Riemannian manifolds, and that this representation is sufficient for path repetition tasks. We demonstrate smooth path following on an inconsistent optometric map with loop closures, using the existing Visual Teach and Repeat (VT&R) framework for vision-in-the-loop control. Path-tracking errors are maintained within nominal values despite disparities of over 2m between the local and global representations of robot pose. Traversal of large map discontinuities is found to have no adverse effect on robot performance, allowing segments of the map to be repeated in a different order than they were trained.},
author = {Es, Sebastian K.Van and Barfoot, Timothy D.},
doi = {10.1109/CRV.2015.17},
file = {:home/matias/Documents/Mendeley Desktop/Es, Barfoot/2015/Es, Barfoot - 2015 - Being in Two Places at Once Smooth Visual Path Following on Globally Inconsistent Pose Graphs.pdf:pdf},
isbn = {9781479919864},
journal = {Proceedings -2015 12th Conference on Computer and Robot Vision, CRV 2015},
keywords = {SLAM,loop closure,topometric map},
pages = {54--61},
title = {{Being in Two Places at Once: Smooth Visual Path Following on Globally Inconsistent Pose Graphs}},
year = {2015}
}
@article{Forster2016,
abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1512.02363},
author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2597321},
eprint = {1512.02363},
file = {::},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Computer vision,sensor fusion,visual-inertial odometry (VIO)},
number = {1},
pages = {1--21},
pmid = {15523098},
title = {{On-Manifold Preintegration for Real-Time Visual-Inertial Odometry}},
url = {https://www.youtube.com/watch?v=CsJkci5lfco&feature=youtu.be%5Cnhttp://ieeexplore.ieee.org/document/7557075/},
volume = {33},
year = {2017}
}
@article{Engel2018,
abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-And camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-The-Art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {1607.02565},
author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
doi = {10.1109/TPAMI.2017.2658577},
eprint = {1607.02565},
file = {:home/matias/Documents/Mendeley Desktop/Engel, Koltun, Cremers/2018/Engel, Koltun, Cremers - 2018 - Direct Sparse Odometry(2).pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D reconstruction,SLAM,Visual odometry,structure from motion},
number = {3},
pages = {611--625},
pmid = {28422651},
title = {{Direct Sparse Odometry}},
volume = {40},
year = {2018}
}
@article{Hartley2018a,
abstract = {The factor graph framework is a convenient modeling technique for robotic state estimation where states are represented as nodes, and measurements are modeled as factors. When designing a sensor fusion framework for legged robots, one often has access to visual, inertial, joint encoder, and contact sensors. While visual-inertial odometry has been studied extensively in this framework, the addition of a preintegrated contact factor for legged robots has been only recently proposed. This allowed for integration of encoder and contact measurements into existing factor graphs, however, new nodes had to be added to the graph every time contact was made or broken. In this work, to cope with the problem of switching contact frames, we propose a hybrid contact preintegration theory that allows contact information to be integrated through an arbitrary number of contact switches. The proposed hybrid modeling approach reduces the number of required variables in the nonlinear optimization problem by only requiring new states to be added alongside camera or selected keyframes. This method is evaluated using real experimental data collected from a Cassie-series robot where the trajectory of the robot produced by a motion capture system is used as a proxy for ground truth. The evaluation shows that inclusion of the proposed preintegrated hybrid contact factor alongside visual-inertial navigation systems improves estimation accuracy as well as robustness to vision failure, while its generalization makes it more accessible for legged platforms.},
archivePrefix = {arXiv},
arxivId = {1803.07531},
author = {Hartley, Ross and Jadidi, Maani Ghaffari and Gan, Lu and Huang, Jiunn Kai and Grizzle, Jessy W. and Eustice, Ryan M.},
doi = {10.1109/IROS.2018.8593801},
eprint = {1803.07531},
file = {::},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3783--3790},
title = {{Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation Using Factor Graphs}},
url = {http://arxiv.org/abs/1803.07531},
year = {2018}
}
@article{Lobos-Tsunekawa2018,
author = {Lobos-Tsunekawa, Kenzo and Leiva, Francisco and Ruiz-del-Solar, Javier},
doi = {10.1109/LRA.2018.2851148},
file = {:home/matias/Documents/Mendeley Desktop/Lobos-Tsunekawa, Leiva, Ruiz-del-Solar/2018/Lobos-Tsunekawa, Leiva, Ruiz-del-Solar - 2018 - Visual Navigation for Biped Humanoid Robots Using Deep Reinforcement Learning.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
keywords = {inverted pendulum,reinforcement learning,swinging-up control},
month = {oct},
number = {4},
pages = {3247--3254},
publisher = {IEEE},
title = {{Visual Navigation for Biped Humanoid Robots Using Deep Reinforcement Learning}},
url = {https://ieeexplore.ieee.org/document/8398461/},
volume = {3},
year = {2018}
}
@article{Farboud-Sheshdeh2014,
abstract = {Stereo visual odometry (VO) is a common technique for estimating a camera's motion, features are tracked across frames and the pose change is subsequently inferred. This position estimation method can play a particularly important role in environments in which the global positioning system (GPS) is not available (e.g., Mars rovers). Recently, some authors have noticed a bias in VO position estimates that grows with distance travelled, this can cause the resulting position estimate to become highly inaccurate. The goals of this paper are (i) to investigate the nature of this bias in VO, (ii) to propose methods of estimating it, and (iii) to provide a correction that can potentially be used online. We identify two effects at play in stereo VO bias: first, the inherent bias in the maximum-likelihood estimation framework, and second, the disparity threshold used to discard far-away and erroneous stereo observations. In order to estimate the bias, we investigate three methods: Monte Carlo sampling, the sigma-point method (with modification), and an existing analytical method in the literature. Based on simulations, we show that our new sigma point method achieves similar accuracy to Monte Carlo, but at a fraction of the computational cost. Finally, we develop a bias correction algorithm by adapting the idea of the bootstrap in statistics, and demonstrate that our bias correction algorithm is capable of reducing approximately 95% of bias in VO problems without incorporating other sensors into the setup. {\textcopyright} 2014 IEEE.},
author = {Farboud-Sheshdeh, Sara and Barfoot, Timothy D. and Kwong, Raymond H.},
doi = {10.1109/CRV.2014.10},
file = {:home/matias/Documents/Mendeley Desktop/Farboud-Sheshdeh, Barfoot, Kwong/2014/Farboud-Sheshdeh, Barfoot, Kwong - 2014 - Towards estimating bias in stereo visual odometry.pdf:pdf},
isbn = {9781479943388},
journal = {Proceedings - Conference on Computer and Robot Vision, CRV 2014},
keywords = {Motion and Path Planning,Visual Navigation},
pages = {8--15},
publisher = {IEEE},
title = {{Towards estimating bias in stereo visual odometry}},
year = {2014}
}
@article{Sibley2010,
abstract = {In this paper we describe a relative approach to simultaneous localization and mapping, based on the insight that a continuous relative representation can make the problem tractable at large scales. First, it is well known that bundle adjustment is the optimal non-linear least-squares formulation for this problem, in that its maximum-likelihood form matches the definition of the Cramerĝ€"Rao lower bound. Unfortunately, computing the maximum-likelihood solution is often prohibitively expensive: this is especially true during loop closures, which often necessitate adjusting all parameters in a loop. In this paper we note that it is precisely the choice of a single privileged coordinate frame that makes bundle adjustment costly, and that this expense can be avoided by adopting a completely relative approach. We derive a new relative bundle adjustment which, instead of optimizing in a single Euclidean space, works in a metric space defined by a manifold. Using an adaptive optimization strategy, we show experimentally that it is possible to solve for the full maximum-likelihood solution incrementally in constant time, even at loop closure. Our approach is, by definition, everywhere locally Euclidean, and we show that the local Euclidean estimate matches that of traditional bundle adjustment. Our system operates online in realtime using stereo data, with fast appearance-based loop closure detection. We show results on over 850,000 images that indicate the accuracy and scalability of the approach, and process over 330 GB of image data into a relative map covering 142 km of Southern England. To demonstrate a baseline sufficiency for navigation, we show that it is possible to find shortest paths in the relative maps we build, in terms of both time and distance. Query images from the web of popular landmarks around London, such as the London Eye or Trafalgar Square, are matched to the relative map to provide route planning goals. {\textcopyright} The Author(s), 2010.},
author = {Sibley, Gabe and Mei, Christopher and Reid, Ian and Newman, Paul},
doi = {10.1177/0278364910369268},
file = {::},
isbn = {0278364910369},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {bundle adjustment,robotics,stereo mapping,visual SLAM},
number = {8},
pages = {958--980},
title = {{Vast-scale outdoor navigation using adaptive relative bundle adjustment}},
volume = {29},
year = {2010}
}
@article{Rouxel2016,
abstract = {Odometry is an important element for the localization of mobile robots. For humanoid robots, it is very prone to integration errors, due to mechanical complexity, uncertainties and foot/ground contacts. Most of the time, a visual odometry is then used to encompass these problems. In this work we propose a method to compensate for odometry drifting using machine learning on a small size low-cost humanoid without vision. This method is tested on different ground conditions and exhibits a significant improvement in odometry accuracy.},
author = {Rouxel, Quentin and Passault, Gr{\'{e}}goire and Hofer, Ludovic and N'Guyen, Steve and Ly, Olivier},
doi = {10.1109/ICRA.2016.7487326},
file = {::},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1810--1816},
title = {{Learning the odometry on a small humanoid robot}},
volume = {2016-June},
year = {2016}
}
@article{Behan2009,
author = {Behan, Thomas and Liao, Ziayi and Zhao, Lian},
doi = {10.5772/7426},
journal = {Recent Advances in Technologies},
title = {{Integer Neural Networks On Embedded Systems}},
year = {2009}
}
@article{Hertzberg2013,
abstract = {Common estimation algorithms, such as least squares estimation or the Kalman filter, operate on a state in a state space S that is represented as a real-valued vector. However, for many quantities, most notably orientations in 3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a vector space locally but has a more complex global topological structure. For integrating these quantities, several ad hoc approaches have been proposed. Here, we present a principled solution to this problem where the structure of the manifold S is encapsulated by two operators, state displacement: S× Rn→S and its inverse:S×S→ Rn. These operators provide a local vector-space view $\delta$ x $\delta$ around a given state x. Generic estimation algorithms can then work on the manifold S mainly by replacing +/- with / where appropriate. We analyze these operators axiomatically, and demonstrate their use in least-squares estimation and the Unscented Kalman Filter. Moreover, we exploit the idea of encapsulation from a software engineering perspective in the Manifold Toolkit, where the / operators mediate between a "flat-vector" view for the generic algorithm and a "named-members" view for the problem specific functions. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1107.1119},
author = {Hertzberg, Christoph and Wagner, Ren{\'{e}} and Frese, Udo and Schr{\"{o}}der, Lutz},
doi = {10.1016/j.inffus.2011.08.003},
eprint = {1107.1119},
issn = {15662535},
journal = {Information Fusion},
keywords = {3D orientation,Boxplus-method,Estimation,Least squares,Manifold,Manifold toolkit,Unscented Kalman Filter},
number = {1},
pages = {57--77},
publisher = {Elsevier B.V.},
title = {{Integrating generic sensor fusion algorithms with sound state representations through encapsulation of manifolds}},
url = {http://dx.doi.org/10.1016/j.inffus.2011.08.003},
volume = {14},
year = {2013}
}
@article{Maddern2017,
author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
doi = {10.1177/0278364916679498},
file = {:home/matias/Documents/Mendeley Desktop/Maddern et al/2017/Maddern et al. - 2017 - 1 year, 1000 km The Oxford RobotCar dataset.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {jan},
number = {1},
pages = {3--15},
title = {{1 year, 1000 km: The Oxford RobotCar dataset}},
url = {http://journals.sagepub.com/doi/10.1177/0278364916679498},
volume = {36},
year = {2017}
}
@phdthesis{Pretto2009a,
author = {Pretto, Alberto},
school = {Universt{\`{a}} di Padova},
title = {{Visual-SLAM for Humanoid Robots}},
year = {2009}
}
@article{Barfoot2020,
abstract = {This short note reviews so-called Natural Gradient Descent (NGD) for multivariate Gaussians. The Fisher Information Matrix (FIM) is derived for several different parameterizations of Gaussians. Careful attention is paid to the symmetric nature of the covariance matrix when calculating derivatives. We show that there are some advantages to choosing a parameterization comprising the mean and inverse covariance matrix and provide a simple NGD update that accounts for the symmetric (and sparse) nature of the inverse covariance matrix.},
archivePrefix = {arXiv},
arxivId = {2001.10025},
author = {Barfoot, Timothy D.},
eprint = {2001.10025},
file = {:home/matias/Documents/Mendeley Desktop/Barfoot/2020/Barfoot - 2020 - Multivariate Gaussian Variational Inference by Natural Gradient Descent.pdf:pdf},
number = {2},
title = {{Multivariate Gaussian Variational Inference by Natural Gradient Descent}},
url = {http://arxiv.org/abs/2001.10025},
year = {2020}
}
@article{Ibrahim2015,
abstract = {Localization and tracking of resources on construction jobsites is an emerging area where the location of materials, labour, and equipment is used to estimate productivity, measure project's progress and/or enhance jobsite safety. GPS has been widely used for outdoor tracking of construction operations. However, GPS is not suitable for indoor applications due to the lack of signal coverage; particularly inside tunnels or buildings. Several indoor localization research had been attempted, however such developments rely heavily on extensive external communication network infrastructures. These developments also are susceptible to electromagnetic interference in noisy construction jobsites. This paper presents indoor localization system using a microcontroller equipped with an inertial measurement unit (IMU). The IMU contains a cluster of sensors: accelerometer, gyroscope and magnetometer. The microcontroller uses a direct cosine matrix algorithm to fuse sensors data and calculate non-gravitational acceleration using nine-degrees-of-freedom motion equations. Current position is calculated based on measured acceleration and heading, while accounting for growing error in speed estimation utilizing jerk integration algorithm. Experimental results are presented to illustrate the relative effectiveness of the developed system, which is able to operate independently of any external aids and visibility conditions.},
author = {Ibrahim, Magdy and Moselhi, Osama},
doi = {10.22260/isarc2015/0059},
file = {::},
journal = {Proceedings of the 32nd International Symposium on Automation and Robotics in Construction and Mining (ISARC 2015)},
keywords = {automated progress reporting,imu,indoor localization},
number = {1},
title = {{IMU-Based Indoor Localization for Construction Applications}},
year = {2017}
}
@inproceedings{Liu2017,
abstract = {In this demo, we present RKSLAM, a robust keyframe-based monocular SLAM system that can reliably handle fast motion with strong rotation and ensure good AR experiences. We contribute two key technical contributions: a novel multi-homography based feature tracking method which is very robust and efficient, and a sliding-window based camera pose optimization scheme which imposes the motion prior constraints between consecutive frames through simulated or real IMU data. Based on RKSLAM, we develop an AR App on a mobile device, which allows the user to freely insert 3D furniture models into the scene to see the AR effect without imagination.},
author = {Liu, Haomin and Zhang, Guofeng and Bao, Hujun},
booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
doi = {10.1109/ISMAR-Adjunct.2016.0111},
isbn = {9781509037407},
keywords = {SLAM,augmented reality,mapping,multiple homography representation,tracking},
pages = {340--341},
title = {{Robust Keyframe-Based Monocular SLAM for Augmented Reality}},
year = {2017}
}
@article{Zucker2015,
abstract = {We present a general system with a focus on addressing three events of the 2013 DARPA Robotics Challenge (DRC) trials: debris clearing, door opening, and wall breaking. Our hardware platform is DRC-HUBO, a redesigned model of the HUBO2+ humanoid robot developed by KAIST and Rainbow, Inc. Our system allowed a trio of operators to coordinate a 32 degree-of-freedom robot on a variety of complex mobile manipulation tasks using a single, unified approach. In addition to descriptions of the hardware and software, and results as deployed on the DRC-HUBO platform, we present some qualitative analysis of lessons learned from this demanding and difficult challenge.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Zucker, Matt and Joo, Sungmoon and Grey, Michael X. and Rasmussen, Christopher and Huang, Eric and Stilman, Michael and Bobick, Aaron},
doi = {10.1002/rob.21570},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {3},
pages = {336--351},
pmid = {22164016},
title = {{A General-purpose System for Teleoperation of the DRC-HUBO Humanoid Robot}},
volume = {32},
year = {2015}
}
@article{Rotella2014,
abstract = {This paper introduces a framework for state estimation on a humanoid robot platform using only common proprioceptive sensors and knowledge of leg kinematics. The presented approach extends that detailed in prior work on a point-foot quadruped platform by adding the rotational constraints imposed by the humanoid's flat feet. As in previous work, the proposed Extended Kalman Filter accommodates contact switching and makes no assumptions about gait or terrain, making it applicable on any humanoid platform for use in any task. A nonlinear observability analysis is performed on both the point-foot and flat-foot filters and it is concluded that the addition of rotational constraints significantly simplifies singular cases and improves the observability characteristics of the system. Results on a simulated walking dataset demonstrate the performance gain of the flat-foot filter as well as confirm the results of the presented observability analysis.},
archivePrefix = {arXiv},
arxivId = {1402.5450},
author = {Rotella, Nicholas and Bloesch, Michael and Righetti, Ludovic and Schaal, Stefan},
doi = {10.1109/IROS.2014.6942674},
eprint = {1402.5450},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Extended Kalman Filter,Foot,Kalman filters,Legged locomotion,Noise,Quaternions,State estimation,Vectors,humanoid flat feet,humanoid robot platform,humanoid robots,leg kinematics,nonlinear control systems,nonlinear filters,nonlinear observability analysis,observability,observability analysis,point foot quadruped platform,proprioceptive sensors,robot kinematics,rotational constraints,sensors,simulated walking dataset,state estimation},
pages = {952--958},
title = {{State estimation for a humanoid robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6942674},
year = {2014}
}
@book{J.Higham2002,
author = {Higham, Nicholas J.},
booktitle = {Accuracy and Stability of Numerical Algorithms},
doi = {10.1137/1.9780898718027},
isbn = {0898715210},
number = {July},
pages = {1--656},
publisher = {SIAM},
title = {{Accuracy and stability of numerical algorithms}},
year = {2002}
}
@article{Hinton2011,
abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain. {\textcopyright} 2011 Springer-Verlag.},
author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
doi = {10.1007/978-3-642-21735-7_6},
file = {::},
isbn = {9783642217340},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Invariance,auto-encoder,shape representation},
number = {PART 1},
pages = {44--51},
title = {{Transforming auto-encoders}},
volume = {6791 LNCS},
year = {2011}
}
@misc{,
title = {{Trucco, Alessandro Verri-Introductory techniques for 3-D computer vision-Prentice Hall (1998).pdf}}
}
@article{Behrens2018,
abstract = {It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition. Behrens et al. review an emerging field building formalisms for understanding the neural basis of flexible behavior. The authors extend these ideas toward representations useful for generalization and structural abstraction, allowing rapid inferences and flexible behavior with little direct experience.},
author = {Behrens, Timothy E.J. and Muller, Timothy H. and Whittington, James C.R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L. and Kurth-Nelson, Zeb},
doi = {10.1016/j.neuron.2018.10.002},
file = {:home/matias/Documents/Mendeley Desktop/Behrens et al/2018/Behrens et al. - 2018 - What Is a Cognitive Map Organizing Knowledge for Flexible Behavior.pdf:pdf},
issn = {10974199},
journal = {Neuron},
keywords = {Cognitive Map,Decision Making,Generalization,Hippocampal Formation,Inference,Prefrontal Cortex,Reinforcement Learning,Spatial Cognition,Statistical Learning,Structure Learning},
number = {2},
pages = {490--509},
pmid = {30359611},
publisher = {Elsevier Inc.},
title = {{What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior}},
url = {https://doi.org/10.1016/j.neuron.2018.10.002},
volume = {100},
year = {2018}
}
@article{Mescheder2017,
abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
archivePrefix = {arXiv},
arxivId = {1705.10461},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1705.10461},
file = {:home/matias/Documents/Mendeley Desktop/Mescheder, Nowozin, Geiger/2017/Mescheder, Nowozin, Geiger - 2017 - The numerics of GANs.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {1826--1836},
title = {{The numerics of GANs}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Niko2015,
abstract = {Place recognition has long been an incompletely solved problem in that all approaches involve significant compromises. Current methods address many but never all of the critical challenges of place recognition - viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf. We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-the-art techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.},
author = {S{\"{u}}nderhauf, Niko and Shirazi, Sareh and Jacobson, Adam and Dayoub, Feras and Pepperell, Edward and Upcroft, Ben and Milford, Michael},
booktitle = {Robotics: Science and Systems},
doi = {10.15607/RSS.2015.XI.022},
file = {:home/matias/Documents/Mendeley Desktop/S{\"{u}}nderhauf et al/2015/S{\"{u}}nderhauf et al. - 2015 - Place recognition with convnet landmarks Viewpoint-robust, condition-robust, training-free.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
title = {{Place recognition with convnet landmarks: Viewpoint-robust, condition-robust, training-free}},
volume = {11},
year = {2015}
}
@article{Mei2010,
abstract = {This paper proposes a new topo-metric representation of the world based on co-visibility that simplifies data association and improves the performance of appearance-based recognition. We introduce the concept of dynamic bag-of-words, which is a novel form of query expansion based on finding cliques in the landmark co-visibility graph. The proposed approach avoids the - often arbitrary - discretisation of space from the robot's trajectory that is common to most image-based loop closure algorithms. Instead we show that reasoning on sets of co-visible landmarks leads to a simple model that out-performs pose-based or view-based approaches. Using real and simulated imagery, we demonstrate that dynamic bag-of-words query expansion can improve precision and recall for appearance-based localisation. {\textcopyright}2010 IEEE.},
author = {Mei, Christopher and Sibley, Gabe and Newman, Paul},
doi = {10.1109/IROS.2010.5652266},
file = {:home/matias/Documents/Mendeley Desktop/Mei, Sibley, Newman/2010/Mei, Sibley, Newman - 2010 - Closing loops without places.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
month = {oct},
pages = {3738--3744},
publisher = {IEEE},
title = {{Closing loops without places}},
url = {http://ieeexplore.ieee.org/document/5652266/},
year = {2010}
}
@inproceedings{Kummerle2011a,
abstract = {The calibration parameters of a mobile robot play a substantial role in navigation tasks. Often these parameters are subject to variations that depend either on environmental changes or on the wear of the devices. In this paper, we propose an approach to simultaneously estimate a map of the environment, the position of the on-board sensors of the robot, and its kinematic parameters. Our method requires no prior knowledge about the environment and relies only on a rough initial guess of the platform parameters. The proposed approach performs on-line estimation of the parameters and it is able to adapt to non-stationary changes of the configuration. We tested our approach in simulated environments and on a wide range of real world data using different types of robotic platforms. {\textcopyright} 2011 IEEE.},
author = {K{\"{u}}mmerle, Rainer and Grisetti, Giorgio and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048393},
isbn = {9781612844541},
issn = {2153-0858},
month = {sep},
pages = {3716--3721},
publisher = {IEEE},
title = {{Simultaneous calibration, localization, and mapping}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6048393},
year = {2011}
}
@article{Intel2019,
author = {Intel},
file = {:home/matias/Documents/Mendeley Desktop/Intel/2019/Intel - 2019 - Intel{\textregistered} RealSense™ Camera D400 series Product Family Datasheet Rev. 012019.pdf:pdf},
number = {January},
title = {{Intel{\textregistered} RealSense™ Camera D400 series Product Family Datasheet Rev. 01/2019}},
url = {https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/Intel-RealSense-D400-Series-Datasheet.pdf},
year = {2019}
}
@article{DeCroon2009,
abstract = {Active vision models can simplify visual tasks, provided that they can select sensible actions given incoming sensory inputs. Many active vision models have been proposed, but a comparative evaluation of these models is lacking. We present a comparison of active vision models from two different approaches. The "probabilistic approach" is an approach in which state estimation is the central goal. The "behavioural approach" is an approach that does not divide the vision process in a state estimation and an acting phase. We identify different types of models of the probabilistic approach, and introduce a model inspired on the behavioural approach. We describe these types of models in a common framework and evaluate their performances on a task of viewpoint selection for the classification of three-dimensional objects. The experimental results reveal how the performances of the active vision models relate to each other. For example, the behavioural model performs as good as the best model from the probabilistic approach. Overall, the experimental results reveal relations between the usefulness of active vision, the number of objects involved in the classification task, and the richness of the visual observations of the models. We conclude that research on active vision should aim at reaching a deeper understanding of these relations by applying active vision models to more complex and real-world tasks. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {de Croon, G. C.H.E. and Sprinkhuizen-Kuyper, I. G. and Postma, E. O.},
doi = {10.1016/j.imavis.2008.06.004},
file = {:home/matias/Documents/Mendeley Desktop/de Croon, Sprinkhuizen-Kuyper, Postma/2009/de Croon, Sprinkhuizen-Kuyper, Postma - 2009 - Comparing active vision models.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Active vision,Behavioural approach,Probabilistic approach},
number = {4},
pages = {374--384},
publisher = {Elsevier B.V.},
title = {{Comparing active vision models}},
url = {http://dx.doi.org/10.1016/j.imavis.2008.06.004},
volume = {27},
year = {2009}
}
@article{Rebecq2017b,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the output is composed of a sequence of asynchronous events rather than actual intensity images, traditional vision algorithms cannot be applied, so that a paradigm shift is needed. We introduce the problem of event-based multi-view stereo (EMVS) for event cameras and propose a solution to it. Unlike traditional MVS methods, which address the problem of estimating dense 3D structure from a set of known viewpoints, EMVS estimates semi-dense 3D structure from an event camera with known trajectory. Our EMVS solution elegantly exploits two inherent properties of an event camera: (1) its ability to respond to scene edges—which naturally provide semi-dense geometric information without any pre-processing operation—and (2) the fact that it provides continuous measurements as the sensor moves. Despite its simplicity (it can be implemented in a few lines of code), our algorithm is able to produce accurate, semi-dense depth maps, without requiring any explicit data association or intensity estimation. We successfully validate our method on both synthetic and real data. Our method is computationally very efficient and runs in real-time on a CPU.},
author = {Rebecq, Henri and Gallego, Guillermo and Mueggler, Elias and Scaramuzza, Davide},
doi = {10.1007/s11263-017-1050-6},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {3D reconstruction,Event cameras,Event-based vision,Multi-view stereo},
number = {12},
pages = {1394--1414},
publisher = {Springer US},
title = {{EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time}},
volume = {126},
year = {2018}
}
@misc{FrancescoBullo20,
author = {Goodwine, Bill},
booktitle = {IEEE Transactions on Automatic Control},
doi = {10.1109/TAC.2005.860277},
file = {::},
issn = {15582523},
number = {12},
pages = {2111},
publisher = {Springer},
title = {{Geometric Control of Mechanical Systems}},
volume = {50},
year = {2005}
}
@article{Kataoka2017,
abstract = {The paper gives futuristic challenges disscussed in the cvpaper.challenge. In 2015 and 2016, we thoroughly study 1,600+ papers in several conferences/journals such as CVPR/ICCV/ECCV/NIPS/PAMI/IJCV.},
archivePrefix = {arXiv},
arxivId = {1707.06436},
author = {Kataoka, Hirokatsu and Shirakabe, Soma and He, Yun and Ueta, Shunya and Suzuki, Teppei and Abe, Kaori and Kanezaki, Asako and Morita, Shin'ichiro and Yabe, Toshiyuki and Kanehara, Yoshihiro and Yatsuyanagi, Hiroya and Maruyama, Shinya and Takasawa, Ryosuke and Fuchida, Masataka and Miyashita, Yudai and Okayasu, Kazushige and Matsuzaki, Yuta},
eprint = {1707.06436},
file = {::},
number = {July},
pages = {1--39},
title = {{cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600 Papers Survey}},
url = {http://arxiv.org/abs/1707.06436},
year = {2017}
}
@article{Davison2018,
abstract = {We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic `Spatial AI' perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or comsumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.},
archivePrefix = {arXiv},
arxivId = {1803.11288},
author = {Davison, Andrew J.},
eprint = {1803.11288},
file = {::},
title = {{FutureMapping: The Computational Structure of Spatial AI Systems}},
url = {http://arxiv.org/abs/1803.11288},
year = {2018}
}
@inproceedings{Platinsky,
abstract = {Real-time monocular SLAM is increasingly mature and entering commercial products. However, there is a divide between two techniques providing similar performance. Despite the rise of 'dense' and 'semi-dense' methods which use large proportions of the pixels in a video stream to estimate motion and structure via alternating estimation, they have not eradicated feature-based methods which use a significantly smaller amount of image information from keypoints and retain a more rigorous joint estimation framework. Dense methods provide more complete scene information, but in this paper we focus on how the amount of information and different optimisation methods affect the accuracy of local motion estimation (monocular visual odometry). This topic becomes particularly relevant after the recent results from a direct sparse system. We propose a new method for fairly comparing the accuracy of SLAM frontends in a common setting. We suggest computational cost models for an overall comparison which indicates that there is relative parity between the approaches at the settings allowed by current serial processors when evaluated under equal conditions.},
author = {Platinsky, Lukas and Davison, Andrew J. and Leutenegger, Stefan},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989599},
isbn = {9781509046331},
issn = {10504729},
month = {may},
pages = {5126--5133},
publisher = {IEEE},
title = {{Monocular visual odometry: Sparse joint optimisation or dense alternation?}},
url = {https://www.doc.ic.ac.uk/$\sim$sleutene/publications/platinskyICRA2017.pdf http://ieeexplore.ieee.org/document/7989599/},
year = {2017}
}
@article{Detone2018,
abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
archivePrefix = {arXiv},
arxivId = {1712.07629},
author = {Detone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
doi = {10.1109/CVPRW.2018.00060},
eprint = {1712.07629},
file = {:home/matias/Documents/Mendeley Desktop/Detone, Malisiewicz, Rabinovich/2018/Detone, Malisiewicz, Rabinovich - 2018 - SuperPoint Self-supervised interest point detection and description.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {337--349},
title = {{SuperPoint: Self-supervised interest point detection and description}},
volume = {2018-June},
year = {2018}
}
@article{Semini2008a,
abstract = {This paper describes the concept, specifications and design of the biologically inspired quadruped robot HyQ, with special focus on the leg design. The main scope of this new robotic platform is to study highly dynamic tasks such as running and jumping. To meet the specifications in terms of performance and dimensions, hydraulic actuation has been chosen due to its high power to weight ratio and fast response. Guidelines on how to choose the design parameters of the hydraulic cylinders including lever length are reported. A two DOF leg prototype has been designed and constructed. The experimental test setup for the leg prototype is explained and the results of first hopping experiments are reported. {\textcopyright} 2008 IEEE.},
author = {Semini, Claudio and Tsagarakis, Nikos G. and Vanderborght, Bram and Yang, Yousheng and Caldwell, Darwin G.},
doi = {10.1109/BIOROB.2008.4762913},
file = {::},
isbn = {9781424428830},
issn = {2155-1774},
journal = {Proceedings of the 2nd Biennial IEEE/RAS-EMBS International Conference on Biomedical Robotics and Biomechatronics, BioRob 2008},
keywords = {Bioinspired Design,Compliance,Hydraulic Actuation,Quadruped Robot},
pages = {593--599},
title = {{HyQ - hydraulically actuated quadruped robot: Hopping leg prototype}},
year = {2008}
}
@article{Jaeger2014a,
abstract = {The human brain is a dynamical system whose extremely complex sensor-driven neural processes give rise to conceptual, logical cognition. Understanding the interplay between nonlinear neural dynamics and concept-level cognition remains a major scientific challenge. Here I propose a mechanism of neurodynamical organization, called conceptors, which unites nonlinear dynamics with basic principles of conceptual abstraction and logic. It becomes possible to learn, store, abstract, focus, morph, generalize, de-noise and recognize a large number of dynamical patterns within a single neural system; novel patterns can be added without interfering with previously acquired ones; neural noise is automatically filtered. Conceptors help explaining how conceptual-level information processing emerges naturally and robustly in neural systems, and remove a number of roadblocks in the theory and applications of recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1403.3369},
author = {Jaeger, Herbert},
eprint = {1403.3369},
file = {::},
number = {31},
title = {{Controlling Recurrent Neural Networks by Conceptors}},
url = {http://arxiv.org/abs/1403.3369},
year = {2014}
}
@article{Mur-Artal2016b,
abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
archivePrefix = {arXiv},
arxivId = {1610.06475},
author = {Mur-Artal, Raul and Tardos, Juan D.},
doi = {10.1109/TRO.2017.2705103},
eprint = {1610.06475},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Localization,RGB-D,mapping,simultaneous localization and mapping (SLAM),stereo},
month = {oct},
number = {5},
pages = {1255--1262},
title = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}},
url = {http://arxiv.org/abs/1610.06475 http://ieeexplore.ieee.org/document/7946260/},
volume = {33},
year = {2017}
}
@article{Toussaint2017,
abstract = {Many state-of-the-art approaches to trajectory optimization and optimal control are intimately related to standard Newton methods. For researchers that work in the intersections of machine learning, robotics, control, and optimization, such relations are highly relevant but sometimes hard to see across disciplines, due also to the different notations and conventions used in the disciplines. The aim of this tutorial is to introduce to constrained trajectory optimization in a manner that allows us to establish these relations. We consider a basic but general formalization of the problem and discuss the structure of Newton steps in this setting. The computation of Newton steps can then be related to dynamic programming, establishing relations to DDP, iLQG, and AICO. We can also clarify how inverting a banded symmetric matrix is related to dynamic programming as well as message passing in Markov chains and factor graphs. Further, for a machine learner, path optimization and Gaussian Processes seem intuitively related problems. We establish such a relation and show how to solve a Gaussian Process-regularized path optimization problem efficiently. Further topics include how to derive an optimal controller around the path, model predictive control in constrained k-order control processes, and the pullback metric interpretation of the Gauss–Newton approximation.},
author = {Toussaint, Marc},
doi = {10.1007/978-3-319-51547-2_15},
file = {:home/matias/Documents/Mendeley Desktop/Toussaint/2017/Toussaint - 2017 - A tutorial on Newton methods for constrained trajectory optimization and relations to SLAM , Gaussian Process smoothi.pdf:pdf},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
number = {1998},
pages = {361--392},
title = {{A tutorial on newton methods for constrained trajectory optimization and relations to SLAM, gaussian process smoothing, optimal control, and probabilistic inference}},
volume = {117},
year = {2017}
}
@article{Chebrolu2020,
abstract = {State estimation is a key ingredient in most robotic systems. Often, state estimation is performed using some form of least squares minimization. Basically, all error minimization procedures that work on real-world data use robust kernels as the standard way for dealing with outliers in the data. These kernels, however, are often hand-picked, sometimes in different combinations, and their parameters need to be tuned manually for a particular problem. In this paper, we propose the use of a generalized robust kernel family, which is automatically tuned based on the distribution of the residuals and includes the common m-estimators. We tested our adaptive kernel with two popular estimation problems in robotics, namely ICP and bundle adjustment. The experiments presented in this paper suggest that our approach provides higher robustness while avoiding a manual tuning of the kernel parameters.},
archivePrefix = {arXiv},
arxivId = {2004.14938},
author = {Chebrolu, Nived and L{\"{a}}be, Thomas and Vysotska, Olga and Behley, Jens and Stachniss, Cyrill},
eprint = {2004.14938},
file = {:home/matias/Documents/Mendeley Desktop/Chebrolu et al/2020/Chebrolu et al. - 2020 - Adaptive Robust Kernels for Non-Linear Least Squares Problems.pdf:pdf},
number = {i},
title = {{Adaptive Robust Kernels for Non-Linear Least Squares Problems}},
url = {http://arxiv.org/abs/2004.14938},
year = {2020}
}
@article{Choi2015,
abstract = {Several pose estimation algorithms, such as n-point and perspective n-point (PnP), have been introduced over the last few decades to solve the relative and absolute pose estimation problems in robotics research. Since the n-point algorithms cannot decide the real scale of robot motion, the PnP algorithms are often addressed to find the absolute scale of motion. This paper introduce a new PnP algorithm which use only two 3D-2D correspondences by considering only planar motion. Experiment results prove that the proposed algorithm solves the absolute motion in real scale with high accuracy and less computational time compared to previous algorithms.},
author = {Choi, Sung In and Park, Soon Yong},
doi = {10.1080/01691864.2015.1024285},
issn = {15685535},
journal = {Advanced Robotics},
keywords = {robot motion,simultaneous localization and mapping,visual odometry},
number = {15},
pages = {1005--1013},
title = {{A new 2-point absolute pose estimation algorithm under planar motion}},
url = {http://www.tandfonline.com/doi/full/10.1080/01691864.2015.1024285#abstract},
volume = {29},
year = {2015}
}
@article{Alismail2016a,
abstract = {We present an algorithm for robust and real-time visual tracking under challenging illumination conditions characterized by poor lighting as well as sudden and drastic changes in illumination. Robustness is achieved by adapting illumination-invariant binary descriptors to dense image alignment using the Lucas and Kanade algorithm. The proposed adaptation preserves the Hamming distance under least-squares minimization, thus preserving the photometric invariance properties of binary descriptors. Due to the compactness of the descriptor, the algorithm runs in excess of 400 fps on laptops and 100 fps on mobile devices.},
author = {Alismail, Hatem and Browning, Brett and Lucey, Simon},
doi = {10.1109/3DV.2016.48},
file = {:home/matias/Documents/Mendeley Desktop/Alismail, Browning, Lucey/2016/Alismail, Browning, Lucey - 2016 - Robust tracking in low light and sudden illumination changes.pdf:pdf},
isbn = {9781509054077},
journal = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
keywords = {Lucas-Kanade,Robust tracking,binary descriptors,direct alignment,illumination change,image registration},
pages = {389--398},
title = {{Robust tracking in low light and sudden illumination changes}},
year = {2016}
}
@article{Nutzi2011,
abstract = {The fusion of inertial and visual data is widely used to improve an object's pose estimation. However, this type of fusion is rarely used to estimate further unknowns in the visual framework. In this paper we present and compare two different approaches to estimate the unknown scale parameter in a monocular SLAM framework. Directly linked to the scale is the estimation of the object's absolute velocity and position in 3D. The first approach is a spline fitting task adapted from Jung and Taylor and the second is an extended Kalman filter. Both methods have been simulated offline on arbitrary camera paths to analyze their behavior and the quality of the resulting scale estimation. We then embedded an online multi rate extended Kalman filter in the Parallel Tracking and Mapping (PTAM) algorithm of Klein and Murray together with an inertial sensor. In this inertial/monocular SLAM framework, we show a real time, robust and fast converging scale estimation. Our approach does not depend on known patterns in the vision part nor a complex temporal synchronization between the visual and inertial sensor. {\textcopyright} 2010 Springer Science+Business Media B.V.},
author = {N{\"{u}}tzi, Gabriel and Weiss, Stephan and Scaramuzza, Davide and Siegwart, Roland},
doi = {10.1007/s10846-010-9490-z},
file = {::},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Absolute scale,IMU vision fusion,Kalman filter,Monocular SLAM},
number = {1-4},
pages = {287--299},
title = {{Fusion of IMU and vision for absolute scale estimation in monocular SLAM}},
volume = {61},
year = {2011}
}
@article{Tong2013,
abstract = {In this paper, we present Gaussian Process Gauss-Newton (GPGN), an algorithm for non-parametric, continuous-time, nonlinear, batch state estimation. This work adapts the methods of Gaussian process (GP) regression to address the problem of batch simultaneous localization and mapping (SLAM) by using the Gauss-Newton optimization method. In particular, we formulate the estimation problem with a continuous-time state model, along with the more conventional discrete-time measurements. Two derivations are presented in this paper, reflecting both the weight-space and function-space approaches from the GP regression literature. Validation is conducted through simulations and a hardware experiment, which utilizes the well-understood problem of two-dimensional SLAM as an illustrative example. The performance is compared with the traditional discrete-time batch Gauss-Newton approach, and we also show that GPGN can be employed to estimate motion with only range/bearing measurements of landmarks (i.e. no odometry), even when there are not enough measurements to constrain the pose at a given timestep. {\textcopyright} The Author(s) 2013.},
author = {Tong, Chi Hay and Furgale, Paul and Barfoot, Timothy D.},
doi = {10.1177/0278364913478672},
file = {:home/matias/Documents/Mendeley Desktop/Tong, Furgale, Barfoot/2013/Tong, Furgale, Barfoot - 2013 - Gaussian Process Gauss-Newton for non-parametric simultaneous localization and mapping.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Gaussian processes,SLAM,continuous time,estimation},
number = {5},
pages = {507--525},
title = {{Gaussian Process Gauss-Newton for non-parametric simultaneous localization and mapping}},
volume = {32},
year = {2013}
}
@article{Davis2013,
abstract = {CHOLMOD is a set of routines for factorizing sparse symmetric positive definite matrices of the form A or AAT, updating/downdating a sparse Cholesky factorization, solving linear systems, updating/downdating the solution to the triangular system Lx = b, and many other sparse matrix functions for both symmetric and unsymmetric matrices. Its supernodal Cholesky factorization relies on LAPACK and the Level-3 BLAS, and obtains a substantial fraction of the peak performance of the BLAS. Both real and complex matrices are supported. It also includes a non-supernodal LDLT factorization method that can factorize symmetric indefinite matrices if all of their leading submatrices are well-conditioned (D is diagonal). CHOLMOD is written in ANSI/ISO C, with both C and MATLAB interfaces. This code works on Microsoft Windows and many versions of Unix and Linux.},
author = {Davis, Timothy a},
doi = {10.1.1.220.238},
journal = {Department of Computer and Information Science and {\ldots}},
keywords = {a sparse cholesky factorization,and,modification package,r guide for cholmod},
pages = {1--140},
title = {{User Guide for CHOLMOD : a sparse Cholesky factorization and modification package}},
url = {http://bbq.dfm.io/$\sim$dfm/research/ceres-solver/deps/SuiteSparse/CHOLMOD/Doc/UserGuide.pdf},
year = {2013}
}
@inproceedings{Osswald2012,
abstract = {In order to successfully climb challenging stair-cases that consist of many steps and contain difficult parts, humanoid robots need to accurately determine their pose. In this paper, we present an approach that fuses the robot's observations from a 2D laser scanner, a monocular camera, an inertial measurement unit, and joint encoders in order to localize the robot within a given 3D model of the environment. We develop an extension to standard Monte Carlo localization (MCL) that draws particles from an improved proposal distribution to obtain highly accurate pose estimates. Furthermore, we introduce a new observation model based on chamfer matching between edges in camera images and the environment model. We thoroughly evaluate our localization approach and compare it to previous techniques in real-world experiments with a Nao humanoid. The results show that our approach significantly improves the localization accuracy and leads to a considerably more robust robot behavior. Our improved proposal in combination with chamfer matching can be generally applied to improve a range-based pose estimate by a consistent matching of lines obtained from vision. {\textcopyright} 2012 IEEE.},
author = {Osswald, Stefan and Hornung, Armin and Bennewitz, Maren},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385657},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {1809--1814},
publisher = {IEEE},
title = {{Improved proposals for highly accurate localization using range and vision data}},
url = {http://ieeexplore.ieee.org/document/6385657/},
year = {2012}
}
@inproceedings{Laidlow2017,
abstract = {While dense visual SLAM methods are capable of estimating dense reconstructions of the environment, they suffer from a lack of robustness in their tracking step, especially when the optimisation is poorly initialised. Sparse visual SLAM systems have attained high levels of accuracy and robustness through the inclusion of inertial measurements in a tightly-coupled fusion. Inspired by this performance, we propose the first tightly-coupled dense RGB-D-inertial SLAM system. Our system has real-time capability while running on a GPU. It jointly optimises for the camera pose, velocity, IMU biases and gravity direction while building up a globally consistent, fully dense surfel-based 3D reconstruction of the environment. Through a series of experiments on both synthetic and real world datasets, we show that our dense visual-inertial SLAM system is more robust to fast motions and periods of low texture and low geometric variation than a related RGB-D-only SLAM system.},
author = {Laidlow, Tristan and Bloesch, Michael and Li, Wenbin and Leutenegger, Stefan},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8206591},
file = {::},
isbn = {9781538626825},
issn = {21530866},
pages = {6741--6748},
title = {{Dense RGB-D-inertial SLAM with map deformations}},
url = {https://www.doc.ic.ac.uk/$\sim$sleutene/publications/IROS2017_Laidlow.pdf},
volume = {2017-Septe},
year = {2017}
}
@article{Kim2015a,
abstract = {A Bayesian filter for rotation groups in 2D and 3D is derived. The prior, propagator, and measurement probability densities are all assumed to be bandlimited functions on SO(2) or SO(3), expressed as a Fourier series on these compact Lie groups. The posterior, which has a higher bandlimit, is computed and then low-pass filtered, resulting in a bandlimited approximation. The benefits and drawbacks of the Fourier approach presented here are examined in contrast to the Gaussian approach designed for small error covariances. While the Gaussian approach is much faster, it breaks down for large error covariances. The point where the Gaussian approach breaks down is analyzed with the Fourier method, indicating the range of error sizes where the switch to Fourier methods is required.},
author = {Kim, Jin Seob and Chirikjian, Gregory S.},
file = {::},
isbn = {9780982443866},
journal = {2015 18th International Conference on Information Fusion, Fusion 2015},
number = {3},
pages = {748--753},
title = {{Bayesian filtering for orientational distributions: A fourier approach}},
year = {2015}
}
@article{Bengio2013,
abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying datagenerating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
archivePrefix = {arXiv},
arxivId = {1305.6663},
author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
eprint = {1305.6663},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Generalized denoising auto-encoders as generative models}},
url = {http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf},
year = {2013}
}
@inproceedings{Ren2015,
abstract = {Abstract The Cherenkov Imaging Telescope Integrated Read Out Chip, CITIROC, is a chip adopted as the front-end of the camera at the focal plane of the imaging Cherenkov ASTRI dual-mirror small size telescope (ASTRI SST-2M) prototype. This paper presents the results of the measurements performed to characterize CITIROC tailored for the ASTRI SST-2M focal plane requirements. In particular, we investigated the trigger linearity and efficiency, as a function of the pulse amplitude. Moreover, we tested its response by performing a set of measurements using a silicon photomultiplier (SiPM) in dark conditions and under light pulse illumination. The CITIROC output signal is found to vary linearly as a function of the input pulse amplitude. Our results show that it is suitable for the ASTRI SST-2M camera.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Impiombato, D. and Giarrusso, S. and Mineo, T. and Catalano, O. and Gargano, C. and {La Rosa}, G. and Russo, F. and Sottile, G. and Billotta, S. and Bonanno, G. and Garozzo, S. and Grillo, A. and Marano, D. and Romeo, G.},
booktitle = {Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {::},
isbn = {0162-8828 VO - PP},
issn = {01689002},
keywords = {ASIC for SiPM,ASTRI,CITIROC,Front-end},
pages = {185--192},
pmid = {27295650},
title = {{Characterization and performance of the ASIC (CITIROC) front-end of the ASTRI camera}},
volume = {794},
year = {2015}
}
@article{Draghici2002,
abstract = {This paper analyzes some aspects of the computational power of neural networks using integer weights in a very restricted range. Using limited range integer values opens the road for efficient VLSI implementations because: (i) a limited range for the weights can be translated into reduced storage requirements and (ii) integer computation can be implemented in a more efficient way than the floating point one. The paper concentrates on classification problems and shows that, if the weights are restricted in a drastic way (both range and precision), the existence of a solution is not to be taken for granted anymore. The paper presents an existence result which relates the difficulty of the problem as characterized by the minimum distance between patterns of different classes to the weight range necessary to ensure that a solution exists. This result allows us to calculate a weight range for a given category of problems and be confident that the network has the capability to solve the given problems with integer weights in that range. Worst-case lower bounds are given for the number of entropy bits and weights necessary to solve a given problem. Various practical issues such as the relationship between the information entropy bits and storage bits are also discussed. The approach presented here uses a worst-case analysis. Therefore, the approach tends to overestimate the values obtained for the weight range, the number of bits and the number of weights. The paper also presents some statistical considerations that can be used to give up the absolute confidence of a successful training in exchange for values more appropriate for practical use. The approach presented is also discussed in the context of the VC-complexity. {\textcopyright} 2002 Published by Elsevier Science Ltd.},
author = {Draghici, Sorin},
doi = {10.1016/S0893-6080(02)00032-1},
issn = {08936080},
journal = {Neural Networks},
keywords = {Integer weights,VC-complexity,VLSI implementation},
month = {apr},
number = {3},
pages = {395--414},
pmid = {12125893},
title = {{On the capabilities of neural networks using limited precision weights}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12125893 http://linkinghub.elsevier.com/retrieve/pii/S0893608002000321},
volume = {15},
year = {2002}
}
@book{Ma2004,
abstract = {This book introduces the geometry of 3-D vision, that is, the reconstruction of 3-D models of objects from a collection of 2-D images. It details the classic theory of two view geometry and shows that a more proper tool for studying the geometry of multiple views is the so-called rank consideration of the multiple view matrix. It also develops practical reconstruction algorithms and discusses possible extensions of the theory.},
author = {Ma, Y and Soatto, S and Ko{\v{s}}eck{\'{a}}, J and Sastry, SS},
booktitle = {Springer-Verlag, New York},
doi = {10.1007/978-0-387-21799-6},
isbn = {9780387281780},
issn = {9780387877075},
number = {108},
pages = {416--528},
publisher = {Springer-Verlag, New York},
title = {{An Invitation to 3D Vision}},
url = {http://link.springer.com/content/pdf/10.1007/b98868.pdf%5Cnhttp://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:An+invitation+to+3D+vision#2},
volume = {19},
year = {2004}
}
@article{Ijspeert2008,
abstract = {The problem of controlling locomotion is an area in which neuroscience and robotics can fruitfully interact. In this article, I will review research carried out on locomotor central pattern generators (CPGs), i.e. neural circuits capable of producing coordinated patterns of high-dimensional rhythmic output signals while receiving only simple, low-dimensional, input signals. The review will first cover neurobiological observations concerning locomotor CPGs and their numerical modelling, with a special focus on vertebrates. It will then cover how CPG models implemented as neural networks or systems of coupled oscillators can be used in robotics for controlling the locomotion of articulated robots. The review also presents how robots can be used as scientific tools to obtain a better understanding of the functioning of biological CPGs. Finally, various methods for designing CPGs to control specific modes of locomotion will be briefly reviewed. In this process, I will discuss different types of CPG models, the pros and cons of using CPGs with robots, and the pros and cons of using robots as scientific tools. Open research topics both in biology and in robotics will also be discussed. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ijspeert, Auke Jan},
doi = {10.1016/j.neunet.2008.03.014},
eprint = {NIHMS150003},
file = {::},
isbn = {08936080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Central pattern generators,Computational models,Dynamical systems,Locomotion,Neural networks,Robots,Systems of coupled oscillators},
number = {4},
pages = {642--653},
pmid = {18555958},
title = {{Central pattern generators for locomotion control in animals and robots: A review}},
volume = {21},
year = {2008}
}
@article{Vogiatzis2011,
abstract = {We investigate the problem of obtaining a dense reconstruction in real-time, from a live video stream. In recent years, multi-view stereo (MVS) has received considerable attention and a number of methods have been proposed. However, most methods operate under the assumption of a relatively sparse set of still images as input and unlimited computation time. Video based MVS has received less attention despite the fact that video sequences offer significant benefits in terms of usability of MVS systems. In this paper we propose a novel video based MVS algorithm that is suitable for real-time, interactive 3d modeling with a hand-held camera. The key idea is a per-pixel, probabilistic depth estimation scheme that updates posterior depth distributions with every new frame. The current implementation is capable of updating 15 million distributions/s. We evaluate the proposed method against the state-of-the-art real-time MVS method and show improvement in terms of accuracy. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Vogiatzis, George and Hern{\'{a}}ndez, Carlos},
doi = {10.1016/j.imavis.2011.01.006},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {3d reconstruction,Multi-view stereo,Real-time,Shape-from-X},
month = {jun},
number = {7},
pages = {434--441},
publisher = {Elsevier B.V.},
title = {{Video-based, real-time multi-view stereo}},
url = {http://dx.doi.org/10.1016/j.imavis.2011.01.006 http://linkinghub.elsevier.com/retrieve/pii/S0262885611000138},
volume = {29},
year = {2011}
}
@article{Vanderborght2013,
abstract = {Variable Impedance Actuators (VIA) have received increasing attention in recent years as many novel applications involving interactions with an unknown and dynamic environment including humans require actuators with dynamics that are not well-achieved by classical stiff actuators. This paper presents an overview of the different VIAs developed and proposes a classification based on the principles through which the variable stiffness and damping are achieved. The main classes are active impedance by control, inherent compliance and damping actuators, inertial actuators, and combinations of them, which are then further divided into subclasses. This classification allows for designers of new devices to orientate and take inspiration and users of VIA's to be guided in the design and implementation process for their targeted application. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Vanderborght, B. and Albu-Schaeffer, A. and Bicchi, A. and Burdet, E. and Caldwell, D. G. and Carloni, R. and Catalano, M. and Eiberger, O. and Friedl, W. and Ganesh, G. and Garabini, M. and Grebenstein, M. and Grioli, G. and Haddadin, S. and Hoppner, H. and Jafari, A. and Laffranchi, M. and Lefeber, D. and Petit, F. and Stramigioli, S. and Tsagarakis, N. and {Van Damme}, M. and {Van Ham}, R. and Visser, L. C. and Wolf, S.},
doi = {10.1016/j.robot.2013.06.009},
file = {::},
isbn = {09218890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Soft robotics,Variable impedance actuators},
number = {12},
pages = {1601--1614},
pmid = {17692490},
publisher = {Elsevier B.V.},
title = {{Variable impedance actuators: A review}},
url = {http://dx.doi.org/10.1016/j.robot.2013.06.009},
volume = {61},
year = {2013}
}
@article{Liu2010,
abstract = {Biology provides examples of efficient machines which greatly outperform conventional technology. Designers in neuromorphic engineering aim to construct electronic systems with the same efficient style of computation. This task requires a melding of novel engineering principles with knowledge gleaned from neuroscience. We discuss recent progress in realizing neuromorphic sensory systems which mimic the biological retina and cochlea, and subsequent sensor processing. The main trends are the increasing number of sensors and sensory systems that communicate through asynchronous digital signals analogous to neural spikes; the improved performance and usability of these sensors; and novel sensory processing methods which capitalize on the timing of spikes from these sensors. Experiments using these sensors can impact how we think the brain processes sensory information. {\textcopyright} 2010 Elsevier Ltd.},
author = {Liu, Shih Chii and Delbruck, Tobi},
doi = {10.1016/j.conb.2010.03.007},
file = {::},
isbn = {0959-4388},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {3},
pages = {288--295},
pmid = {20493680},
publisher = {Elsevier Ltd},
title = {{Neuromorphic sensory systems}},
url = {http://dx.doi.org/10.1016/j.conb.2010.03.007},
volume = {20},
year = {2010}
}
@article{Engel2013,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to benefit from the simplicity and accuracy of dense tracking-which does not depend on visual features-while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications. {\textcopyright} 2013 IEEE.},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
doi = {10.1109/ICCV.2013.183},
file = {::},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {SLAM,dense,monocular,stereo,visual odometry},
pages = {1449--1456},
title = {{Semi-dense visual odometry for a monocular camera}},
year = {2013}
}
@article{Maimone2007,
abstract = {NASA's two Mars Exploration Rovers (MER) have successfully demonstrated a robotic Visual Odometry capability on another world for the first time. This provides each rover with accurate knowledge of its position, allowing it to autonomously detect and compensate for any unforeseen slip encountered during a drive. It has enabled the rovers to drive safely and more effectively in highly sloped and sandy terrains and has resulted in increased mission science return by reducing the number of days required to drive into interesting areas. The MER Visual Odometry system comprises onboard software for comparing stereo pairs taken by the pointable mast-mounted 45 deg FOV Navigation cameras (NAVCAMs). The system computes an update to the 6 degree of freedom rover pose (x, y, z, roll, pitch, yaw) by tracking the motion of autonomously selected terrain features between two pairs of 256 × 256 stereo images. It has demonstrated good performance with high rates of successful convergence (97% on Spirit, 95% on Opportunity), successfully detected slip ratios as high as 125%, and measured changes as small as 2 mm, even while driving on slopes as high as 31 deg. Visual Odometry was used over 14% of the first 10.7 km driven by both rovers. During the first 2 years of operations, Visual Odometry evolved from an "extra credit" capability into a critical vehicle safety system. In this paper we describe our Visual Odometry algorithm, discuss several driving strategies that rely on it (including Slip Checks, Keep-out Zones, and Wheel Dragging), and summarize its results from the first 2 years of operations on Mars. {\textcopyright} 2007 Wiley Periodicals, Inc.},
author = {Maimone, Mark and Cheng, Yang and Matthies, Larry},
doi = {10.1002/rob.20184},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {3},
pages = {169--186},
title = {{Two years of visual odometry on the Mars Exploration Rovers}},
volume = {24},
year = {2007}
}
@inproceedings{Chilian2011,
abstract = {For autonomous navigation tasks it is important that the robot always has a good estimate of its current pose with respect to its starting position and - in terms of orientation - with respect to the gravity vector. For this, the robot should make use of all available information and be robust against the failure of single sensors. In this paper a multisensor data fusion algorithm for the six-legged walking robot DLR Crawler is presented. The algorithm is based on an indirect feedback information filter that fuses measurements from an inertial measurement unit (IMU) with relative 3D leg odometry measurements and relative 3D visual odometry measurements from a stereo camera. Errors of the visual odometry are computed and considered in the filtering process in order to achieve accurate pose estimates which are robust against visual odometry failure. The algorithm was successfully tested and results are presented. {\textcopyright} 2011 IEEE.},
author = {Chilian, Annett and Hirschm{\"{u}}ller, Heiko and G{\"{o}}rner, Martin},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048125},
file = {::},
isbn = {9781612844541},
issn = {2153-0858},
month = {sep},
pages = {2497--2504},
publisher = {IEEE},
title = {{Multisensor data fusion for robust pose estimation of a six-legged walking robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6048125},
year = {2011}
}
@article{Forster2017,
abstract = {Direct methods for visual odometry (VO) have gained popularity for their capability to exploit information from all intensity gradients in the image. However, low computational speed as well as missing guarantees for optimality and consistency are limiting factors of direct methods, in which established feature-based methods succeed instead. Based on these considerations, we propose a semidirect VO (SVO) that uses direct methods to track and triangulate pixels that are characterized by high image gradients, but relies on proven feature-based methods for joint optimization of structure and motion. Together with a robust probabilistic depth estimation algorithm, this enables us to efficiently track pixels lying on weak corners and edges in environments with little or high-frequency texture. We further demonstrate that the algorithm can easily be extended to multiple cameras, to track edges, to include motion priors, and to enable the use of very large field of view cameras, such as fisheye and catadioptric ones. Experimental evaluation on benchmark datasets shows that the algorithm is significantly faster than the state of the art while achieving highly competitive accuracy.},
archivePrefix = {arXiv},
arxivId = {1204.3968},
author = {Forster, Christian and Zhang, Zichao and Gassner, Michael and Werlberger, Manuel and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2623335},
eprint = {1204.3968},
file = {::},
isbn = {9783902823625},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Robot vision,simultaneous localization and mapping (SLAM)},
month = {apr},
number = {2},
pages = {249--265},
title = {{SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems}},
url = {http://ieeexplore.ieee.org/document/7782863/},
volume = {33},
year = {2017}
}
@article{Kunze2018,
abstract = {Autonomous systems will play an essential role in many applications across diverse domains including space, marine, air, field, road, and service robotics. They will assist us in our daily routines and perform dangerous, dirty, and dull tasks. However, enabling robotic systems to perform autonomously in complex, real-world scenarios over extended time periods (i.e., weeks, months, or years) poses many challenges. Some of these have been investigated by subdisciplines of Artificial Intelligence (AI) including navigation and mapping, perception, knowledge representation and reasoning, planning, interaction, and learning. The different subdisciplines have developed techniques that, when re-integrated within an autonomous system, can enable robots to operate effectively in complex, long-term scenarios. In this letter, we survey and discuss AI techniques as 'enablers' for long-term robot autonomy, current progress in integrating these techniques within long-running robotic systems, and the future challenges and opportunities for AI in long-term autonomy.},
archivePrefix = {arXiv},
arxivId = {1807.05196},
author = {Kunze, Lars and Hawes, Nick and Duckett, Tom and Hanheide, Marc and Krajnik, Tomas},
doi = {10.1109/LRA.2018.2860628},
eprint = {1807.05196},
file = {:home/matias/Documents/Mendeley Desktop/Kunze et al/2018/Kunze et al. - 2018 - Artificial Intelligence for Long-Term Robot Autonomy A Survey.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {AI-based methods,Autonomous agents,long-term autonomy},
number = {4},
pages = {4023--4030},
publisher = {IEEE},
title = {{Artificial Intelligence for Long-Term Robot Autonomy: A Survey}},
volume = {3},
year = {2018}
}
@article{Yi2015,
abstract = {This paper describes the technical approach, hardware design, and software algorithms that have been used by Team THOR in the DARPA Robotics Challenge (DRC) Trials 2013 competition. To overcome big hurdles such as a short development time and limited budget, we focused on forming modular components - in both hardware and software - to allow for efficient and cost-effective parallel development. The hardware of THOR-OP (Tactical Hazardous Operations Robot-Open Platform) consists of standardized, advanced actuators and structural components. These aspects allowed for efficient maintenance, quick reconfiguration, and most importantly, a relatively low build cost. We also pursued modularity in the software, which consisted of a hybrid locomotion engine, a hierarchical arm controller, and a platform-independent remote operator interface. These modules yielded multiple control options with different levels of autonomy to suit various situations. The flexible software architecture allowed rapid development, quick migration to hardware changes, and multiple parallel control options. These systems were validated at the DRC Trials, where THOR-OP performed well against other robots and successfully acquired finalist status.},
author = {Yi, Seung Joon and McGill, Stephen G. and Vadakedathu, Larry and He, Qin and Ha, Inyong and Han, Jeakweon and Song, Hyunjong and Rouleau, Michael and Zhang, Byoung Tak and Hong, Dennis and Yim, Mark and Lee, Daniel D.},
doi = {10.1002/rob.21555},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {may},
number = {3},
pages = {315--335},
title = {{Team THOR's Entry in the DARPA Robotics Challenge Trials 2013}},
url = {http://doi.wiley.com/10.1002/rob.21555},
volume = {32},
year = {2015}
}
@article{Marchant2016,
abstract = {This thesis addresses the problem of trajectory planning for monitoring extreme val- ues of an environmental phenomenon that changes in space and time. The most relevant case study corresponds to environmental monitoring using an autonomous mobile robot for air, water and land pollution monitoring. Since the dynamics of the phenomenon are initially unknown, the planning algorithm needs to satisfy two objectives simultaneously: 1) Learn and predict spatial-temporal patterns and, 2) find areas of interest (e.g. high pollution), addressing the exploration-exploitation trade-off. Consequently, the thesis brings the following contributions: Firstly, it applies and formulates Bayesian Optimisation (BO) to planning in robotics. By maintaining a Gaussian Process (GP) model of the environmental phenomenon the planning algorithms are able to learn the spatial and temporal patterns. A new family of acquisition functions which consider the position of the robot is proposed, allowing an efficient trajectory planning. Secondly, BO is generalised for optimisation over continuous paths, not only determ- ining where and when to sample, but also how to get there. Under these new cir- cumstances, the optimisation of the acquisition function for each iteration of the BO algorithm becomes costly, thus a second layer of BO is included in order to effectively reduce the number of iterations. Finally, this thesis presents Sequential Bayesian Optimisation (SBO), which is a gen- eralisation of the plain BO algorithm with the goal of achieving non-myopic trajectory planning. SBO is formulated under a Partially Observable Markov Decision Process (POMDP) framework, which can find the optimal decision for a sequence of actions with their respective outcomes. An online solution of the POMDP based on Monte Carlo Tree Search (MCTS) allows an efficient search of the optimal action for multi- step lookahead. The proposed planning algorithms are evaluated under different scenarios. Experi- ments on large scale ozone pollution monitoring and indoor light intensity monitor- ing are conducted for simulated and real robots. The results show the advantages of planning over continuous paths and also demonstrate the benefit of deeper search strategies using SBO.},
author = {Matus, Rom{\'{a}}n Marchant},
isbn = {9781848882928},
pages = {113},
title = {{Bayesian Optimisation for Planning in Dynamic Environments}},
year = {2016}
}
@inproceedings{Kastner2015,
abstract = {In this paper, we present an automatic approach for the kinematic calibration of the humanoid robot NAO. The kinematic calibration has a deep impact on the performance of a robot playing soccer, which is walking and kicking, and therefore it is a crucial step prior to a match. So far, the existing calibration methods are time-consuming and error-prone, since they rely on the assistance of humans. The automatic calibration procedure instead consists of a self-acting measurement phase, in which two checkerboards, that are attached to the robot's feet, are visually observed by a camera under several different kinematic configurations, and a final optimization phase, in which the calibration is formulated as a non-linear least squares problem, that is finally solved utilizing the Levenberg-Marquardt algorithm.},
author = {Kastner, Tobias and Rofer, Thomas and Laue, Tim},
booktitle = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
doi = {10.1007/978-3-319-18615-3_19},
isbn = {9783319186146},
issn = {03029743},
pages = {233--244},
title = {{Automatic robot calibration for the NAO}},
volume = {8992},
year = {2015}
}
@techreport{Czarnetzki2010,
author = {Tasse, Stefan and Urbann, Oliver and Hofmann, Matthias and Schwarz, Ingmar},
institution = {Technishe Universit{\"{a}}t Dortmund},
title = {{Nao Devils Dortmund Team Report 2011}},
year = {2011}
}
@article{Mullane2011,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little $q$-Jacobi polynomials in the limit $q=-1$. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for $q=-1$.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Mullane, John and Vo, Ba-Ngu and Adams, Martin and Vo, Ba-Tuong},
doi = {10.1007/978-3-642-21390-8},
eprint = {1011.1669},
file = {:home/matias/Documents/Mendeley Desktop/Mullane et al/2011/Mullane et al. - 2011 - Random Finite Sets for Robot Mapping and SLAM.pdf:pdf},
isbn = {978-3-642-21389-2},
issn = {1610-7438},
journal = {Robotics Research},
pages = {329--340},
pmid = {15003161},
publisher = {Springer Berlin Heidelberg},
series = {Springer Tracts in Advanced Robotics},
title = {{Random Finite Sets for Robot Mapping and SLAM}},
url = {http://arxiv.org/abs/1011.1669%5Cnhttp://dx.doi.org/10.1088/1751-8113/44/8/085201%5Cnhttp://link.springer.com/10.1007/978-3-642-21390-8},
volume = {72},
year = {2011}
}
@techreport{Rotella2014a,
author = {Rotella, Nicholas},
file = {::},
institution = {University of Southern California},
title = {{The Blind Humanoid - Kinematic and Inertial Data Fusion}},
year = {2014}
}
@article{Hauberg2013,
abstract = {In recent years there has been a growing interest in problems, where either the observed data or hidden state variables are confined to a known Riemannian manifold. In sequential data analysis this interest has also been growing, but rather crude algorithms have been applied: either Monte Carlo filters or brute-force discretisations. These approaches scale poorly and clearly show a missing gap: no generic analogues to Kalman filters are currently available in non-Euclidean domains. In this paper, we remedy this issue by first generalising the unscented transform and then the unscented Kalman filter to Riemannian manifolds. As the Kalman filter can be viewed as an optimisation algorithm akin to the Gauss-Newton method, our algorithm also provides a general-purpose optimisation framework on manifolds. We illustrate the suggested method on synthetic data to study robustness and convergence, on a region tracking problem using covariance features, an articulated tracking problem, a mean value optimisation and a pose optimisation problem. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
author = {Hauberg, S{\o}ren and Lauze, Fran{\c{c}}ois and Pedersen, Kim Steenstrup},
doi = {10.1007/s10851-012-0372-9},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Filtering theory,Optimisation on manifolds,Riemannian manifolds,Unscented Kalman filter},
number = {1},
pages = {103--120},
title = {{Unscented kalman filtering on riemannian manifolds}},
url = {http://link.springer.com/article/10.1007/s10851-012-0372-9},
volume = {46},
year = {2013}
}
@article{Dellaert2017,
abstract = {The objective of this monograph is to review prior studies and propose new research directions for the corporate governance of Chinese listed firms. The focus of this monograph is to investigate the underlying relation between China's institutional environment and its listed firms' corporate governance, and to show how formal and informal governance mechanisms actually work within these firms. A top-down institutional framework is adopted to integrate prior research and guide us in identifying first-order factors that shape the corporate governance practice in China. Following this institutional framework, I propose a number of research directions that study the formal and informal governance approaches unique to China's environment.},
author = {Dellaert, Frank and Kaess, Michael},
doi = {10.1561/2300000043},
file = {:home/matias/Documents/Mendeley Desktop/Dellaert, Kaess/2017/Dellaert, Kaess - 2017 - Factor Graphs for Robot Perception.pdf:pdf},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
number = {1-2},
pages = {1--139},
title = {{Factor Graphs for Robot Perception}},
volume = {6},
year = {2017}
}
@article{Hutter2014,
abstract = {This paper provides an overview about StarlETH: a compliant quadrupedal robot that is designed to study fast, efficient, versatile, and robust locomotion. The platform is driven by highly compliant series elastic actuation, which makes the system fully torque controllable, energetically efficient, and well suited for dynamic maneuvers. Using model-based control strategies, this medium dog-sized machine is capable of various gaits ranging from static walking to dynamic running over challenging terrain. StarlETH is equipped with an onboard PC, batteries, and various sensor equipment that enables enduring autonomous operation. In this paper, we provide an overview about the underlying locomotion control algorithms, outline a real-time control and simulation environment, and conclude the work with a number of experiments to demonstrate the performance of the presented hardware and controllers.},
author = {Hutter, Marco and Gehring, Christian and H{\"{o}}pflinger, Mark A. and Bl{\"{o}}sch, Michael and Siegwart, Roland},
doi = {10.1109/TRO.2014.2360493},
file = {::},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Compliant system,Dynamic locomotion,Legged robots,Quadruped robot,Series elastic actuation},
number = {6},
pages = {1427--1440},
title = {{Toward combining speed, efficiency, versatility, and robustness in an autonomous quadruped}},
volume = {30},
year = {2014}
}
@article{Williams2014,
abstract = {We present a parallelized navigation architecture that is capable of running in real-time and incorporating long-term loop closure constraints while producing the optimal Bayesian solution. This architecture splits the inference problem into a low-latency update that incorporates new measurements using just the most recent states (filter), and a high-latency update that is capable of closing long loops and smooths using all past states (smoother). This architecture employs the probabilistic graphical models of factor graphs, which allows the low-latency inference and high-latency inference to be viewed as sub-operations of a single optimization performed within a single graphical model. A specific factorization of the full joint density is employed that allows the different inference operations to be performed asynchronously while still recovering the optimal solution produced by a full batch optimization. Due to the real-time, asynchronous nature of this algorithm, updates to the state estimates from the high-latency smoother will naturally be delayed until the smoother calculations have completed. This architecture has been tested within a simulated aerial environment and on real data collected from an autonomous ground vehicle. In all cases, the concurrent architecture is shown to recover the full batch solution, even while updated state estimates are produced in real-time.},
author = {Williams, Stephen and Indelman, Vadim and Kaess, Michael and Roberts, Richard and Leonard, John J. and Dellaert, Frank},
doi = {10.1177/0278364914531056},
isbn = {978-1-4673-0417-7},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Information fusion,SLAM,filtering,probabilistic graphical models,real time navigation,smoothing},
number = {12},
pages = {1544--1568},
title = {{Concurrent filtering and smoothing: A parallel architecture for real-time navigation and full smoothing}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364914531056},
volume = {33},
year = {2014}
}
@article{Alcantarilla2013a,
abstract = {We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.},
author = {Alcantarilla, Pablo F. and Nuevo, Jes{\'{u}}s and Bartoli, Adrien},
doi = {10.5244/C.27.13},
file = {:home/matias/Documents/Mendeley Desktop/Alcantarilla, Nuevo, Bartoli/2013/Alcantarilla, Nuevo, Bartoli - 2013 - Fast explicit diffusion for accelerated features in nonlinear scale spaces.pdf:pdf},
journal = {BMVC 2013 - Electronic Proceedings of the British Machine Vision Conference 2013},
title = {{Fast explicit diffusion for accelerated features in nonlinear scale spaces}},
year = {2013}
}
@article{Krajnik2017a,
abstract = {We present an evaluation of standard image features in the context of long-term visual teach-and-repeat navigation of mobile robots, where the environment exhibits significant changes in appearance caused by seasonal weather variations and daily illumination changes. We argue that for long-term autonomous navigation, the viewpoint-, scale- and rotation- invariance of the standard feature extractors is less important than their robustness to the mid- and long-term environment appearance changes. Therefore, we focus our evaluation on the robustness of image registration to variable lighting and naturally-occurring seasonal changes. We combine detection and description components of different image extractors and evaluate their performance on five datasets collected by mobile vehicles in three different outdoor environments over the course of one year. Moreover, we propose a trainable feature descriptor based on a combination of evolutionary algorithms and Binary Robust Independent Elementary Features, which we call GRIEF (Generated BRIEF). In terms of robustness to seasonal changes, the most promising results were achieved by the SpG/CNN and the STAR/GRIEF feature, which was slightly less robust, but faster to calculate.},
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Crist{\'{o}}foris, Pablo and Kusumam, Keerthy and Neubert, Peer and Duckett, Tom},
doi = {10.1016/j.robot.2016.11.011},
file = {:home/matias/Documents/Mendeley Desktop/Krajn{\'{i}}k et al/2017/Krajn{\'{i}}k et al. - 2017 - Image features for visual teach-and-repeat navigation in changing environments.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Long-term autonomy,Mobile robotics,Visual navigation},
pages = {127--141},
title = {{Image features for visual teach-and-repeat navigation in changing environments}},
volume = {88},
year = {2017}
}
@article{Cummins2008,
abstract = {This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment-identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mobile robotics. {\textcopyright} 2008 SAGE Publications.},
author = {Cummins, Mark and Newman, Paul},
doi = {10.1177/0278364908090961},
file = {:home/matias/Documents/Mendeley Desktop/Cummins, Newman/2008/Cummins, Newman - 2008 - FAB-MAP Probabilistic localization and mapping in the space of appearance.pdf:pdf},
isbn = {0278364908090},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Appearance based navigation,Place recognition,Topological SLAM},
number = {6},
pages = {647--665},
title = {{FAB-MAP: Probabilistic localization and mapping in the space of appearance}},
volume = {27},
year = {2008}
}
@article{Lichtsteiner2008,
abstract = {This paper describes a 128×128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1 % in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is > 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 $\mu$s at > 1 klux pixel illumination. The sensor is built in a 0.35 $\mu$m 4M2P process. It has 40×40 $\mu$m2 pixels with 9.4% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements. {\textcopyright} 2008 IEEE.},
author = {Lichtsteiner, Patrick and Posch, Christoph and Delbruck, Tobi},
doi = {10.1109/JSSC.2007.914337},
file = {::},
isbn = {0018-9200 VO - 43},
issn = {00189200},
journal = {IEEE Journal of Solid-State Circuits},
keywords = {Address-event representation (AER),Asynchronous vision sensor,High-speed imaging,Image sensors,Machine vision,Neural network hardware,Neuromorphic circuit,Robot vision systems,Visual system,Wide dynamic range imaging},
number = {2},
pages = {566--576},
title = {{A 128 × 128 120 dB 15 $\mu$s latency asynchronous temporal contrast vision sensor}},
volume = {43},
year = {2008}
}
@article{Greene2020,
abstract = {We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.},
author = {Greene, W. Nicholas and Roy, Nicholas},
doi = {10.1109/ICRA40945.2020.9196900},
file = {:home/matias/Documents/Mendeley Desktop/Greene, Roy/2020/Greene, Roy - 2020 - Metrically-Scaled Monocular SLAM using Learned Scale Factors.pdf:pdf},
isbn = {9781728173955},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {43--50},
title = {{Metrically-Scaled Monocular SLAM using Learned Scale Factors}},
year = {2020}
}
@article{Urban2016,
abstract = {The basis for most vision based applications like robotics, self-driving cars and potentially augmented and virtual reality is a robust, continuous estimation of the position and orientation of a camera system w.r.t the observed environment (scene). In recent years many vision based systems that perform simultaneous localization and mapping (SLAM) have been presented and released as open source. In this paper, we extend and improve upon a state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled multi-camera systems (MCS) using the MultiCol model. In addition, we include a performance evaluation on accurate ground truth and compare the robustness of the proposed method to a single camera version of the SLAM system. An open source implementation of the proposed multi-fisheye camera SLAM system can be found on-line https://github.com/urbste/MultiCol-SLAM.},
archivePrefix = {arXiv},
arxivId = {1610.07336},
author = {Urban, Steffen and Hinz, Stefan},
eprint = {1610.07336},
file = {:home/matias/Documents/Mendeley Desktop/Urban, Hinz/2016/Urban, Hinz - 2016 - MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System.pdf:pdf},
keywords = {bundle adjustment,egomotion estimation,fisheye camera,loop closing,multi-camera system,slam},
title = {{MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System}},
url = {http://arxiv.org/abs/1610.07336},
year = {2016}
}
@article{Linegar2015,
abstract = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
doi = {10.1109/ICRA.2015.7138985},
file = {:home/matias/Documents/Mendeley Desktop/Linegar, Churchill, Newman/2015/Linegar, Churchill, Newman - 2015 - Work Smart , Not Hard Recalling Relevant Experiences for Vast-Scale but Time-Constrained Localisati.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {90--97},
publisher = {IEEE},
title = {{Work smart, not hard: Recalling relevant experiences for vast-scale but time-constrained localisation}},
volume = {2015-June},
year = {2015}
}
@article{Indelman2013,
abstract = {This paper presents a new approach for high-rate information fusion in modern inertial navigation systems, that have a variety of sensors operating at different frequencies. Optimal information fusion corresponds to calculating the maximum a posteriori estimate over the joint probability distribution function (pdf) of all states, a computationally-expensive process in the general case. Our approach consists of two key components, which yields a flexible, high-rate, near-optimal inertial navigation system. First, the joint pdf is represented using a graphical model, the factor graph, that fully exploits the system sparsity and provides a plug and play capability that easily accommodates the addition and removal of measurement sources. Second, an efficient incremental inference algorithm over the factor graph is applied, whose performance approaches the solution that would be obtained by a computationally-expensive batch optimization at a fraction of the computational cost. To further aid high-rate performance, we introduce an equivalent IMU factor based on a recently developed technique for IMU pre-integration, drastically reducing the number of states that must be added to the system. The proposed approach is experimentally validated using real IMU and imagery data that was recorded by a ground vehicle, and a statistical performance study is conducted in a simulated aerial scenario. A comparison to conventional fixed-lag smoothing demonstrates that our method provides a considerably improved trade-off between computational complexity and performance. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Indelman, Vadim and Williams, Stephen and Kaess, Michael and Dellaert, Frank},
doi = {10.1016/j.robot.2013.05.001},
file = {::},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Graphical models,Incremental inference,Inertial navigation,Multi-sensor fusion,Plug and play architecture},
month = {aug},
number = {8},
pages = {721--738},
title = {{Information fusion in navigation systems via factor graph based incremental smoothing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092188901500247X http://linkinghub.elsevier.com/retrieve/pii/S092188901300081X},
volume = {61},
year = {2013}
}
@article{Agamennoni2015,
author = {Agamennoni, G and Furgale, P and Siegwart, R},
file = {:home/matias/Documents/Mendeley Desktop/Agamennoni, Furgale, Siegwart/2015/Agamennoni, Furgale, Siegwart - 2015 - Self-tuning M-estimators.pdf:pdf},
isbn = {9781479969234},
pages = {4628--4635},
publisher = {IEEE},
title = {{Self-tuning M-estimators}},
year = {2015}
}
@article{Carlone2019,
abstract = {We study a visual-inertial navigation (VIN) problem in which a robot needs to estimate its state using an on-board camera and an inertial sensor, without any prior knowledge of the external environment. We consider the case in which the robot can allocate limited resources to VIN, due to tight computational constraints. Therefore, we answer the following question: under limited resources, what are the most relevant visual cues to maximize the performance of VIN? Our approach has four key ingredients. First, it is task-driven, in that the selection of the visual cues is guided by a metric quantifying the VIN performance. Second, it exploits the notion of anticipation, since it uses a simplified model for forward-simulation of robot dynamics, predicting the utility of a set of visual cues over a future time horizon. Third, it is efficient and easy to implement, since it leads to a greedy algorithm for the selection of the most relevant visual cues. Fourth, it provides formal performance guarantees: we leverage submodularity to prove that the greedy selection cannot be far from the optimal (combinatorial) selection. Simulations and real experiments on agile drones show that our approach ensures state-of-The-Art VIN performance while maintaining a lean processing time. In the easy scenarios, our approach outperforms appearance-based feature selection in terms of localization errors. In the most challenging scenarios, it enables accurate VIN while appearance-based feature selection fails to track robot's motion during aggressive maneuvers.},
archivePrefix = {arXiv},
arxivId = {1610.03344},
author = {Carlone, Luca and Karaman, Sertac},
doi = {10.1109/TRO.2018.2872402},
eprint = {1610.03344},
file = {:home/matias/Documents/Mendeley Desktop/Carlone, Karaman/2019/Carlone, Karaman - 2019 - Attention and Anticipation in Fast Visual-Inertial Navigation.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Aerial robotics,computer vision,sensor fusion,simultaneous localization and mapping (SLAM)},
number = {1},
pages = {1--20},
title = {{Attention and Anticipation in Fast Visual-Inertial Navigation}},
volume = {35},
year = {2019}
}
@inproceedings{Sturm2012,
abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools. {\textcopyright} 2012 IEEE.},
author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385773},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {573--580},
pmid = {6385773},
publisher = {IEEE},
title = {{A benchmark for the evaluation of RGB-D SLAM systems}},
url = {http://ieeexplore.ieee.org/document/6385773/},
year = {2012}
}
@article{Gal2016,
abstract = {Purpose: As user-generated content (UGC) is entering the news cycle alongside content captured by news professionals, it is important to detect misleading content as early as possible and avoid disseminating it. The purpose of this paper is to present an annotated dataset of 380 user-generated videos (UGVs), 200 debunked and 180 verified, along with 5,195 near-duplicate reposted versions of them, and a set of automatic verification experiments aimed to serve as a baseline for future comparisons. Design/methodology/approach: The dataset was formed using a systematic process combining text search and near-duplicate video retrieval, followed by manual annotation using a set of journalism-inspired guidelines. Following the formation of the dataset, the automatic verification step was carried out using machine learning over a set of well-established features. Findings: Analysis of the dataset shows distinctive patterns in the spread of verified vs debunked videos, and the application of state-of-the-art machine learning models shows that the dataset poses a particularly challenging problem to automatic methods. Research limitations/implications: Practical limitations constrained the current collection to three platforms: YouTube, Facebook and Twitter. Furthermore, there exists a wealth of information that can be drawn from the dataset analysis, which goes beyond the constraints of a single paper. Extension to other platforms and further analysis will be the object of subsequent research. Practical implications: The dataset analysis indicates directions for future automatic video verification algorithms, and the dataset itself provides a challenging benchmark. Social implications: Having a carefully collected and labelled dataset of debunked and verified videos is an important resource both for developing effective disinformation-countering tools and for supporting media literacy activities. Originality/value: Besides its importance as a unique benchmark for research in automatic verification, the analysis also allows a glimpse into the dissemination patterns of UGC, and possible telltale differences between fake and real content.},
author = {Papadopoulou, Olga and Zampoglou, Markos and Papadopoulos, Symeon and Kompatsiaris, Ioannis},
doi = {10.1108/OIR-03-2018-0101},
file = {::},
issn = {14684527},
journal = {Thesis},
keywords = {Dataset,Disinformation detection,Fake news,Social media,User-generated content,Video verification},
number = {1},
pages = {72--88},
title = {{Uncertainty in Deep Learning Yarin}},
volume = {43},
year = {2016}
}
@article{Bailey2006,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {::},
isbn = {1610-7438},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {108--117},
pmid = {1638022},
title = {{Simultaneous localization and mapping (SLAM): Part II}},
url = {http://ieeexplore.ieee.org/ielx5/100/35300/01678144.pdf?tp=&arnumber=1678144&isnumber=35300},
volume = {13},
year = {2006}
}
@article{Gaßmann2005,
abstract = {Proper navigation of walking machines in unstructured terrain requires the knowledge of the spatial position and orientation of the robot. There are many approaches for localization of mobile robots in outdoor environment, but their application to walking robots is rather rare. In particular, middle sized robots like LAURON III don't provide the possibility to carry large or heavy sensors. Due to many degrees of freedom of walking robots the localization task becomes a even more complex challenge. This paper discusses the problem, presents a method of resolution and describes the first steps towards a localization system for the six-legged walking robot LAURON III. {\textcopyright}2005 IEEE.},
author = {Ga{\ss}mann, Bernd and Zacharias, Franziska and Z{\"{o}}llner, J. Marius and Dillmann, R{\"{u}}diger},
doi = {10.1109/ROBOT.2005.1570322},
file = {::},
isbn = {078038914X},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Kalman Filter,Odometry,Walking robots},
number = {April},
pages = {1471--1476},
title = {{Localization of walking robots}},
volume = {2005},
year = {2005}
}
@inproceedings{Ramezani2020,
abstract = {In this paper we present a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College, Oxford. The dataset includes data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Additionally, we used a tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing $\sim$290 million points). Using the map we inferred centimeter-accurate 6 Degree of Freedom (DoF) ground truth for the position of the device for each LiDAR scan to enable better evaluation of LiDAR and vision localisation, mapping and reconstruction systems. This ground truth is the particular novel contribution of this dataset and we believe that it will enable systematic evaluation which many similar datasets have lacked. The dataset combines both built environments, open spaces and vegetated areas so as to test localization and mapping systems such as vision-based navigation, visual and LiDAR SLAM, 3D LIDAR reconstruction and appearance-based place recognition. The dataset is available at: ori.ox.ac.uk/datasets/newer-college-dataset},
archivePrefix = {arXiv},
arxivId = {2003.05691},
author = {Ramezani, Milad and Wang, Yiduo and Camurri, Marco and Wisth, David and Mattamala, Matias and Fallon, Maurice},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
eprint = {2003.05691},
file = {:home/matias/Documents/Mendeley Desktop/Ramezani et al/2020/Ramezani et al. - 2020 - The Newer College Dataset Handheld LiDAR, Inertial and Vision with Ground Truth.pdf:pdf},
title = {{The Newer College Dataset: Handheld LiDAR, Inertial and Vision with Ground Truth}},
url = {http://arxiv.org/abs/2003.05691},
year = {2020}
}
@article{Belter2013,
abstract = {Purpose - The purpose of this paper is to describe a novel application of the recently introduced concept from computer vision to self-localization of a walking robot in unstructured environments. The technique described in this paper enables a walking robot with a monocular vision system (single camera) to obtain precise estimates of its pose with regard to the six degrees of freedom. This capability is essential in search and rescue missions in collapsed buildings, polluted industrial plants, etc. Design/methodology/approach - The Parallel Tracking and Mapping (PTAM) algorithm and the Inertial Measurement Unit (IMU) are used to determine the 6-d.o.f. pose of a walking robot. Bundle-adjustment-based tracking and structure reconstruction are applied to obtain precise camera poses from the monocular vision data. The inclination of the robot's platform is determined by using IMU. The self-localization system is used together with the RRT-based motion planner, which allows to walk autonomously on rough, previously unknown terrain. The presented system operates on-line on the real hexapod robot. Efficiency and precision of the proposed solution are demonstrated by experimental data. Findings - The PTAM-based self-localization system enables the robot to walk autonomously on rough terrain. The software operates on-line and can be implemented on the robot's on-board PC. Results of the experiments show that the position error is small enough to allow robust elevation mapping using the laser scanner. In spite of the unavoidable feet slippages, the walking robot which uses PTAM for self-localization can precisely estimate its position and successfully recover from motion execution errors. Research limitations/implications - So far the presented self-localization system was tested in limited-scale indoor experiments. Experiments with more realistic outdoor scenarios are scheduled as further work. Practical implications - Precise self-localization may be one of the most important factors enabling the use of walking robots in practical USAR missions. The results of research on precise self-localization in 6-d.o.f. may be also useful for autonomous robots in other application areas: construction, agriculture, military. Originality/value - The vision-based self-localization algorithm used in the presented research is not new, but the contribution lies in its implementation/integration on a walking robot, and experimental evaluation in the demanding problem of precise self-localization in rough terrain. {\textcopyright} Emerald Group Publishing Limited.},
author = {Belter, Dominik and Skrzypczynski, Piotr},
doi = {10.1108/01439911311309924},
isbn = {0143-991x},
issn = {0143991X},
journal = {Industrial Robot},
keywords = {Feature tracking,Monocular vision,Motion,Programming and algorithm theory,Robots,Self-localization,Structure from motion,Walking robot},
number = {3},
pages = {229--237},
title = {{Precise self-localization of a walking robot on rough terrain using parallel tracking and mapping}},
volume = {40},
year = {2013}
}
@article{Suh2020,
abstract = {In this paper, we tackle the problem of pushing piles of small objects into a desired target set using visual feedback. Unlike conventional single-object manipulation pipelines, which estimate the state of the system parametrized by pose, the underlying physical state of this system is difficult to observe from images. Thus, we take the approach of reasoning directly in the space of images, and acquire the dynamics of visual measurements in order to synthesize a visual-feedback policy. We present a simple controller using an image-space Lyapunov function, and evaluate the closed-loop performance using three different class of models for image prediction: deep-learning-based models for image-to-image translation, an object-centric model obtained from treating each pixel as a particle, and a switched-linear system where an action-dependent linear map is used. Through results in simulation and experiment, we show that for this task, a linear model works surprisingly well -- achieving better prediction error, downstream task performance, and generalization to new environments than the deep models we trained on the same amount of data. We believe these results provide an interesting example in the spectrum of models that are most useful for vision-based feedback in manipulation, considering both the quality of visual prediction, as well as compatibility with rigorous methods for control design and analysis. Project site: https://sites.google.com/view/linear-visual-foresight/home},
archivePrefix = {arXiv},
arxivId = {2002.09093},
author = {Suh, H. J. Terry and Tedrake, Russ},
eprint = {2002.09093},
file = {:home/matias/Documents/Mendeley Desktop/Suh, Tedrake/2020/Suh, Tedrake - 2020 - The Surprising Effectiveness of Linear Models for Visual Foresight in Object Pile Manipulation.pdf:pdf},
keywords = {deformable objects,image,manipulation,piles of objects,prediction,vision-based control,visual foresight},
pages = {1--16},
title = {{The Surprising Effectiveness of Linear Models for Visual Foresight in Object Pile Manipulation}},
url = {http://arxiv.org/abs/2002.09093},
year = {2020}
}
@techreport{Booij2009,
abstract = {Vision-based localization, mapping and navigation is often performed by search- ing for corresponding image points and estimating the epipolar geometry. It is known that the possible relative poses of a cameramounted on amobile robot that moves over a planar ground floor, has two degrees of freedom. This report provides insight in the problem of estimating the exact planar robot pose difference using only two image point correspondences. We describe an algorithm which uses this minimal set of correspondences termed the Two-point algorithm. It is shown that sometimes two non-degenerate correspondences do not define a unique relative robot pose, but lead to two possible real solutions. The algorithm is especially useful as hypothesis generator for the well known RANSAC (RANdom SAmple Consensus) method. The algorithm is evaluated using both simulated data and data acquired by a mobile robot equipped with an omnidirectional camera. The improvement over existing methods is analogous to the improvement of the well known Five-point algorithm over other algorithms for general non-planar camera motions.},
address = {Amsterdam, The Netherlands},
author = {Booij, Olaf and Zivkovic, Zoran},
file = {::},
institution = {Intelligent Systems Laboratory Amsterdam, University of Amsterdam},
keywords = {computer vision,robot localization,robot pose estimation},
title = {{The planar two point algorithm}},
url = {http://hdl.handle.net/11245/1.308876},
year = {2009}
}
@article{Anderson2015a,
abstract = {This paper shows how to carry out batch continuous-time trajectory estimation for bodies translating and rotating in three-dimensional (3D) space, using a very efficient form of Gaussian-process (GP) regression. The method is fast, singularity-free, uses a physically motivated prior (the mean is constant body-centric velocity), and permits trajectory queries at arbitrary times through GP interpolation. Landmark estimation can be folded in to allow for simultaneous trajectory estimation and mapping (STEAM), a variant of SLAM. The key to making the approach efficient is to select a GP prior that has a block-tridiagonal inverse kernel matrix, resulting in fast inference (at a set of measurement or subset of key times) and interpolation (at a set of additional query times). We define the prior using a first-order stochastic differential equation (SDE) model on SE(3)×ℝ6; by explicitly estimating both pose and body-centric velocity, the Markov property of the SDE ensures the exact sparsity of the inverse kernel matrix. While the exactly sparse GP approach has been investigated for linear, time-varying SDEs on vectorspaces, our contribution lies in the extension of these results to the Lie group, SE(3). We validate the method experimentally on a STEAM problem involving a stereo camera translating and rotating in 3D while observing point landmarks.},
author = {Anderson, Sean and Barfoot, Timothy D.},
doi = {10.1109/IROS.2015.7353368},
file = {:home/matias/Documents/Mendeley Desktop/Anderson, Barfoot/2015/Anderson, Barfoot - 2015 - Full STEAM Ahead Exactly Sparse Gaussian Process Regression for Batch Continuous-Time Trajectory Estimation.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Estimation,Gaussian processes,Robots,Sensors,Three-dimensional displays,Trajectory,Uncertainty},
number = {3},
pages = {157--164},
publisher = {IEEE},
title = {{Full STEAM ahead: Exactly sparse Gaussian process regression for batch continuous-time trajectory estimation on SE(3)}},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Faragasso2013,
abstract = {We present a control-based approach for visual navigation of humanoid robots in office-like environments. In particular, the objective of the humanoid is to follow a maze of corridors, walking as close as possible to their center to maximize motion safety. Our control algorithm is inspired by a technique originally designed for unicycle robots and extended here to cope with the presence of turns and junctions. The feedback signals computed for the unicycle are transformed to inputs that are suited for the locomotion system of the humanoid, producing a natural, human-like behavior. Experimental results for the humanoid robot NAO are presented to show the validity of the approach, and in particular the successful extension of the controller to turns and junctions. {\textcopyright} 2013 IEEE.},
author = {Faragasso, Angela and Oriolo, Giuseppe and Paolillo, Antonio and Vendittelli, Marilena},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631021},
isbn = {9781467356411},
issn = {10504729},
keywords = {Humanoid Robots,Visual Navigation},
month = {may},
number = {SEPTEMBER},
pages = {3190--3195},
publisher = {IEEE},
title = {{Vision-based corridor navigation for humanoid robots}},
url = {http://ieeexplore.ieee.org/document/6631021/},
year = {2013}
}
@article{Schneider2018,
abstract = {Robust and accurate visual-inertial estimation is crucial to many of today's challenges in robotics. Being able to localize against a prior map and obtain accurate and drift-free pose estimates can push the applicability of such systems even further. Most of the currently available solutions, however, either focus on a single session use case, lack localization capabilities, or do not provide an end-to-end pipeline. We believe that only a complete system, combining state-of-the-art algorithms, scalable multisession mapping tools, and a flexible user interface, can become an efficient research platform. We, therefore, present maplab, an open, research-oriented visual-inertial mapping framework for processing and manipulating multisession maps, written in C++. On the one hand, maplab can be seen as a ready-to-use visual-inertial mapping and localization system. On the other hand, maplab provides the research community with a collection of multisession mapping tools that include map merging, visual-inertial batch optimization, and loop closure. Furthermore, it includes an online frontend that can create visual-inertial maps and also track a global drift-free pose within a localization map. In this letter, we present the system architecture, five use cases, and evaluations of the system on public datasets. The source code of maplab is freely available for the benefit of the robotics research community.},
archivePrefix = {arXiv},
arxivId = {1711.10250},
author = {Schneider, Thomas and Dymczyk, Marcin and Fehr, Marius and Egger, Kevin and Lynen, Simon and Gilitschenski, Igor and Siegwart, Roland},
doi = {10.1109/LRA.2018.2800113},
eprint = {1711.10250},
file = {:home/matias/Documents/Mendeley Desktop/Schneider et al/2018/Schneider et al. - 2018 - Maplab An Open Framework for Research in Visual-Inertial Mapping and Localization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Mapping,localization,visual-based navigation},
number = {3},
pages = {1418--1425},
title = {{Maplab: An Open Framework for Research in Visual-Inertial Mapping and Localization}},
volume = {3},
year = {2018}
}
@article{Pathak2019,
abstract = {Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action, and there is another limb nearby, the latter is magnetically connected to the ‘parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these dynamic and modular agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the structure of the agent, compared to static and monolithic baselines. Project video and code are available at https://pathak22.github.io/modular-assemblies/.},
archivePrefix = {arXiv},
arxivId = {1902.05546},
author = {Pathak, Deepak and Lu, Chris and Darrell, Trevor and Isola, Phillip and Efros, Alexei A.},
eprint = {1902.05546},
file = {:home/matias/Documents/Mendeley Desktop/Pathak et al/2019/Pathak et al. - 2019 - Learning to Control Self-Assembling Morphologies A Study of Generalization via Modularity.pdf:pdf},
journal = {arXiv},
number = {NeurIPS},
title = {{Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity}},
year = {2019}
}
@article{Zhang2016,
abstract = {Information Retrieval (IR) is concerned with indexing and retrieving documents including information relevant to a user's information need. Relevance Feedback (RF) is a class of effective algorithms for improving Information Retrieval (IR) and it consists of gathering further data representing the user's information need and automatically creating a new query. In this paper, we propose a class of RF algorithms inspired by quantum detection to re-weight the query terms and to re-rank the document retrieved by an IR system. These algorithms project the query vector on a subspace spanned by the eigenvector which maximizes the distance between the distribution of quantum probability of relevance and the distribution of quantum probability of non-relevance. The experiments showed that the RF algorithms inspired by quantum detection can outperform the state-of-the-art algorithms.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Melucci, Massimo},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {::},
isbn = {1506.02142},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Information retrieval,quantum detection,quantum mechanics,relevance feedback},
number = {4},
pages = {1022--1034},
pmid = {88045},
title = {{Relevance Feedback Algorithms Inspired by Quantum Detection}},
url = {http://arxiv.org/abs/1611.03530},
volume = {28},
year = {2016}
}
@article{Bloesch2017a,
abstract = {This letter deals with recursive filtering for dynamic systems where an explicit process model is not easily devisable. Most Bayesian filters assume the availability of such an explicit process model, and thus may require additional assumptions or fail to properly leverage all available information. In contrast, we propose a filter that employs a purely residual-based modeling of the available information and thus achieves higher modeling flexibility. While this letter is related to the descriptor Kalman filter, it also represents a step toward batch optimization and allows the integration of further techniques, such as robust weighting for outlier rejection. We derive recursive filter equations that exhibit similar computational complexity when compared to their Kalman filter counterpart - the extended information filter. The applicability of the proposed approach is experimentally confirmed on two different real mobile robotic state estimation problems.},
author = {Bloesch, Michael and Burri, Michael and Sommer, Hannes and Siegwart, Roland and Hutter, Marco},
doi = {10.1109/LRA.2017.2776340},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Sensor fusion,localization,probability and statistical methods},
number = {1},
pages = {573--580},
title = {{The Two-State Implicit Filter Recursive Estimation for Mobile Robots}},
url = {http://ieeexplore.ieee.org/document/8118116/},
volume = {3},
year = {2018}
}
@book{Harari2014,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Udbhav and Rachna},
booktitle = {Journal of the Practice of Cardiovascular Sciences},
doi = {10.4103/jpcs.jpcs_19_17},
isbn = {978-0062316097},
issn = {2395-5414},
number = {1},
pages = {61},
publisher = {Harper},
title = {{Sapiens: A brief history of humankind}},
volume = {3},
year = {2017}
}
@inproceedings{Tsotsos2012,
abstract = {We describe an ego-motion estimation system developed specifically for humanoid robots, integrating visual and inertial sensors. It addresses the challenge of significant scale changes due to forward motion with a finite field of view by using recent sparse multi-scale feature tracking techniques. Additionally, it addresses the challenge of long-range temporal correlation due to walking gaits by employing a kinematic-statistical model that does not require accurate knowledge of the robot dynamics and calibration. Our system achieves performance comparable to the state of the art at a fraction of the (inertial measurement unit) cost, on a challenging dataset that we have created. {\textcopyright} 2012 IEEE.},
author = {Tsotsos, Konstantine and Pretto, Alberto and Soatto, Stefano},
booktitle = {IEEE-RAS International Conference on Humanoid Robots},
doi = {10.1109/HUMANOIDS.2012.6651597},
isbn = {9781467313698},
issn = {21640572},
month = {nov},
pages = {704--711},
publisher = {IEEE},
title = {{Visual-inertial ego-motion estimation for humanoid platforms}},
url = {http://ieeexplore.ieee.org/document/6651597/},
year = {2012}
}
@article{Berczi2015,
abstract = {This paper presents an approach to learning robot terrain assessment from human demonstration. An operator drives a robot for a short period of time, supervising the gathering of traversable and untraversable terrain data. After this initial training period, the robot can then predict the traversability of new terrain based on its experiences. We improve on current methods in two ways: first, we maintain a richer (higher-dimensional) representation of the terrain that is better able to distinguish between different training examples. Second, we use a Gaussian-process classifier for terrain assessment due to its superior introspective abilities (leading to better uncertainty estimates) when compared to other classifier methods in the literature. Our method is tested on real data and shown to outperform current methods both in classification accuracy and uncertainty estimation.},
author = {Berczi, Laszlo Peter and Posner, Ingmar and Barfoot, Timothy D.},
doi = {10.1109/ICRA.2015.7139637},
file = {:home/matias/Documents/Mendeley Desktop/Berczi, Posner, Barfoot/2015/Berczi, Posner, Barfoot - 2015 - Learning to Assess Terrain from Human Demonstration Using an Introspective Gaussian-Process Classifier.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {3178--3185},
publisher = {IEEE},
title = {{Learning to assess terrain from human demonstration using an introspective Gaussian-process classifier}},
volume = {2015-June},
year = {2015}
}
@incollection{Engel2014,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on , thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU. {\textcopyright} 2014 Springer International Publishing.},
author = {Engel, Jakob and Sch{\"{o}}ps, Thomas and Cremers, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10605-2_54},
isbn = {9783319106045},
issn = {16113349},
number = {PART 2},
pages = {834--849},
title = {{LSD-SLAM: Large-Scale Direct monocular SLAM}},
url = {http://link.springer.com/10.1007/978-3-319-10605-2_54},
volume = {8690 LNCS},
year = {2014}
}
@article{Cieslewski2018,
abstract = {The extraction and matching of interest points is a prerequisite for many geometric computer vision problems. Traditionally, matching has been achieved by assigning descriptors to interest points and matching points that have similar descriptors. In this paper, we propose a method by which interest points are instead already implicitly matched at detection time. With this, descriptors do not need to be calculated, stored, communicated, or matched any more. This is achieved by a convolutional neural network with multiple output channels and can be thought of as a collection of a variety of detectors, each specialised to specific visual features. This paper describes how to design and train such a network in a way that results in successful relative pose estimation performance despite the limitation on interest point count. While the overall matching score is slightly lower than with traditional methods, the approach is descriptor free and thus enables localization systems with a significantly smaller memory footprint and multi-agent localization systems with lower bandwidth requirements. The network also outputs the confidence for a specific interest point resulting in a valid match. We evaluate performance relative to state-of-the-art alternatives.},
archivePrefix = {arXiv},
arxivId = {1811.10681},
author = {Cieslewski, Titus and Bloesch, Michael and Scaramuzza, Davide},
eprint = {1811.10681},
file = {:home/matias/Documents/Mendeley Desktop/Cieslewski, Bloesch, Scaramuzza/2018/Cieslewski, Bloesch, Scaramuzza - 2018 - Matching features without descriptors Implicitly matched interest points.pdf:pdf},
journal = {30th British Machine Vision Conference 2019, BMVC 2019},
title = {{Matching features without descriptors: Implicitly matched interest points}},
url = {http://arxiv.org/abs/1811.10681},
year = {2020}
}
@article{Dedonato2015,
abstract = {The DARPA Robotics Challenge (DRC) requires teams to integrate mobility, manipulation, and perception to accomplish several disaster-response tasks.We describe our hardware choices and software architecture, which enable human-in-the-loop control of a 28 degree-of-freedom ATLAS humanoid robot over a limited bandwidth link. We discuss our methods, results, and lessons learned for the DRC Trials tasks. The effectiveness of our system architecture was demonstrated as the WPI-CMU DRC Team scored 11 out of a possible 32 points, ranked seventh (out of 16) at the DRC Trials, and was selected as a finalist for the DRC Finals.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {DeDonato, Mathew and Dimitrov, Velin and Du, Ruixiang and Giovacchini, Ryan and Knoedler, Kevin and Long, Xianchao and Polido, Felipe and Gennert, Michael A. and Padir, Taşkin and Feng, Siyuan and Moriguchi, Hirotaka and Whitman, Eric and Xinjilefu, X. and Atkeson, Christopher G.},
doi = {10.1002/rob.21567},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {275--292},
pmid = {22164016},
title = {{Human-in-the-loop control of a humanoid robot for disaster response: A report from the DARPA robotics challenge trials}},
volume = {32},
year = {2015}
}
@inproceedings{Jaegle2015,
abstract = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.},
archivePrefix = {arXiv},
arxivId = {1602.04886},
author = {Jaegle, Andrew and Phillips, Stephen and Daniilidis, Kostas},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487206},
eprint = {1602.04886},
isbn = {9781467380263},
issn = {10504729},
month = {may},
pages = {773--780},
publisher = {IEEE},
title = {{Fast, robust, continuous monocular egomotion computation}},
url = {http://arxiv.org/abs/1602.04886 http://ieeexplore.ieee.org/document/7487206/},
volume = {2016-June},
year = {2016}
}
@phdthesis{Vlasbom2014,
abstract = {This report discusses the design, implementation and evaluation of a state estimator for bipedal robots, making use of the robot's full-body dynamics. The Unscented Kalman Filter is chosen to fuse the predictions with the measurements. To evaluate the new filter design, a second estimator is implemented as a benchmark, using a conventional Linear Inverted Pendulum Model for the prediction and a Kalman Filter for data fusion. Simulations of three different motions show how the CoM esti- mates of both methods deal with different kinds of uncertainties and an experiment was done as a preliminary application on the TUlip bipedal robot. Estimation with a pendulum model gave incorrect estimates in the presence of sensory bias. Using the full-body dynamics, however, the effect of sensory bias was reduced significantly. Moreover, the proposed method was shown to be robust against parametric errors. However, the performance varied between different motions, making it hard to tune. Improving the filter to inherently work for various movements as well as making the filter numerically efficient enough for online implementation still requires further research.},
author = {Vlasbom, E},
keywords = {msc,subject,thesis},
school = {Delft University of Technology},
title = {{Nonlinear State Estimation for a Bipedal Robot}},
year = {2014}
}
@article{Bae2017,
abstract = {In this research, a new state estimator based on moving horizon estimation theory is suggested for the humanoid robot state estimation. So far, there are almost no studies on the moving horizon estimator (MHE)-based humanoid state estimator. Instead, a large number of humanoid state estimators based on the Kalman filter (KF) have been proposed. However, such estimators cannot guarantee optimality when the system model is nonlinear or when there is a non-Gaussian modeling error. In addition, with KF, it is difficult to incorporate inequality constraints. Since a humanoid is a complex system, its mathematical model is normally nonlinear, and is limited in its ability to characterize the system accurately. Therefore, KF-based humanoid state estimation has unavoidable limitations. To overcome these limitations, we propose a new approach to humanoid state estimation by using a MHE. It can accommodate not only nonlinear systems and constraints, but also it can partially cope with non-Gaussian modeling error. The proposed estimator framework facilitates the use of a simple model, even in the presence of a large modeling error. In addition, it can estimate the humanoid state more accurately than a KF-based estimator. The performance of the proposed approach was verified experimentally.},
author = {Bae, Hyoin and Oh, Jun Ho},
doi = {10.1080/01691864.2017.1326317},
file = {::},
issn = {15685535},
journal = {Advanced Robotics},
keywords = {Humanoid state estimation,Kalman filter,constrained state estimation,humanoid robot,moving horizon estimation,robust state estimation},
number = {13},
pages = {695--705},
publisher = {Taylor & Francis},
title = {{Humanoid state estimation using a moving horizon estimator}},
url = {https://www.tandfonline.com/doi/full/10.1080/01691864.2017.1326317},
volume = {31},
year = {2017}
}
@article{Michel2006,
abstract = {As navigation autonomy becomes an increasingly important research topic for biped humanoid robots, efficient approaches to perception and mapping that are suited to the unique characteristics of humanoids and their typical operating environments will be required. This paper presents a system for online environment reconstruction that utilizes both external sensors for global localization, and on-body sensors for detailed local mapping. An external optical motion capture system is used to accurately localize on-board sensors that integrate successive 2D views of a calibrated camera and range measurements from a SwissRanger SR-2 time-of-flight sensor to construct global environment maps in real-time. Environment obstacle geometry is encoded in 2D occupancy grids and 2.5D height maps for navigation planning. We present an on-body implementation for the HRP-2 humanoid robot that, combined with a footstep planner, enables the robot to autonomously traverse dynamic environments containing unpredictably moving obstacles. {\textcopyright} 2006 IEEE.},
author = {Michel, Philipp and Chestnutt, Joel and Kagami, Satoshi and Nishiwaki, Koichi and Kuffner, James and Kanade, Takeo},
doi = {10.1109/ROBOT.2006.1642171},
file = {::},
isbn = {0780395069},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {May},
pages = {3089--3094},
title = {{Online environment reconstruction for biped navigation}},
volume = {2006},
year = {2006}
}
@article{Zielinski2018,
abstract = {In this paper, we propose a new mammal-like mechanical design of the compliant robotic leg. We propose the application of elastic components to reduce the mechanical impact during landing phase and protect the gearboxes of the servomotors. We also use the elastic tendon which stores the energy in springs. The stored energy is then released at the beginning of the flight phase to increase the height of the jump. We propose and verify the dynamic model of the leg. Finally, in the series of experiments, we show the mechanical properties of the leg.},
author = {Zieli{\'{n}}ski, Micha{\l} and Belter, Dominik},
doi = {10.1007/978-3-319-77179-3_47},
file = {::},
isbn = {9783319771786},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Elastic tendon,Legged robots,Serial elastic actuators},
pages = {500--509},
title = {{Mechanical design and control of compliant leg for a quadruped robot}},
volume = {743},
year = {2018}
}
@article{Hafner2020,
abstract = {We introduce a unified objective for action and perception of intelligent agents. Extending representation learning and control, we minimize the joint divergence between the world and a target distribution. Intuitively, such agents use perception to align their beliefs with the world, and use actions to align the world with their beliefs. Minimizing the joint divergence to an expressive target maximizes the mutual information between the agent's representations and inputs, thus inferring representations that are informative of past inputs and exploring future inputs that are informative of the representations. This lets us derive intrinsic objectives, such as representation learning, information gain, empowerment, and skill discovery from minimal assumptions. Moreover, interpreting the target distribution as a latent variable model suggests expressive world models as a path toward highly adaptive agents that seek large niches in their environments, while rendering task rewards optional. The presented framework provides a common language for comparing a wide range of objectives, facilitates understanding of latent variables for decision making, and offers a recipe for designing novel objectives. We recommend deriving future agent objectives from the joint divergence to facilitate comparison, to point out the agent's target distribution, and to identify the intrinsic objective terms needed to reach that distribution.},
archivePrefix = {arXiv},
arxivId = {2009.01791},
author = {Hafner, Danijar and Ortega, Pedro A. and Ba, Jimmy and Parr, Thomas and Friston, Karl and Heess, Nicolas},
eprint = {2009.01791},
file = {:home/matias/Documents/Mendeley Desktop/Hafner et al/2020/Hafner et al. - 2020 - Action and Perception as Divergence Minimization.pdf:pdf},
pages = {1--23},
title = {{Action and Perception as Divergence Minimization}},
url = {http://arxiv.org/abs/2009.01791},
year = {2020}
}
@article{Heeger1992,
abstract = {As an observer moves and explores the environment, the visual stimulation in his/her eye is constantly changing. Somehow he/she is able to perceive the spatial layout of the scene, and to discern his/her movement through space. Computational vision researchers have been trying to solve this problem for a number of years with only limited success. It is a difficult problem to solve because the optical flow field is nonlinearly related to the 3D motion and depth parameters. Here, we show that the nonlinear equation describing the optical flow field can be split by an exact algebraic manipulation to form three sets of equations. The first set relates the flow field to only the translational component of 3D motion. Thus, depth and rotation need not be known or estimated prior to solving for translation. Once the translation has been recovered, the second set of equations can be used to solve for rotation. Finally, depth can be estimated with the third set of equations, given the recovered translation and rotation. The algorithm applies to the general case of arbitrary motion with respect to an arbitrary scene. It is simple to compute, and it is plausible biologically. The results reported in this article demonstrate the potential of our new approach, and show that it performs favorably when compared with two other well-known algorithms. {\textcopyright} 1992 Kluwer Academic Publishers.},
author = {Heeger, David J. and Jepson, Allan D.},
doi = {10.1007/BF00128130},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {95--117},
title = {{Subspace methods for recovering rigid motion I: Algorithm and implementation}},
volume = {7},
year = {1992}
}
@article{Booij2011,
abstract = {A common task for a mobile robot, is to build a map of its environment. It needs such a map to localize itself in the environment, and to navigate to a specific place. In this thesis, we propose methods to build such a map on the basis of images. These images are taken with a camera that is mounted onto the robot. The methods are specifically developed for robots operating in real home environments, such as autonomous vacuum cleaners. Therefore, all methods are evaluated on image sets which were acquired in real home environments. Building a map is particularly difficult in human inhabited home environments. We can identify two main challenges. First, the captured images are often of poor quality. As opposed to brightly lit office spaces and outdoor environments, home environments usually have bad lighting condition. On top of this, humans can block the view of the robot and change the appearance of the environment, for example by switching off lights. Such circumstances call for a robust mapping approach. Second, the robot should be able to communicate with humans about the map and add labels, such as `kitchen'' or `chair'', to the map. To allow for such communication, during map building, the method should be efficient enough to perform in real-time. Different types of maps have been proposed to solve this robot task. In this thesis, we focus on a specific type of map termed the `view-based topological map''. In Chapter 2, we see that this is a commonly used map in the field of vision-based robotics. It can be seen as a graph for which each node denotes an image and each link between two images denotes that they depict an overlapping part of the environment. The basic task of map-building and localization is finding for a new image all other images that could link with it. This later task is called `view-based data association'' and depends on the ability to compare images with each other. Two images can be compared by automatically extracting a set of salient image points from both of them and matching these sets to obtain a number of image point correspondences. If two images depict an overlapping part of the environment, then the spatial layout of the correspondences depends greatly on the relative robot pose. This relative pose can be parameterized by a rotation and a translation up to scale. To make image comparison robust and efficient, it is beneficial to incorporate constraints on the possible robot poses. We thoroughly investigate how the assumption that the robot moves over a planar surface can be used to improve existing algorithms. We derive an algorithm that computes the planar relative pose given only two correspondences and combine it with the well-known RANSAC algorithm (RANdom SAmple Consensus). On top of that, we propose an efficient method based on the Hough Transform that determines a probability density over the space of all planar relative poses. On the one hand, this can be used to estimate the relative pose given two images. In Chapter 3, we show that our relative pose solution is more accurate and efficient than the popular RANSAC-based method. On the other hand, it can be used as an image similarity measure, by estimating the probability that robot pose is indeed correct. In Chapter 4, we show that this image similarity measure is better than existing measures used for both topological view-based mapping and the related task of image-based place recognition. Even if an efficient image comparison method is used, for very large view-based maps with thousands of images, it becomes infeasible to compare with all images. In Chapter 5, we propose a method that finds a set of key images that best represents the complete set of images of the map. This set of key images defines a subgraph of the complete graph representing the map. It is based on a graph theoretic technique called the `Connected Dominating Set'' (CDS). We explain why this algorithm is the optimal choice and how it can be used for view-based mapping and localization procedures. The results of these procedures are close to the ones obtained when comparing with all images, while being an order of magnitude faster. Combining the image comparison technique with the CDS method, results in scalable real-time data association. We show, in two appendices, that this can be used to perform real-time interactive map-building and goal-directed navigation. In addition, the proposed data association method could be used to improve existing SLAM systems (Simultaneous Localization And Mapping). This would allow for efficient and robust 3D geometric map-building of real home environments.},
author = {Booij, Olaf},
title = {{View-based mapping for wheeled robots}},
year = {2011}
}
@article{Sarlin2019,
abstract = {Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: We first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization.},
archivePrefix = {arXiv},
arxivId = {1812.03506},
author = {Sarlin, Paul Edouard and Cadena, Cesar and Siegwart, Roland and Dymczyk, Marcin},
doi = {10.1109/CVPR.2019.01300},
eprint = {1812.03506},
file = {:home/matias/Documents/Mendeley Desktop/Sarlin et al/2019/Sarlin et al. - 2019 - From coarse to fine Robust hierarchical localization at large scale.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Datasets and Evaluation,Deep Learning,Recognition: Detection,Retrieval,Robotics + Driving,Vision A},
pages = {12708--12717},
title = {{From coarse to fine: Robust hierarchical localization at large scale}},
volume = {2019-June},
year = {2019}
}
@article{Jones,
author = {Jones, Eagle and Vedaldi, Andrea and Soatto, Stefano},
doi = {10.1.1.183.9572},
journal = {Workshop on Dynamical Vision},
title = {{Inertial structure from motion with autocalibration}},
url = {http://vision.jhu.edu/iccv2007-wdv/WDV07-jones.pdf},
year = {2007}
}
@article{Wang2020,
abstract = {Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. We call the resulting descriptors CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak supervision, CAPS descriptors outperform even prior fully-supervised descriptors and achieve state-of-the-art performance on a variety of geometric tasks.},
archivePrefix = {arXiv},
arxivId = {2004.13324},
author = {Wang, Qianqian and Zhou, Xiaowei and Hariharan, Bharath and Snavely, Noah},
eprint = {2004.13324},
file = {:home/matias/Documents/Mendeley Desktop/Wang et al/2020/Wang et al. - 2020 - Learning Feature Descriptors using Camera Pose Supervision.pdf:pdf},
keywords = {camera pose,correspondence,feature descriptors,image matching,local features},
pages = {1--18},
title = {{Learning Feature Descriptors using Camera Pose Supervision}},
url = {http://arxiv.org/abs/2004.13324},
year = {2020}
}
@article{Dissanayake2001,
abstract = {The simultaneous localization and map building (SLAM) problem asks if it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and then to incrementally build a map of this environment while simultaneously using this map to compute absolute vehicle location. Starting from the estimation-theoretic foundations of this problem developed in [1]-[3], this paper proves that a solution to the SLAM problem is indeed possible. The underlying structure of the SLAM problem is first elucidated. A proof that the estimated map converges monotonically to a relative map with zero uncertainty is then developed. It is then shown that the absolute accuracy of the map and the vehicle location reach a lower bound defined only by the initial vehicle uncertainty. Together, these results show that it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and, using relative observations only, incrementally build a perfect map of the world and to compute simultaneously a bounded estimate of vehicle location. This paper also describes a substantial implementation of the SLAM algorithm on a vehicle operating in an outdoor environment using millimeter-wave (MMW) radar to provide relative map observations. This implementation is used to demonstrate how some key issues such as map management and data association can be handled in a practical environment. The results obtained are cross-compared with absolute locations of the map landmarks obtained by surveying. In conclusion, this paper discusses a number of key issues raised by the solution to the SLAM problem including suboptimal map-building algorithms and map management.},
author = {{Gamini Dissanayake}, M. W.M. and Newman, Paul and Clark, Steven and Durrant-Whyte, Hugh F. and Csorba, M.},
doi = {10.1109/70.938381},
isbn = {1042-296X},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Autonomous navigation,Millimeter wave radar,Simultaneous localization and map building},
number = {3},
pages = {229--241},
title = {{A solution to the simultaneous localization and map building (SLAM) problem}},
volume = {17},
year = {2001}
}
@article{Sardain2004,
abstract = {In the area of biped robot research, much progress has been made in the past few years. However, some difficulties remain to be dealt with, particularly about the implementation of fast and dynamic walking gaits, in other words anthropomorphic gaits, especially on uneven terrain. In this perspective, both concepts of center of pressure (CoP) and zero moment point (ZMP) are obviously useful. In this paper, the two concepts are strictly defined, the CoP with respect to ground-feet contact forces, the ZMP with respect to gravity plus inertia forces. Then, the coincidence of CoP and ZMP is proven, and related control aspects are examined. Finally, a virtual CoP-ZMP is defined, allowing us to extend the concept when walking on uneven terrain. This paper is a theoretical study. Experimental results are presented in a companion paper, analyzing the evolution of the ground contact forces obtained from a human walker wearing robot feet as shoes. {\textcopyright} 2004 IEEE.},
author = {Sardain, Philippe and Bessonnet, Guy},
doi = {10.1109/TSMCA.2004.832811},
file = {::},
isbn = {1083-4427 VO - 34},
issn = {10834427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans.},
keywords = {Biped robot,CoP,ZMP,center of pressure,mechanical feet,uneven terrain,walking,zero moment point},
number = {5},
pages = {630--637},
title = {{Forces acting on a biped robot. Center of pressure - Zero moment point}},
volume = {34},
year = {2004}
}
@article{Wright2015,
abstract = {Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.},
archivePrefix = {arXiv},
arxivId = {1502.04759},
author = {Wright, Stephen J.},
doi = {10.1007/s10107-015-0892-3},
eprint = {1502.04759},
isbn = {0025-5610},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Coordinate descent,Parallel numerical computing,Randomized algorithms},
number = {1},
pages = {3--34},
pmid = {22081779},
title = {{Coordinate descent algorithms}},
volume = {151},
year = {2015}
}
@article{Orthey2020,
abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
archivePrefix = {arXiv},
arxivId = {2007.09435},
author = {Real, Fabio and Batou, Anas and Ritto, Thiago and Desceliers, Christophe},
doi = {10.1177/ToBeAssigned},
eprint = {2007.09435},
file = {:home/matias/Documents/Mendeley Desktop/Orthey, Akbar, Toussaint/2020/Orthey, Akbar, Toussaint - 2020 - Stochastic modeling for hysteretic bit–rock interaction of a drill string under torsional vibrations.pdf:pdf},
journal = {Journal of Vibration and Control},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
month = {jul},
pages = {107754631982824},
title = {{Stochastic modeling for hysteretic bit–rock interaction of a drill string under torsional vibrations}},
url = {http://arxiv.org/abs/2007.09435},
year = {2019}
}
@article{Civera2008,
abstract = {We present a new parametrization for point features within monocular simultaneous localization and mapping (SLAM) that permits efficient and accurate representation of uncertainty during undelayed initialization and beyond, all within the standard extended Kalman filter (EKF). The key concept is direct parametrization of the inverse depth of features relative to the camera locations from which they were first viewed, which produces measurement equations with a high degree of linearity. Importantly, our parametrization can cope with features over a huge range of depths, even those that are so far from the camera that they present little parallax during motion - maintaining sufficient representative uncertainty that these points retain the opportunity to "come in" smoothly from infinity if the camera makes larger movements. Feature initialization is undelayed in the sense that even distant features are immediately used to improve camera motion estimates, acting initially as bearing references but not permanently labeled as such. The inverse depth parametrization remains well behaved for features at all stages of SLAM processing, but has the drawback in computational terms that each point is represented by a 6-D state vector as opposed to the standard three of a Euclidean XYZ representation. We show that once the depth estimate of a feature is sufficiently accurate, its representation can safely be converted to the Euclidean XYZ form, and propose a linearity index that allows automatic detection and conversion to maintain maximum efficiency - only low parallax features need be maintained in inverse depth form for long periods. We present a real-time implementation at 30 Hz, where the parametrization is validated in a fully automatic 3-D SLAM system featuring a handheld single camera with no additional sensing. Experiments show robust operation in challenging indoor and outdoor environments with a very large ranges of scene depth, varied motion, and also real time 360° loop closing. {\textcopyright} 2008 IEEE.},
author = {Civera, Javier and Davison, Andrew J. and Montiel, J. M.Mart{\'{i}}nez},
doi = {10.1109/TRO.2008.2003276},
file = {::},
isbn = {1552-3098 VO - 24},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Monocular simultaneous localization and mapping (S,Real-time vision},
month = {oct},
number = {5},
pages = {932--945},
title = {{Inverse depth parametrization for monocular SLAM}},
url = {http://ieeexplore.ieee.org/document/4637878/},
volume = {24},
year = {2008}
}
@article{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill},
file = {:home/matias/Documents/Mendeley Desktop/Dalal, Triggs/2005/Dalal, Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
journal = {CVPR '05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1},
keywords = {human-detection,local-feature,object-detection},
pages = {886--893},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {citeulike-article-id:3047126%5Cnhttp://dx.doi.org/10.1109/CVPR.2005.177},
year = {2005}
}
@incollection{Mattamala2015,
abstract = {This paper presents an efficient active vision system which controls the head of a humanoid soccer robot. The system explicitly separates static information obtained offline from the map, and dynamic information from mobile objects, such as the ball and other players. Both types of information are mapped and handled in a simplified structure called action space, which assigns scores to each possible action of the robot's head. Scores also consider the movement constraints of the robot's head. Due to its simplicity and efficient information handling, the proposed active vision system is able to run in real-time in less than 1ms. The performance of the system in a robot soccer environment is tested via simulation and real experiments.},
author = {Mattamala, Mat{\'{i}}as and Villegas, Constanza and Y{\'{a}}{\~{n}}ez, Jos{\'{e}} Miguel and Cano, Pablo and Ruiz-Del-Solar, Javier},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-29339-4_26},
file = {::},
isbn = {9783319293387},
issn = {16113349},
keywords = {Active vision,Camera control,RoboCup,Robot soccer,Self-localization,Standard platform league},
number = {August},
pages = {316--327},
publisher = {Springer},
title = {{A dynamic and efficient active vision system for humanoid soccer robots}},
url = {http://link.springer.com/10.1007/978-3-319-29339-4_26},
volume = {9513},
year = {2015}
}
@article{Hornung2009,
abstract = {Cameras are popular sensors for robot navigation tasks such as localization as they are inexpensive, lightweight, and provide rich data. However, fast movements of a mobile robot typically reduce the performance of vision-based localization systems due to motion blur. In this paper, we present a reinforcement learning approach to choose appropriate velocity profiles for vision-based navigation. The learned policy minimizes the time to reach the destination and implicitly takes the impact of motion blur on observations into account. To reduce the size of the resulting policies, which is desirable in the context of memory-constrained systems, we compress the learned policy via a clustering approach. Extensive simulated and real-world experiments demonstrate that our learned policy significantly outperforms any policy that uses a constant velocity. We furthermore show, that our policy is applicable to different environments. Additional experiments demonstrate that our compressed policies do not result in a performance loss compared to the originally learned policy. {\textcopyright} 2009 IEEE.},
author = {Hornung, Armin and Strasdat, Hauke and Bennewitz, Maren and Burgard, Wolfram},
doi = {10.1109/IROS.2009.5354634},
isbn = {9781424438044},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
pages = {4590--4595},
title = {{Learning efficient policies for vision-based navigation}},
year = {2009}
}
@article{Troiani2013,
abstract = {We propose a novel method to estimate the relative motion between two consecutive camera views, which only requires the observation of a single feature in the scene and the knowledge of the angular rates from an inertial measurement unit, under the assumption that the local camera motion lies in a plane perpendicular to the gravity vector. Using this 1-point motion parametrization, we provide two very efficient algorithms to remove the outliers from the feature-matching process. Thanks to their inherent efficiency, the proposed algorithms are very suitable for computationally-limited robots. We test the proposed approaches on both synthetic and real data, using video footage from a small flying quadrotor. We show that our methods outperform standard RANSAC-based implementations by up to two orders of magnitude in speed, while being able to identify the majority of the inliers. {\textcopyright} 2013 IEEE.},
author = {Troiani, Chiara and Martinelli, Agostino and Laugier, Christian and Scaramuzza, Davide},
doi = {10.1109/ECMR.2013.6698813},
isbn = {9781479902637},
journal = {2013 European Conference on Mobile Robots, ECMR 2013 - Conference Proceedings},
pages = {13--18},
title = {{1-Point-based monocular motion estimation for computationally-limited micro aerial vehicles}},
year = {2013}
}
@article{Ahn2012,
abstract = {This paper addresses a vision-based 3D motion estimation framework for humanoid robots, which copes with human-like walking pattern. A humanoid robot, called Roboray, is designed for dynamic walking control with heel-toe motion like a human. In spite of stability and energy efficiency of the dynamic walking, it accompanies larger swaying motion and more uncertainty in camera movement than the conventional ZMP (Zero Moment Point)-based walking does. The framework effectively uses on-board odometry information from the robot to improve the performance of the visionbased motion estimation. To accomplish this, we propose an onboard odometry filter which fuses kinematic odometry, visual odometry, and raw IMU data. And the odometry filter is combined with vision-based SLAM to provide accurate motion model, so it enhances the SLAM estimates. Experimental results in indoor environment verify that the framework can successfully estimate the pose of Roboray in real-time. {\textcopyright} 2012 IEEE.},
author = {Ahn, Sunghwan and Yoon, Sukjune and Hyung, Seungyong and Kwak, Nosan and Roh, Kyung Shik},
doi = {10.1109/IROS.2012.6385743},
file = {::},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4006--4012},
title = {{On-board odometry estimation for 3D vision-based SLAM of humanoid robot}},
year = {2012}
}
@article{Mur-Artal2016a,
abstract = {In recent years there have been excellent results in visual-inertial odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However, these approaches lack the capability to close loops and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this letter, we present a novel tightly coupled visual-inertial simultaneous localization and mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of $1\%$ and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation.},
archivePrefix = {arXiv},
arxivId = {1610.05949},
author = {Mur-Artal, Raul and Tardos, Juan D.},
doi = {10.1109/LRA.2017.2653359},
eprint = {1610.05949},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,Sensor fusion,visual-based navigation},
month = {apr},
number = {2},
pages = {796--803},
title = {{Visual-Inertial Monocular SLAM with Map Reuse}},
url = {http://arxiv.org/abs/1610.05949 http://ieeexplore.ieee.org/document/7817784/},
volume = {2},
year = {2017}
}
@article{Heng2013,
abstract = {Multiple cameras are increasingly prevalent on robotic and human-driven vehicles. These cameras come in a variety of wide-angle, fish-eye, and catadioptric models. Furthermore, wheel odometry is generally available on the vehicles on which the cameras are mounted. For robustness, vision applications tend to use wheel odometry as a strong prior for camera pose estimation, and in these cases, an accurate extrinsic calibration is required in addition to an accurate intrinsic calibration. To date, there is no known work on automatic intrinsic calibration of generic cameras, and more importantly, automatic extrinsic calibration of a rig with multiple generic cameras and odometry. We propose an easy-to-use automated pipeline that handles both intrinsic and extrinsic calibration; we do not assume that there are overlapping fields of view. At the begining, we run an intrinsic calibration for each generic camera. The intrinsic calibration is automatic and requires a chessboard. Subsequently, we run an extrinsic calibration which finds all camera-odometry transforms. The extrinsic calibration is unsupervised, uses natural features, and only requires the vehicle to be driven around for a short time. The intrinsic parameters are optimized in a final bundle adjustment step in the extrinsic calibration. In addition, the pipeline produces a globally-consistent sparse map of landmarks which can be used for visual localization. The pipeline is publicly available as a standalone C++ package. {\textcopyright} 2013 IEEE.},
author = {Heng, Lionel and Li, Bo and Pollefeys, Marc},
doi = {10.1109/IROS.2013.6696592},
file = {:home/matias/Documents/Mendeley Desktop/Heng, Li, Pollefeys/Unknown/Heng, Li, Pollefeys - Unknown - CamOdoCal Automatic Intrinsic and Extrinsic Calibration of a Rig with Multiple Generic Cameras and Odom.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1793--1800},
title = {{CamOdoCal: Automatic intrinsic and extrinsic calibration of a rig with multiple generic cameras and odometry}},
year = {2013}
}
@article{Ranganathan2011,
abstract = {We present a novel algorithm for topological mapping, which is the problem of finding the graph structure of an environment from a sequence of measurements. Our algorithm, called Online Probabilistic Topological Mapping (OPTM), systematically addresses the problem by constructing the posterior on the space of all possible topologies given measurements. With each successive measurement, the posterior is updated incrementally using a Rao-Blackwellized particle filter. We present efficient sampling mechanisms using data-driven proposals and prior distributions on topologies that further enable OPTM's operation in an online manner. OPTM can incorporate various sensors seamlessly, as is demonstrated by our use of appearance, laser, and odometry measurements. OPTM is the first topological mapping algorithm that is theoretically accurate, systematic, sensor independent, and online, and thus advances the state of the art significantly. We evaluate the algorithm on a robot in diverse environments. {\textcopyright} 2011 The Author(s).},
author = {Ranganathan, Ananth and Dellaert, Frank},
doi = {10.1177/0278364910393287},
file = {:home/matias/Documents/Mendeley Desktop/Ranganathan, Dellaert/2011/Ranganathan, Dellaert - 2011 - Online probabilistic topological mapping.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Appearance modelling,Bayesian estimation,Particle filtering,Rao-Blackwellization,Robot mapping,Topological maps},
number = {6},
pages = {755--771},
title = {{Online probabilistic topological mapping}},
volume = {30},
year = {2011}
}
@article{Hartley2018b,
abstract = {State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking.},
archivePrefix = {arXiv},
arxivId = {1712.05873},
author = {Hartley, Ross and Mangelson, Josh and Gan, Lu and Jadidi, Maani Ghaffari and Walls, Jeffrey M. and Eustice, Ryan M. and Grizzle, Jessy W.},
doi = {10.1109/ICRA.2018.8460748},
eprint = {1712.05873},
file = {:home/matias/Documents/Mendeley Desktop/Hartley et al/2018/Hartley et al. - 2018 - Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4422--4429},
publisher = {IEEE},
title = {{Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors}},
year = {2018}
}
@phdthesis{Sunderhauf2012,
abstract = {SLAM (Simultaneous Localization And Mapping) has been a very active and almost ubiquitous problem in the field of mobile and autonomous robotics for over two decades. For many years, filter-based methods have dominated the SLAM literature, but a change of paradigms could be observed recently. Current state of the art solutions of the SLAM problem are based on efficient sparse least squares optimization techniques. However, it is commonly known that least squares methods are by default not robust against outliers. In SLAM, such outliers arise mostly from data association errors like false positive loop closures. Since the optimizers in current SLAM systems are not robust against outliers, they have to rely heavily on certain preprocessing steps to prevent or reject all data association errors. Especially false positive loop closures will lead to catastrophically wrong solutions with current solvers. The problem is commonly accepted in the literature, but no concise solution has been proposed so far. The main focus of this work is to develop a novel formulation of the optimization-based SLAM problem that is robust against such outliers. The developed approach allows the back-end part of the SLAM system to change parts of the topological structure of the problem's factor graph representation during the optimization process. The back-end can thereby discard individual constraints and converge towards correct solutions even in the presence of many false positive loop closures. This largely increases the overall robustness of the SLAM system and closes a gap between the sensor-driven front-end and the back-end optimizers. The approach is evaluated on both large scale synthetic and real-world datasets. This work furthermore shows that the developed approach is versatile and can be applied beyond SLAM, in other domains where least squares optimization problems are solved and outliers have to be expected. This is successfully demonstrated in the domain of GPS-based vehicle localization in urban areas where multipath satellite observations often impede high-precision position estimates. 3},
author = {S{\"{u}}nderhauf, Niko},
keywords = {Appearance-Based Place Recognition,Factor Graph,GNSS-based Localization,Multipath Mitigation,Nonlinear Least Squares Optimization,Outlier Rejection,Pose Graph SLAM,Robust Optimization,Simultaneous Localization and Mapping},
number = {April 1981},
pages = {1--231},
school = {Technischen Universit{\"{a}}t Chemnitz},
title = {{Robust Optimization for Simultaneous Localization and Mapping}},
year = {2012}
}
@article{Krombach2016,
abstract = {Visual motion estimation is challenging, due to high data rates, fast camera motions, featureless or repetitive environments, uneven lighting, and many other issues. In this work, we propose a two-layer approach for visual odometry with stereo cameras, which runs in real-time and combines feature-based matching with semi-dense direct image alignment. Our method initializes semi-dense depth estimation, which is computationally expensive, from motion that is tracked by a fast but robust feature point-based method. By that, we are not only able to efficiently estimate the pose of the camera with a high frame rate, but also to reconstruct the 3D structure of the environment at image gradients, which is useful, e.g., for mapping and obstacle avoidance. Experiments on datasets captured by a micro aerial vehicle (MAV) show that our approach is faster than state-of-the-art methods without losing accuracy. Moreover, our combined approach achieves promising results on the KITTI dataset, which is very challenging for direct methods, because of the low frame rate in conjunction with fast motion.},
author = {Krombach, Nicola and Droeschel, David and Behnke, Sven},
doi = {10.1007/978-3-319-48036-7_62},
file = {::},
isbn = {9783319480350},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {3D-reconstruction,MAVs,Semi-dense,Visual Odometry (VO),Visual SLAM},
number = {July},
pages = {855--868},
title = {{Combining feature-based and direct methods for semi-dense real-time stereo visual odometry}},
url = {http://www.ais.uni-bonn.de/papers/IAS-14_2016_Krombach.pdf},
volume = {531},
year = {2017}
}
@incollection{Todorov2006,
abstract = {The methodology of optimal control theory is used to highlight some of the current policy issues raised in the UK by the central government's allocation of grants to local authorities. It is shown that there is a link between the number of policy variables and targets and the freedom of action of individual local authorities. One consequence of current policy is shown to be that it makes optimal control theory an even more relevant tool for the analysis of the rate support grant system. Another is that the demise of local control appears to be the logical outcome in the long run.-Author},
author = {Livesey, D. A.},
booktitle = {Environment & Planning C: Government & Policy},
chapter = {Optimal Co},
doi = {10.1068/c040121},
file = {::},
isbn = {0792386086},
issn = {0263-774X},
number = {2},
pages = {121--129},
pmid = {20170501},
title = {{Optimal control theory and grants in aid.}},
url = {http://books.google.com/books?hl=en&lr=&id=bsQMWXXHzrYC&oi=fnd&pg=PA269&dq=Optimal+Control+Theory&ots=LFoRhTASwg&sig=QF2QBA3vXYMoZ32cFL5H3yFEie4 http://epc.sagepub.com/lookup/doi/10.1068/c040121},
volume = {4},
year = {1986}
}
@article{Huang2018,
abstract = {Recent years have witnessed a significant increase in the number of paper submissions to computer vision conferences. The sheer volume of paper submissions and the insufficient number of competent reviewers cause a considerable burden for the current peer review system. In this paper, we learn a classifier to predict whether a paper should be accepted or rejected based solely on the visual appearance of the paper (i.e., the gestalt of a paper). Experimental results show that our classifier can safely reject 50% of the bad papers while wrongly reject only 0.4% of the good papers, and thus dramatically reduce the workload of the reviewers. We also provide tools for providing suggestions to authors so that they can improve the gestalt of their papers.},
archivePrefix = {arXiv},
arxivId = {1812.08775},
author = {Huang, Jia-Bin},
eprint = {1812.08775},
file = {:home/matias/Documents/Mendeley Desktop/Huang/2018/Huang - 2018 - Deep Paper Gestalt.pdf:pdf},
pages = {1--7},
title = {{Deep Paper Gestalt}},
url = {http://arxiv.org/abs/1812.08775},
year = {2018}
}
@article{Bell1993,
abstract = {We show that the iterated Kalman filter (IKF) update is an application of the Gauss—Newton method for approximating a maximum likelihood estimate. We also present an example in which the iterated Kalman filter update and maximum likelihood estimate show correct convergence behavior as the observation becomes more accurate, whereas the extended Kalman filter update does not. {\textcopyright} 1993, IEEE. All rights reserved.},
author = {Cathey, Frederick W.},
doi = {10.1109/9.250476},
isbn = {0018-9286},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {2},
pages = {294--297},
pmid = {250476},
title = {{The Iterated Kalman Filter Update as a Gauss—Newton Method}},
url = {http://ieeexplore.ieee.org/document/250476/},
volume = {38},
year = {1993}
}
@article{Kofinas2014,
abstract = {The design of complex dynamic motions for humanoid robots is achievable only through the use of robot kinematics. In this paper, we study the problems of forward and inverse kinematics for the Aldebaran NAO humanoid robot and present a complete, exact, analytical solution to both problems, including a software library implementation for real-time on-board execution. The forward kinematics allow NAO developers to map any configuration of the robot from its own joint space to the three-dimensional physical space, whereas the inverse kinematics provide closed-form solutions to finding joint configurations that drive the end effectors of the robot to desired target positions in the three-dimensional physical space. The proposed solution was made feasible through a decomposition into five independent problems (head, two arms, two legs), the use of the Denavit-Hartenberg method, the analytical solution of a non-linear system of equations, and the exploitation of body and joint symmetries. The main advantage of the proposed inverse kinematics solution compared to existing approaches is its accuracy, its efficiency, and the elimination of singularities. In addition, we suggest a generic guideline for solving the inverse kinematics problem for other humanoid robots. The implemented, freely-available, NAO kinematics library, which additionally offers center-of-mass calculations and Jacobian inverse kinematics, is demonstrated in three motion design tasks: basic center-of-mass balancing, pointing to a moving ball, and human-guided balancing on two legs.},
author = {Kofinas, Nikolaos and Orfanoudakis, Emmanouil and Lagoudakis, Michail G.},
doi = {10.1007/s10846-013-0015-4},
file = {::},
isbn = {0921-0296},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Aldebaran NAO robot,Humanoid robots,Robot kinematics},
number = {2},
pages = {251--264},
title = {{Complete Analytical Forward and Inverse Kinematics for the NAO Humanoid Robot}},
volume = {77},
year = {2014}
}
@inproceedings{Burri2015,
abstract = {In this work, we present a model-based estimation scheme for multi-rotor Micro Aerial Vehicles (MAVs). Although modeling approaches for MAVs have been presented in the past, these models have rarely been used for real-time state estimation onboard MAVs. Building on this work, we identify the most dominant effects and propose an easy-to-use calibration scheme for estimation of the model parameters. Given the calibration estimates for these parameters, we derive a state estimator where the state prediction of the indirect Extended Kalman Filter (EKF) is driven by a MAV model. Solely using measurements from the Inertial Measurement Unit (IMU) and a barometric pressure sensor - both available on almost every MAV - our model-based formulation keeps the estimated velocity of the MAV bounded in all directions, as opposed to state of the art IMU-model driven state estimators onboard MAVs. This is crucial for keeping MAVs airborne safely, for instance in the case of failures or re-initialization of vision based localization systems.},
author = {Burri, Michael and D{\"{a}}twiler, Manuel and Achtelik, Markus W. and Siegwart, Roland},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2015.7139935},
isbn = {978-1-4799-6923-4},
issn = {10504729},
month = {may},
number = {June},
pages = {5278--5283},
publisher = {IEEE},
title = {{Robust state estimation for Micro Aerial Vehicles based on system dynamics}},
url = {http://ieeexplore.ieee.org/document/7139935/},
volume = {2015-June},
year = {2015}
}
@article{Rea2013,
abstract = {Fast reaction to sudden and potentially interesting stimuli is a crucial feature for safe and reliable interaction with the environment. Here we present a biologically inspired attention system developed for the humanoid robot iCub. It is based on input from unconventional event-driven vision sensors and an efficient computational method. The resulting system shows low-latency and fast determination of the location of the focus of attention. The performance is benchmarked against an instance of the state of the art in robotics artificial attention system used in robotics. Results show that the proposed system is two orders of magnitude faster that the benchmark in selecting a new stimulus to attend. {\textcopyright} 2013 Rea, Metta and Bartolozzi.},
author = {Rea, Francesco and Metta, Giorgio and Bartolozzi, Chiara},
doi = {10.3389/fnins.2013.00234},
isbn = {1662-453X},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Event-driven,Humanoid robotics,Neuromorphic,Saliency map,Visual attention},
number = {7 DEC},
pages = {1--11},
pmid = {24379753},
title = {{Event-driven visual attention for the humanoid robot iCub}},
volume = {7},
year = {2013}
}
@article{Handa2016,
abstract = {We introduce gvnn, a neural network library in Torch aimed towards bridging the gap between classic geometric computer vision and deep learning. Inspired by the recent success of Spatial Transformer Networks, we propose several new layers which are often used as parametric transformations on the data in geometric computer vision. These layers can be inserted within a neural network much in the spirit of the original spatial transformers and allow backpropagation to enable end-to-end learning of a network involving any domain knowledge in geometric computer vision. This opens up applications in learning invariance to 3D geometric transformation for place recognition, end-to-end visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error.},
archivePrefix = {arXiv},
arxivId = {1607.07405},
author = {Handa, Ankur and Bloesch, Michael and Pătrăucean, Viorica and Stent, Simon and McCormac, John and Davison, Andrew},
doi = {10.1007/978-3-319-49409-8_9},
eprint = {1607.07405},
file = {::},
isbn = {9783319494081},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Geometric vision,Spatial transformer networks,Unsupervised learning},
pages = {67--82},
title = {{Gvnn: Neural network library for geometric computer vision}},
volume = {9915 LNCS},
year = {2016}
}
@misc{Furgale2014,
author = {Furgale, Paul},
file = {:home/matias/Documents/Mendeley Desktop/Furgale/2014/Furgale - 2014 - Representing Robot Pose the Good, the Bad, and the Ugly.pdf:pdf},
title = {{Representing Robot Pose: the Good, the Bad, and the Ugly}},
url = {http://paulfurgale.info/s/Workshop-Rotations_v102key.pdf},
year = {2014}
}
@inproceedings{Dequaire2016,
abstract = {This paper proposes an appearance-based approach to estimating localisation performance in the context of visual teach and repeat. Specifically, it aims to estimate the likely corridor around a taught trajectory within which a vision-based localisation system is still able to localise itself. In contrast to prior art, our system is able to predict this localisation envelope for trajectories in similar, yet geographically distant locations where no repeat runs have yet been performed. Thus, by characterising the localisation performance in one region, we are able to predict performance in another. To achieve this, we leverage a Gaussian Process regressor to estimate the likely number of feature matches for any keyframe in the teach run, based on a combination of trajectory properties such as curvature and an appearance model of the keyframe. Using data from real traversals, we demonstrate that our approach performs as well as prior art when it comes to interpolating localisation performance based on a number of repeat runs, while also performing well at generalising performance estimation to freshly taught trajectories.},
author = {Dequaire, Julie and Tong, Chi Hay and Churchill, Winston and Posner, Ingmar},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487209},
file = {:home/matias/Documents/Mendeley Desktop/Dequaire et al/2016/Dequaire et al. - 2016 - Off the beaten track Predicting localisation performance in visual teach and repeat.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {795--800},
title = {{Off the beaten track: Predicting localisation performance in visual teach and repeat}},
volume = {2016-June},
year = {2016}
}
@article{MacKay1996,
abstract = {Lower perinatal and neonatal mortality have been achieved in the developed countries following advancement of neonatal care, introduction of high technologies, and better knowledge of pathophysiology of the newborn infants. Other contributing factors are organised delivery room care with skillful resuscitative techniques as well as risk identification and efficient transport of the sick infants including in utero transfer of the fetus, etc. It cannot be assumed that similar results can be attained in developing countries where financial and human resources are the problems. With limited resources, it is necessary to prioritize neonatal care in the developing countries. It is essential to collect minimum meaningful perinatal data to define the problems of each individual country. This is crucial for monitoring, auditing, evaluation, and planning of perinatal health care of the country. The definition and terminology in perinatology should also be uniform and standardised for comparative studies. Paediatricians should be well trained in resuscitation and stabilisation of the newborn infants. Resuscitation should begin in the delivery room and a resuscitation team should be formed. This is the best way to curtail complication and morbidity of asphyxiated births. Nosocomial infections have been the leading cause of neonatal deaths. It is of paramount importance to prevent infections in the nursery. Staff working in the nursery should pay attention to usage of sterilised equipment, isolation of infected babies and aseptic procedures. Paediatricians should avoid indiscriminate use of antibiotics. Most important of all, hand-washing before examination of the baby is mandatory and should be strictly adhered to. Other simpler measures include warming devices for maintenance of body temperature of the newborn babies, blood glucose monitoring, and antenatal steroid for mothers in premature labour. In countries where neonatal jaundice is prevalent, effective management to prevent kernicterus is essential. Simple assisted ventilatory device such as nasal continuous positive airway pressure (nCPAP) is also useful.},
author = {MacKay, David J. C.},
doi = {10.1007/978-94-015-8729-7},
file = {:home/matias/Documents/Mendeley Desktop/MacKay/1996/MacKay - 1996 - Maximum Entropy and Bayesian Methods.pdf:pdf},
isbn = {0792328515},
issn = {9048144078},
journal = {Maximum Entropy and Bayesian Methods},
pages = {43--59},
pmid = {8993147},
title = {{Maximum Entropy and Bayesian Methods}},
url = {http://link.springer.com/chapter/10.1007/978-94-015-8729-7_2%5Cnpapers3://publication/uuid/4C73D436-E70B-49D8-AA7E-031803718650},
year = {1996}
}
@article{Anderson2013,
abstract = {This paper introduces an optimised method for extracting natural landmarks to improve localisation during RoboCup soccer matches. The method uses modified 1D SURF features extracted from pixels on the robot's horizon. Consistent with the original SURF algorithm, the extracted features are robust to lighting changes, scale changes, and small changes in viewing angle or to the scene itself. Furthermore, we show that on a typical laptop 1D SURF runs more than one thousand times faster than SURF, achieving sub-millisecond performance. This makes the method suitable for visual navigation of resource constrained mobile robots. We demonstrate that by using just two stored images, it is possible to largely resolve the RoboCup SPL field end ambiguity. {\textcopyright} 2013 Springer-Verlag.},
author = {Anderson, Peter and Yusmanthia, Yongki and Hengst, Bernhard and Sowmya, Arcot},
doi = {10.1007/978-3-642-39250-4_12},
isbn = {9783642392498},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {118--129},
title = {{Robot localisation using natural landmarks}},
volume = {7500 LNAI},
year = {2013}
}
@article{Gallego2017,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high-speed motions or in scenes characterized by high dynamic range. These features, along with a very low power consumption, make event cameras an ideal complement to standard cameras for VR/AR and video game applications. With these applications in mind, this paper tackles the problem of accurate, low-latency tracking of an event camera from an existing photometric depth map (i.e., intensity plus depth information) built via classic dense reconstruction pipelines. Our approach tracks the 6-DOF pose of the event camera upon the arrival of each event, thus virtually eliminating latency. We successfully evaluate the method in both indoor and outdoor scenes and show that - because of the technological advantages of the event camera - our pipeline works in scenes characterized by high-speed motion, which are still inaccessible to standard cameras.},
archivePrefix = {arXiv},
arxivId = {1607.03468},
author = {Gallego, Guillermo and Lund, Jon E.A. and Mueggler, Elias and Rebecq, Henri and Delbruck, Tobi and Scaramuzza, Davide},
doi = {10.1109/TPAMI.2017.2769655},
eprint = {1607.03468},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {AR/VR,Bayes filter,Event-based vision,asynchronous processing,conjugate priors,dynamic vision sensor,high speed,low latency,pose tracking},
number = {10},
pages = {2402--2412},
pmid = {29990121},
title = {{Event-Based, 6-DOF Camera Tracking from Photometric Depth Maps}},
volume = {40},
year = {2018}
}
@article{Yang2020,
abstract = {Semidefinite Programming (SDP) and Sums-of-Squ-ares (SOS) relaxations have led to certifiably optimal non-minimal solvers for several robotics and computer vision problems. However, most non-minimal solvers rely on least squares formulations, and, as a result, are brittle against outliers. While a standard approach to regain robustness against outliers is to use robust cost functions, the latter typically introduce other non-convexities, preventing the use of existing non-minimal solvers. In this letter, we enable the simultaneous use of non-minimal solvers and robust estimation by providing a general-purpose approach for robust global estimation, which can be applied to any problem where a non-minimal solver is available for the outlier-free case. To this end, we leverage the Black-Rangarajan duality between robust estimation and outlier processes (which has been traditionally applied to early vision problems), and show that graduated non-convexity (GNC) can be used in conjunction with non-minimal solvers to compute robust solutions, without requiring an initial guess. we demonstrate the resulting robust non-minimal solvers in applications, including point cloud and mesh registration, pose graph optimization, and image-based object pose estimation (also called shape alignment). Our solvers are robust to 70-80% of outliers, outperform RANSAC, are more accurate than specialized local solvers, and faster than specialized global solvers. We also propose the first certifiably optimal non-minimal solver for shape alignment using SOS relaxation.},
archivePrefix = {arXiv},
arxivId = {1909.08605},
author = {Yang, Heng and Antonante, Pasquale and Tzoumas, Vasileios and Carlone, Luca},
doi = {10.1109/LRA.2020.2965893},
eprint = {1909.08605},
file = {:home/matias/Documents/Mendeley Desktop/Yang et al/2020/Yang et al. - 2020 - Graduated Non-Convexity for Robust Spatial Perception From Non-Minimal Solvers to Global Outlier Rejection.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Graduated non-convexity,global optimization,outlier rejection,robust estimation,spatial perception},
number = {2},
pages = {1127--1134},
title = {{Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection}},
volume = {5},
year = {2020}
}
@article{Nister2004a,
abstract = {We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.},
author = {Nist{\'{e}}r, David and Naroditsky, Oleg and Bergen, James},
doi = {10.1109/cvpr.2004.1315094},
file = {::},
isbn = {0769521584},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {C},
pages = {652--659},
title = {{Visual odometry}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1315094%5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315094},
volume = {1},
year = {2004}
}
@misc{Society2012,
abstract = {Ethernet local area network operation is specified for selected speeds of operation from 1 Mb/s to 100 Gb/s using a common media access control (MAC) specification and management information base (MIB). The Carrier Sense Multiple Access with Collision Detection (CSMA/CD) MAC protocol specifies shared medium (half duplex) operation, as well as full duplex operation. Speed specific Media Independent Interfaces (MIIs) allow use of selected Physical Layer devices (PHY) for operation over coaxial, twisted-pair or fiber optic cables. System considerations for multisegment shared access networks describe the use of Repeaters that are defined for operational speeds up to 1000 Mb/s. Local Area Network (LAN) operation is supported at all speeds. Other specified capabilities include various PHY types for access networks, PHYs suitable for metropolitan area network applications, and the provision of power over selected twisted-pair PHY types.},
author = {{IEEE Computer Society}},
booktitle = {IEEE Standard for Ethernet},
doi = {10.1109/IEEESTD.2012.6419735},
isbn = {973-07381-7312-2},
keywords = {10 Gigabit Ethernet,100 Gigabit Ethernet,1000BASE,100BASE,100GBASE,10BASE,10GBASE,40 Gigabit Ethernet,40GBASE,AUI,Auto Negotiation,Backplane Ethernet,DTE Power via the MDI,EPON,Ethernet,Ethernet in the First Mile,Ethernet passive optical network,Fast Ethernet,GMII,Gigabit Ethernet,IEEE 802.3,MDI,MIB,MII,PHY,PMA,Physical Layer,Power over Ethernet,VLAN TAG,XGMII,attachment unit interface,data processing,information exchange,local area network,management,media independent interface,medium dependent interface,physical coding sublayer,physical medium attachment,repeater,type field},
number = {December},
pages = {1--400},
title = {{IEEE Standard for Ethernet - Section Two}},
volume = {2012},
year = {2012}
}
@article{Hamrick2020,
abstract = {Machine learning is currently involved in some of the most vigorous debates it has ever seen. Such debates often seem to go around in circles, reaching no conclusion or resolution. This is perhaps unsurprising given that researchers in machine learning come to these discussions with very different frames of reference, making it challenging for them to align perspectives and find common ground. As a remedy for this dilemma, we advocate for the adoption of a common conceptual framework which can be used to understand, analyze, and discuss research. We present one such framework which is popular in cognitive science and neuroscience and which we believe has great utility in machine learning as well: Marr's levels of analysis. Through a series of case studies, we demonstrate how the levels facilitate an understanding and dissection of several methods from machine learning. By adopting the levels of analysis in one's own work, we argue that researchers can be better equipped to engage in the debates necessary to drive forward progress in our field.},
archivePrefix = {arXiv},
arxivId = {2004.05107},
author = {Hamrick, Jessica and Mohamed, Shakir},
eprint = {2004.05107},
file = {:home/matias/Documents/Mendeley Desktop/Hamrick, Mohamed/2020/Hamrick, Mohamed - 2020 - Levels of Analysis for Machine Learning.pdf:pdf},
number = {Iclr},
pages = {1--6},
title = {{Levels of Analysis for Machine Learning}},
url = {http://arxiv.org/abs/2004.05107},
year = {2020}
}
@incollection{Tasse2013,
abstract = {This paper evaluates the benefits of modeling the dynamic environment of robot soccer games as a SLAM problem. Moving objects such as other robots and the ball are not only tracked individually, but modeled in a full state and used for localization at the same time. This is described as an implementation of an efficient system capable of running in real time on limited platforms such as the humanoid robot Nao, and the system's benefit is evaluated using real world experiments. {\textcopyright} 2013 Springer-Verlag.},
author = {Tasse, Stefan and Hofmann, Matthias and Urbann, Oliver},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39250-4_33},
isbn = {9783642392498},
issn = {03029743},
pages = {368--379},
title = {{SLAM in the dynamic context of robot soccer games}},
url = {http://link.springer.com/10.1007/978-3-642-39250-4_33},
volume = {7500 LNAI},
year = {2013}
}
@inproceedings{Krajnik2018,
abstract = {We present a novel concept for teach-and-repeat visual navigation. The proposed concept is based on a mathematical model, which indicates that in teach-and-repeat navigation scenarios, mobile robots do not need to perform explicit localisation. Rather than that, a mobile robot which repeats a previously taught path can simply 'replay' the learned velocities, while using its camera information only to correct its heading relative to the intended path. To support our claim, we establish a position error model of a robot, which traverses a taught path by only correcting its heading. Then, we outline a mathematical proof which shows that this position error does not diverge over time. Based on the insights from the model, we present a simple monocular teach-and-repeat navigation method. The method is computationally efficient, it does not require camera calibration, and it can learn and autonomously traverse arbitrarily-shaped paths. In a series of experiments, we demonstrate that the method can reliably guide mobile robots in realistic indoor and outdoor conditions, and can cope with imperfect odometry, landmark deficiency, illumination variations and naturally-occurring environment changes. Furthermore, we provide the navigation system and the datasets gathered at www.github.com/gestom/stroll-bearnav.},
archivePrefix = {arXiv},
arxivId = {1711.05348},
author = {Krajnik, Tomas and Majer, Filip and Halodova, Lucie and Vintr, Tomas},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2018.8593803},
eprint = {1711.05348},
file = {:home/matias/Documents/Mendeley Desktop/Krajnik et al/2018/Krajnik et al. - 2018 - Navigation without localisation Reliable teach and repeat based on the convergence theorem.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
pages = {1657--1664},
title = {{Navigation without localisation: Reliable teach and repeat based on the convergence theorem}},
year = {2018}
}
@article{Krotkov2017,
abstract = {The DARPA Robotics Challenge (DRC) program conducted a series of prize-based competition events to develop and demonstrate technology for disaster response. This article provides the official and definitive account of DRC Finals as the culmination of the DRC program. The article details the eight tasks (Drive, Egress, Door, Valve, Wall, Surprise [Plug and Switch], Rubble [Obstacle or Debris], and Stairs) constituting the Challenge, and describes how the competition encouraged supervised autonomous operation by intentionally degrading the communications channel between the remote human operators. The article presents the results of the DRC Finals and places those results in perspective by identifying both strengths and weaknesses of robot performance exhibited at the competition.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Krotkov, Eric and Hackett, Douglas and Jackel, Larry and Perschbacher, Michael and Pippine, James and Strauss, Jesse and Pratt, Gill and Orlowski, Christopher},
doi = {10.1002/rob.21683},
eprint = {10.1.1.91.5767},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {229--240},
pmid = {22164016},
title = {{The DARPA Robotics Challenge Finals: Results and Perspectives}},
url = {http://doi.wiley.com/10.1002/rob.21683},
volume = {34},
year = {2017}
}
@article{Zhang2020,
abstract = {Considering visual localization accuracy at the planning time gives preference to robot motion that can be better localized and thus has the potential of improving vision-based navigation, especially in visually degraded environments. To integrate the knowledge about localization accuracy in motion planning algorithms, a central task is to quantify the amount of information that an image taken at a 6 degree-of-freedom pose brings for localization, which is often represented by the Fisher information. However, computing the Fisher information from a set of sparse landmarks (i.e., a point cloud), which is the most common map for visual localization, is inefficient. This approach scales linearly with the number of landmarks in the environment and does not allow the reuse of the computed Fisher information. To overcome these drawbacks, we propose the first dedicated map representation for evaluating the Fisher information of 6 degree-of-freedom visual localization for perception-aware motion planning. By formulating the Fisher information and sensor visibility carefully, we are able to separate the rotational invariant component from the Fisher information and store it in a voxel grid, namely the Fisher information field. This step only needs to be performed once for a known environment. The Fisher information for arbitrary poses can then be computed from the field in constant time, eliminating the need of costly iterating all the 3D landmarks at the planning time. Experimental results show that the proposed Fisher information field can be applied to different motion planning algorithms and is at least one order-of-magnitude faster than using the point cloud directly. Moreover,the proposed map representation is differentiable, resulting in better performance than the point cloud when used in trajectory optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {2008.03324},
author = {Zhang, Zichao and Scaramuzza, Davide},
eprint = {2008.03324},
file = {:home/matias/Documents/Mendeley Desktop/Zhang, Scaramuzza/2020/Zhang, Scaramuzza - 2020 - Fisher Information Field an Efficient and Differentiable Map for Perception-aware Planning.pdf:pdf},
month = {aug},
pages = {1--18},
title = {{Fisher Information Field: an Efficient and Differentiable Map for Perception-aware Planning}},
url = {http://arxiv.org/abs/2008.03324},
year = {2020}
}
@article{Paul2010,
abstract = {This paper describes a probabilistic framework for appearance based navigation and mapping using spatial and visual appearance data. Like much recent work on appearance based navigation we adopt a bag-of-words approach in which positive or negative observations of visual words in a scene are used to discriminate between already visited and new places. In this paper we add an important extra dimension to the approach. We explicitly model the spatial distribution of visual words as a random graph in which nodes are visual words and edges are distributions over distances. Care is taken to ensure that the spatial model is able to capture the multi-modal distributions of inter-word spacing and account for sensor errors both in word detection and distances. Crucially, these inter-word distances are viewpoint invariant and collectively constitute strong place signatures and hence the impact of using both spatial and visual appearance is marked. We provide results illustrating a tremendous increase in precision-recall area compared to a state-of-the-art visual appearance only systems. {\textcopyright}2010 IEEE.},
author = {Paul, Rohan and Newman, Paul},
doi = {10.1109/ROBOT.2010.5509587},
file = {:home/matias/Documents/Mendeley Desktop/Paul, Newman/2010/Paul, Newman - 2010 - FAB-MAP 3D Topological mapping with spatial and visual appearance.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2649--2656},
title = {{FAB-MAP 3D: Topological mapping with spatial and visual appearance}},
year = {2010}
}
@article{Karumanchi2017,
abstract = {This paper discusses hardware and software improvements to the RoboSimian system leading up to and during the 2015 DARPA Robotics Challenge (DRC) Finals. Team RoboSimian achieved a 5th place finish by achieving 7 points in 47:59 min. We present an architecture that was structured to be adaptable at the lowest level and repeatable at the highest level. The low-level adaptability was achieved by leveraging tactile measurements from force torque sensors in the wrist coupled with whole-body motion primitives. We use the term “behaviors” to conceptualize this low-level adaptability. Each behavior is a contact-triggered state machine that enables execution of short-order manipulation and mobility tasks autonomously. At a high level, we focused on a teach-and-repeat style of development by storing executed behaviors and navigation poses in an object/task frame for recall later. This enabled us to perform tasks with high repeatability on competition day while being robust to task differences from practice to execution.},
author = {Karumanchi, Sisir and Edelberg, Kyle and Baldwin, Ian and Nash, Jeremy and Reid, Jason and Bergh, Charles and Leichty, John and Carpenter, Kalind and Shekels, Matthew and Gildner, Matthew and Newill-Smith, David and Carlton, Jason and Koehler, John and Dobreva, Tatyana and Frost, Matthew and Hebert, Paul and Borders, James and Ma, Jeremy and Douillard, Bertrand and Backes, Paul and Kennedy, Brett and Satzinger, Brian and Lau, Chelsea and Byl, Katie and Shankar, Krishna and Burdick, Joel},
doi = {10.1002/rob.21676},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {305--332},
title = {{Team RoboSimian: Semi-autonomous Mobile Manipulation at the 2015 DARPA Robotics Challenge Finals}},
volume = {34},
year = {2017}
}
@article{Valls2018,
abstract = {This paper introduces jl{\"{u}}ela driverless: the first autonomous racecar to win a Formula Student Driverless competition. In this competition, among other challenges, an autonomous racecar is tasked to complete 10 laps of a previously unknown racetrack as fast as possible and using only onboard sensing and computing. The key components of fl{\"{u}}ela's design are its modular redundant sub-systems that allow robust performance despite challenging perceptual conditions or partial system failures. The paper presents the integration of key components of our autonomous racecar, i.e., system design, EKF-based state estimation, LiDAR-based perception, and particle filter-based SLAM. We perform an extensive experimental evaluation on real-world data, demonstrating the system's effectiveness by outperforming the next-best ranking team by almost half the time required to finish a lap. The autonomous racecar reaches lateral and longitudinal accelerations comparable to those achieved by experienced human drivers.},
archivePrefix = {arXiv},
arxivId = {1804.03252},
author = {Valls, Miguel I. and Hendrikx, Hubertus F.C. and Reijgwart, Victor J.F. and Meier, Fabio V. and Sa, Inkyu and Dube, Renaud and Gawel, Abel and Burki, Mathias and Siegwart, Roland},
doi = {10.1109/ICRA.2018.8462829},
eprint = {1804.03252},
file = {::},
isbn = {9781538630815},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2048--2055},
title = {{Design of an Autonomous Racecar: Perception, State Estimation and System Integration}},
url = {http://arxiv.org/abs/1804.03252},
year = {2018}
}
@article{Kaiser2016,
abstract = {State of the art approaches for visual-inertial sensor fusion use filter-based or optimization-based algorithms. Due to the nonlinearity of the system, a poor initialization can have a dramatic impact on the performance of these estimation methods. Recently, a closed-form solution providing such an initialization was derived in [1]. That solution determines the velocity (angular and linear) of a monocular camera in metric units by only using inertial measurements and image features acquired in a short time interval. In this letter, we study the impact of noisy sensors on the performance of this closed-form solution. We show that the gyroscope bias, not accounted for in [1], significantly affects the performance of the method. Therefore, we introduce a new method to automatically estimate this bias. Compared to the original method, the new approach now models the gyroscope bias and is robust to it. The performance of the proposed approach is successfully demonstrated on real data from a quadrotor MAV.},
author = {Kaiser, Jacques and Martinelli, Agostino and Fontana, Flavio and Scaramuzza, Davide},
doi = {10.1109/LRA.2016.2521413},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Localization,Sensor Fusion,Visual-Based Navigation},
number = {1},
pages = {18--25},
title = {{Simultaneous State Initialization and Gyroscope Bias Calibration in Visual Inertial Aided Navigation}},
volume = {2},
year = {2017}
}
@article{Fehr2018,
abstract = {Industrial facilities often require periodic visual inspections of key installations. Examining these points of interest is time consuming, potentially hazardous or require special equipment to reach. MAVs are ideal platforms to automate this expensive and tedious task. In this work we present a novel system that enables a human operator to teach a visual inspection task to an autonomous aerial vehicle by simply demonstrating the task using a handheld device. To enable robust operation in confined, GPS-denied environments, the system employs the Google Tango visual-inertial mapping framework as the only source of pose estimates. In a first step the operator records the desired inspection path and defines the inspection points. The mapping framework then computes a feature-based localization map, which is shared with the robot. After take-off, the robot estimates its pose based on this map and plans a smooth trajectory through the way points defined by the operator. Furthermore, the system is able to track the poses of other robots or the operator, localized in the same map, and follow them in real-time while keeping a safe distance.},
archivePrefix = {arXiv},
arxivId = {1803.09650},
author = {Fehr, Marius and Schneider, Thomas and Dymczyk, Marcin and Sturm, J{\"{u}}rgen and Siegwart, Roland},
eprint = {1803.09650},
file = {:home/matias/Documents/Mendeley Desktop/Fehr et al/2018/Fehr et al. - 2018 - Visual-Inertial Teach and Repeat for Aerial Inspection.pdf:pdf},
month = {mar},
pages = {1--3},
title = {{Visual-Inertial Teach and Repeat for Aerial Inspection}},
url = {http://arxiv.org/abs/1803.09650},
year = {2018}
}
@phdthesis{Torres-Torriti2003,
author = {Of, Design and Feedback, Stabilizing and For, Controls and Systems, Strongly Nonlinear},
number = {March},
school = {McGill University},
title = {{DESIGN OF STABILIZING FEEDBACK CONTROLS FOR Miguel Torres-Torriti}},
url = {http://www.cim.mcgill.ca/$\sim$migueltt/eeeng/publications/prop.pdf},
year = {2003}
}
@article{Ondruska2015,
abstract = {This paper explores the idea of reducing a robot's energy consumption while following a trajectory by turning off the main localisation subsystem and switching to a lower-powered, less accurate odometry source at appropriate times. This applies to scenarios where the robot is permitted to deviate from the original trajectory, which allows for energy savings. Sensor scheduling is formulated as a probabilistic belief planning problem. Two algorithms are presented which generate feasible perception schedules: the first is based upon a simple heuristic; the second leverages dynamic programming to obtain optimal plans. Both simulations and real-world experiments on a planetary rover prototype demonstrate over 50% savings in perception-related energy, which translates into a 12% reduction in total energy consumption.},
author = {Ondr{\'{u}}{\v{s}}ka, Peter and Gurəu, Corina and Marchegiani, Letizia and Tong, Chi Hay and Posner, Ingmar},
doi = {10.1109/ICRA.2015.7139866},
file = {:home/matias/Documents/Mendeley Desktop/Ondr{\'{u}}{\v{s}}ka et al/2015/Ondr{\'{u}}{\v{s}}ka et al. - 2015 - Scheduled perception for energy-efficient path following.pdf:pdf},
isbn = {9781479969234},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {4799--4806},
title = {{Scheduled perception for energy-efficient path following}},
volume = {2015-June},
year = {2015}
}
@article{Campusanoa,
author = {Campusano, Miguel and Fabry, Johan},
keywords = {live programming,nested state machines,robots},
number = {Dcc},
title = {{An Interpreter For Live Robot Programming}}
}
@book{Mason2001,
abstract = {"Manipulation" refers to a variety of physical changes made to the world around us. Mechanics of Robotic Manipulation addresses one form of robotic manipulation, moving objects, and the various processes involvedgrasping, carrying, pushing, dropping, throwing, and so on. Unlike most books on the subject, it focuses on manipulation rather than manipulators. This attention to processes rather than devices allows a more fundamental approach, leading to results that apply to a broad range of devices, not just robotic arms.The book draws both on classical mechanics and on classical planning, which introduces the element of imperfect information. The book does not propose a specific solution to the problem of manipulation, but rather outlines a path of inquiry.},
author = {Mason, Matthew T.},
booktitle = {Mechanics of Robotic Manipulation},
doi = {10.7551/mitpress/4527.001.0001},
file = {::},
isbn = {9780262133968},
publisher = {MIT Press},
title = {{Mechanics of Robotic Manipulation}},
year = {2019}
}
@inproceedings{Hutter2016,
abstract = {This paper introduces ANYmal, a quadrupedal robot that features outstanding mobility and dynamic motion capability. Thanks to novel, compliant joint modules with integrated electronics, the 30 kg, 0.5m tall robotic dog is torque controllable and very robust against impulsive loads during running or jumping. The presented machine was designed with a focus on outdoor suitability, simple maintenance, and user-friendly handling to enable future operation in real world scenarios. Performance tests with the joint actuators indicated a torque control bandwidth of more than 70 Hz, high disturbance rejection capability, as well as impact robustness when moving with maximal velocity. It is demonstrated in a series of experiments that ANYmal can execute walking gaits, dynamically trot at moderate speed, and is able to perform special maneuvers to stand up or crawl very steep stairs. Detailed measurements unveil that even full-speed running requires less than 280W, resulting in an autonomy of more than 2 h.},
author = {Hutter, Marco and Gehring, Christian and Jud, Dominic and Lauber, Andreas and Bellicoso, C. Dario and Tsounis, Vassilios and Hwangbo, Jemin and Bodie, Karen and Fankhauser, Peter and Bloesch, Michael and Diethelm, Remo and Bachmann, Samuel and Melzer, Amir and Hoepflinger, Mark},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7758092},
file = {::},
isbn = {9781509037629},
issn = {21530866},
month = {oct},
pages = {38--44},
publisher = {IEEE},
title = {{ANYmal - A highly mobile and dynamic quadrupedal robot}},
url = {http://ieeexplore.ieee.org/document/7758092/},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{Bloesch2012,
abstract = {This paper introduces a state estimation framework for legged robots that allows estimating the full pose of the robot without making any assumptions about the geometrical structure of its environment. This is achieved by means of an Observability Constrained Extended Kalman Filter that fuses kinematic encoder data with on-board IMU measurements. By including the absolute position of all footholds into the filter state, simple model equations can be formulated which accurately capture the uncertainties associated with the intermittent ground contacts. The resulting filter simultaneously estimates the position of all footholds and the pose of the main body. In the algorithmic formulation, special attention is paid to the consistency of the linearized filter: it maintains the same observability properties as the nonlinear system, which is a prerequisite for accurate state estimation. The presented approach is implemented in simulation and validated experimentally on an actual quadrupedal robot.},
author = {Bloesch, Michael and Hutter, Marco and Hoepflinger, Mark A. and Leutenegger, Stefan and Gehring, Christian and Remy, C. David and Siegwart, Roland},
booktitle = {Robotics: Science and Systems},
doi = {10.7551/mitpress/9816.003.0008},
isbn = {9780262519687},
issn = {2330765X},
month = {jul},
pages = {17--24},
publisher = {Robotics: Science and Systems Foundation},
title = {{State estimation for legged robots: Consistent fusion of leg kinematics and IMU}},
url = {http://www.roboticsproceedings.org/rss08/p03.pdf},
volume = {8},
year = {2013}
}
@inproceedings{Kolter2009,
abstract = {Legged robots offer the potential to navigate highly challenging terrain, and there has recently been much progress in this area. However, a great deal of this recent work has operated under the assumption that either the robot has complete knowledge of its environment or that its environment is suitably regular so as to be navigated with only minimal perception, an unrealistic assumption in many real-world domains. In this paper we present an integrated perception and control system for a quadruped robot that allows it to perceive and traverse previously unseen, rugged terrain that includes large, irregular obstacles. A key element of the system is a novel terrain modeling algorithm, used for filling in the occluded models resulting from on-board vision systems. We apply our approach to the LittleDog robot, and show that it allows the robot to walk over challenging terrain using only on-board perception.},
author = {Kolter, J. Zico and {Youngjun Kim} and Ng, Andrew Y.},
booktitle = {2009 IEEE International Conference on Robotics and Automation},
doi = {10.1109/robot.2009.5152795},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
month = {may},
pages = {1557--1564},
publisher = {IEEE},
title = {{Stereo vision and terrain modeling for quadruped robots}},
url = {http://ieeexplore.ieee.org/document/5152795/},
year = {2009}
}
@article{Wirbel2013,
abstract = {In this paper, we present our work to try and implement a SLAM algorithm on a humanoid robot platform, the NAO robot produced by Aldebaran Robotics. We first start by testing a visual SLAM algorithm which uses keypoints as visual landmarks and tries to estimate their positions, and adapt it to the specific constraints of the platform: restricted CPU, monocular camera, low speed and drifting odometry. We conclude that running a full monocular visual SLAM on the robot is not yet available, but that some specific keypoints can be robustly tracked even while walking. Then we use them to derive the robot orientation and build a compass feature based on the robot camera, which can be used for example to ensure that the robot walks straight. Therefore we showed vision can be efficiently used to improve NAO's navigation. {\textcopyright} 2013 IEEE.},
author = {Wirbel, Emilie and Steux, Bruno and Bonnabel, Silvere and {De La Fortelle}, Arnaud},
doi = {10.1109/ICNSC.2013.6548820},
isbn = {9781467351980},
journal = {2013 10th IEEE International Conference on Networking, Sensing and Control, ICNSC 2013},
pages = {678--683},
title = {{Humanoid robot navigation: From a visual SLAM to a visual compass}},
year = {2013}
}
@article{Haynes2017,
abstract = {CHIMP, the CMU Highly Intelligent Mobile Platform, is a humanoid robot capable of executing complex tasks in dangerous, degraded, human-engineered environments, such as those found in disaster response scenarios. CHIMP is uniquely designed for mobile manipulation in challenging environments, as the robot performs manipulation tasks using an upright posture, yet it uses more stable prostrate postures for mobility through difficult terrain. In this paper, we report on the improvements made to CHIMP—both in its mechanical design and its software systems—in preparation for the DARPA Robotics Challenge Finals in June 2015. These include details on CHIMP's novel mechanical design, actuation systems, robust construction, all-terrain mobility, supervised autonomy approach, and unique user interfaces utilized for the challenge. Additionally, we provide an overview of CHIMP's performance, and we detail the various lessons learned over the course of the challenge. CHIMP was one of the winners of the DARPA Robotics Challenge, completing all tasks and finishing in 3rd place out of 23 teams. Notably, CHIMP was the only robot to stand back up after accidentally falling over, a testament to the robustness engineered into the robot and a remote operator's ability to execute complex tasks using a highly capable robot. We present CHIMP as a concrete engineering example of a successful disaster response robot.},
author = {Haynes, G. Clark and Stager, David and Stentz, Anthony and {Vande Weghe}, J. Michael and Zajac, Brian and Herman, Herman and Kelly, Alonzo and Meyhofer, Eric and Anderson, Dean and Bennington, Dane and Brindza, Jordan and Butterworth, David and Dellin, Chris and George, Michael and Gonzalez-Mora, Jose and Jones, Morgan and Kini, Prathamesh and Laverne, Michel and Letwin, Nick and Perko, Eric and Pinkston, Chris and Rice, David and Scheifflee, Justin and Strabala, Kyle and Waldbaum, Mark and Warner, Randy},
doi = {10.1002/rob.21696},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {2},
pages = {281--304},
title = {{Developing a Robust Disaster Response Robot: CHIMP and the Robotics Challenge}},
volume = {34},
year = {2017}
}
@article{Bhardwaj2019,
abstract = {Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.},
archivePrefix = {arXiv},
arxivId = {1907.09591},
author = {Bhardwaj, Mohak and Boots, Byron and Mukadam, Mustafa},
eprint = {1907.09591},
file = {:home/matias/Documents/Mendeley Desktop/Bhardwaj, Boots, Mukadam/2019/Bhardwaj, Boots, Mukadam - 2019 - Differentiable Gaussian Process Motion Planning.pdf:pdf},
keywords = {differentiable planning,learning to plan,motion planning},
title = {{Differentiable Gaussian Process Motion Planning}},
url = {http://arxiv.org/abs/1907.09591},
volume = {2},
year = {2019}
}
@article{Guizilini2019,
abstract = {The ability to generate accurate terrain models is of key importance in a wide variety of robotics tasks, ranging from path planning and trajectory optimization to environment exploration and mining applications. This paper introduces a novel regression methodology for terrain modeling that can approximate arbitrarily complex functions based on a series of simple kernel calculations, using variational Bayesian inference. A sparse feature vector is used to efficiently project input points into a high-dimensional reproducing kernel Hilbert space, according to a set of inducing points automatically generated from clustering available data. Each inducing point maintains its own regression model in addition to individual kernel parameters, and the entire set is iteratively optimized as more data are collected in order to maximize a global variational lower bound. We also show how kernel and regression model parameters can be jointly learned, to achieve a better approximation of the underlying function. Experimental results show that the proposed methodology consistently outperforms current state-of-the-art techniques, while producing a continuous model with a fully probabilistic treatment of uncertainties, well-defined gradients, and highly scalable to large-scale datasets. As a practical application of the proposed terrain modeling technique, we explore the problem of trajectory optimization, deriving gradients that allow the efficient generation of continuous paths using standard optimization algorithms, minimizing a series of useful properties (i.e. distance traveled, changes in elevation, and terrain variance).},
author = {Guizilini, Vitor and Ramos, Fabio},
doi = {10.1177/0278364919844586},
file = {:home/matias/Documents/Mendeley Desktop/Guizilini, Ramos/2019/Guizilini, Ramos - 2019 - Variational Hilbert regression for terrain modeling and trajectory optimization.pdf:pdf},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {Hilbert space,Terrain modeling,kernel methods,trajectory optimization,variational Bayes},
number = {12-13},
pages = {1375--1387},
title = {{Variational Hilbert regression for terrain modeling and trajectory optimization}},
volume = {38},
year = {2019}
}
@article{Gadd2018,
abstract = {This paper presents a mercantile framework for the decentralised sharing of navigation expertise amongst a fleet of robots which perform regular missions into a common but variable environment. We build on our earlier work and allow individual agents to intermittently initiate trades based on a real-time assessment of the nature of their missions or demand for localisation capability, and to choose trading partners with discrimination based on an internally evolving set of beliefs in the expected value of trading with each other member of the team. To this end, we suggest some obligatory properties that a formalisation of the distributed versioning of experience maps should exhibit, to ensure the eventual convergence in the state of each agent's map under a sequence of pairwise exchanges, as well as the uninterrupted integrity of the representation under versioning operations. To mitigate limitations in hardware and network resources, the "data market" is catalogued by distinct sections of the world, which the agents treat as "products" for appraisal and purchase. To this end, we demonstrate and evaluate our system using the publicly available Oxford RobotCar Dataset, the hand-labelled data market catalogue (approaching 446km of fully indexed sections-of-interest) for which we plan to release alongside the existing raw stereo imagery. We show that, by refining market policies over time, agents achieve improved localisation in a directed and accelerated manner.},
archivePrefix = {arXiv},
arxivId = {1801.05607},
author = {Gadd, Matthew and Newman, Paul},
eprint = {1801.05607},
file = {:home/matias/Documents/Mendeley Desktop/Gadd, Newman/2018/Gadd, Newman - 2018 - The Data Market Policies for Decentralised Visual Localisation.pdf:pdf},
number = {Cl},
title = {{The Data Market: Policies for Decentralised Visual Localisation}},
url = {http://arxiv.org/abs/1801.05607},
year = {2018}
}
@article{Estefo2016,
abstract = {The middleware for robotics ROS has become the de-facto standard for developing robot applications. Thanks to our experience using ROS we conjectured that the quality of code of ROS is low, yielding a poor user experience for ROS users and posing important barriers to robot software development. In this work we present a first quantification of code quality of the ROS ecosystem through an analysis of code duplication in launchfiles. Our experience led us to believe that these configuration files exhibit a significant amount of code duplication, and this study shows that it is indeed the case. We find that 25% of packages with multiple launchfiles have duplicated code, and that clones are highly similar.},
author = {Estefo, Pablo and Robbes, Romain and Fabry, Johan},
doi = {10.1109/SCCC.2015.7416575},
isbn = {9781467398176},
issn = {15224902},
journal = {Proceedings - International Conference of the Chilean Computer Science Society, SCCC},
number = {Dcc},
title = {{Code duplication in ROS launchfiles}},
volume = {2016-Febru},
year = {2016}
}
@phdthesis{Censi2013,
abstract = {Could a "brain in a jar" be able to control an unknown robotic body to which it is connected, and use it to achieve useful tasks, without any prior assumptions on the body's sensors and actuators? Other than of purely intellectual interest, this question is relevant to the medium-term challenges of robotics: as the complexity of robotics applications grows, automated learning techniques might reduce design effort and increase the robustness and reliability of the solutions. In this work, the problem of "bootstrapping" is studied in the context of the Vehicles universe, which is an idealization of simple mobile robots, after the work of Braitenberg. The first thread of results consists in analyzing such simple senso- rimotor cascades and proposing models of varying complexity that can be learned from data. The second thread regards how to properly formalize the notions of "absence of as- sumptions", as a particular form of invariance that the bootstrapping agent must satisfy, and proposes some invariance-based design techniques.},
author = {Censi, Andrea},
school = {California Institute of Technology},
title = {{Bootstrapping Vehicles: a Formal Approach to Unsupervised Sensorimotor Learning Based on Invariance}},
url = {http://resolver.caltech.edu/CaltechTHESIS:10282012-082208075},
year = {2012}
}
@article{Gallego2017a,
abstract = {We present an algorithm to estimate the rotational motion of an event camera. In contrast to traditional cameras, which produce images at a fixed rate, event cameras have independent pixels that respond asynchronously to brightness changes, with microsecond resolution. Our method leverages the type of information conveyed by these novel sensors (i.e., edges) to directly estimate the angular velocity of the camera, without requiring optical flow or image intensity estimation. The core of the method is a contrast maximization design. The method performs favorably against ground truth data and gyroscopic measurements from an Inertial Measurement Unit, even in the presence of very high-speed motions (close to 1000 deg/s).},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Gallego, Guillermo and Scaramuzza, Davide},
doi = {10.1109/LRA.2016.2647639},
eprint = {1311.2901},
isbn = {9783319464664},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Computer vision for other robotic applications,localization},
number = {2},
pages = {632--639},
pmid = {10463930},
title = {{Accurate angular velocity estimation with an event camera}},
url = {http://ieeexplore.ieee.org/document/7805257/},
volume = {2},
year = {2017}
}
@article{Chirikjian2014,
abstract = {Extended Kalman filters on Lie groups arise naturally in the context of pose estimation and more generally in robot localization and mapping. Typically in such settings one deals with nonlinear measurement models that are handled through linearization and linearized uncertainty transformation. To circumvent the loss of accuracy resulting from the typical coordinate-based linearization, this paper develops a method for accurately describing the probability density associated with nonlinear measurement models by a second-order approximation of a distribution defined directly on the Lie group configuration space. We show that, like the case of linearized measurement models, this density can be described well as a Gaussian distribution in exponential coordinates (though with different mean and covariance than those that result from linearized measurement models). And therefore previously developed methods for propagation of uncertainty and fusion of measurements can be applied to this generalized formulation without the a priori assumption of linearized measurement. A case study using a range-bearing model in planar robot localization is presented to demonstrate the method.},
author = {Chirikjian, Gregory and Kobilarov, Marin},
doi = {10.1109/CDC.2014.7040393},
file = {::},
isbn = {9781467360906},
issn = {07431546},
journal = {Proceedings of the IEEE Conference on Decision and Control},
number = {February},
pages = {6401--6406},
title = {{Gaussian approximation of non-linear measurement models on Lie groups}},
volume = {2015-Febru},
year = {2014}
}
@article{Calafiore2015,
abstract = {Pose graph optimization (PGO) is the problem of estimating a set of poses from pairwise relative measurements. PGO is a nonconvex problem and, currently, no known technique can guarantee the computation of a global optimal solution. In this paper, we show that Lagrangian duality allows computing a globally optimal solution under conditions that are satisfied in most robotics applications and enables to certify optimality of a given estimate. Our first contribution is to frame planar PGO in the complex domain. This makes analysis easier and allows drawing connections with existing literature on unit gain graphs. The second contribution is to formulate and analyze the properties of the Lagrangian dual problem in the complex domain. Our analysis shows that the duality gap is connected to the number of zero eigenvalues of the penalized pose graph matrix. We prove that if this matrix has a single zero eigenvalue, then 1) the duality gap is zero, 2) the primal PGO problem has a unique solution (up to an arbitrary roto-translation), and 3) the primal solution can be computed by scaling an eigenvector of the penalized pose graph matrix. The third contribution is algorithmic: We leverage duality to devise and algorithm that computes the optimal solution when the penalized matrix has a single zero eigenvalue. We also propose a suboptimal variant when the zero eigenvalues are multiple. Finally, we show that duality provides computational tools to verify if a given estimate (e.g., computed using iterative solvers) is globally optimal. We conclude the paper with an extensive numerical analysis. Empirical evidence shows that, in the vast majority of cases (100% of the tests under noise regimes of practical robotics applications), the penalized pose graph matrix has a single zero eigenvalue; hence, our approach allows computing (or verifying) the optimal solution.},
archivePrefix = {arXiv},
arxivId = {1505.0343},
author = {Carlone, Luca and Calafiore, Giuseppe C. and Tommolillo, Carlo and Dellaert, Frank},
doi = {10.1109/TRO.2016.2544304},
eprint = {1505.0343},
file = {::},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Maximum likelihood estimation,mobile robots,motion estimation,motion measurement,optimization,position measurement,rotation measurement,simultaneous localization and mapping,state estimation},
month = {jun},
number = {3},
pages = {545--565},
title = {{Planar Pose Graph Optimization: Duality, Optimal Solutions, and Verification}},
url = {http://arxiv.org/abs/1505.0343 http://ieeexplore.ieee.org/document/7470464/},
volume = {32},
year = {2016}
}
@article{Tatsch2017,
abstract = {We introduce Dimitri, an open-software & open-hardware humanoid robot with 31 DOFs, fitted with cost-effective modular compliant joints and parallel link legs, designed for advanced human-robot interaction research, force-informed object handling and intelligent environment discovery. Our main innovation is in the design of a robust full-body biped humanoid robot equipped with very low-cost polyurethane torsional spring fixed to traditional servo motors and a circuit to measure angular displacement, transforming the system into a series elastic actuator (SEA). In order to illustrate the robot's qualities in the field of machine learning applied to robotics and manipulation, a multiple timescale recurrent neural network (MTRNN) is implemented, allowing the robot to replicate combined movement sequences earlier taught via interactive demonstration.},
author = {Tatsch, Christopher and Ahmadi, Ahmadreza and Bottega, Fabr{\'{i}}cio and Tani, Jun and {da Silva Guerra}, Rodrigo},
doi = {10.1007/s10846-017-0727-y},
file = {::},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Compliant joints,Humanoid robot,MTRNN,Neural network},
number = {2},
pages = {291--300},
title = {{Dimitri: an Open-Source Humanoid Robot with Compliant Joint}},
volume = {91},
year = {2018}
}
@article{Eade2013a,
abstract = {Let x ∈ X be the state parameters to be optimized, with n degrees of freedom. The goal of the optimization is to maximize the likelihood of a set of observa-tions given the parameters, under a specified observation model. 1.1 Observations For some problems, the observations are represented directly, either as vectors in R m or as elements on a manifold with m degrees of freedom. For example, observations of points in an image are vectors in R 2 (m = 2). Observations of coordinate transformations in 3D are elements of SE(3) (m = 6). In these cases, we refer to the collective observation as z ∈ Z. Often the collective observation is built by stacking up M independent observations {z i ∈ Z}:},
author = {Eade, Ethan},
number = {3},
pages = {1--9},
title = {{Gauss-Newton / Levenberg-Marquardt Optimization}},
volume = {1},
year = {2013}
}
@article{Hast2013,
abstract = {A novel idea on how to make RANSAC repeatable is presented, which will find the optimal set in nearly every run for certain types of applications. The proposed algorithm can be used for such transformations that can be constructed by more than the minimal points required. We give examples on matching of aerial images using the Direct Linear Transformation, which requires at least four points. Moreover, we give examples on how the algorithm can be used for finding a plane in 3D using three points or more. Due to its random nature, standard RANSAC is not always able to find the optimal set even for moderately contaminated sets and it usually performs badly when the number of inliers is less than 50%. However, our algorithm is capable of finding the optimal set for heavily contaminated sets, even for an inlier ratio under 5%. The proposed algorithm is based on several known methods, which we modify in a unique way and together they produce a result that is quite different from what each method can produce on its own.},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
doi = {10.1.1.699.9947},
issn = {12136972},
journal = {Journal of WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://uu.diva-portal.org/smash/record.jsf?pid=diva2:624363},
volume = {21},
year = {2013}
}
@article{Rebecq2017a,
abstract = {We present EVO, an event-based visual odometry algorithm. Our algorithm successfully leverages the outstanding properties of event cameras to track fast camera motions while recovering a semidense three-dimensional (3-D) map of the environment. The implementation runs in real time on a standard CPU and outputs up to several hundred pose estimates per second. Due to the nature of event cameras, our algorithm is unaffected by motion blur and operates very well in challenging, high dynamic range conditions with strong illumination changes. To achieve this, we combine a novel, event-based tracking approach based on image-to-model alignment with a recent event-based 3-D reconstruction algorithm in a parallel fashion. Additionally, we show that the output of our pipeline can be used to reconstruct intensity images from the binary event stream, though our algorithm does not require such intensity information. We believe that this work makes significant progress in simultaneous localization and mapping by unlocking the potential of event cameras. This allows us to tackle challenging scenarios that are currently inaccessible to standard cameras.},
author = {Rebecq, Henri and Horstschaefer, Timo and Gallego, Guillermo and Scaramuzza, Davide},
doi = {10.1109/LRA.2016.2645143},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {SLAM,localization,mapping},
number = {2},
pages = {593--600},
title = {{EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping in Real Time}},
url = {http://ieeexplore.ieee.org/document/7797445/},
volume = {2},
year = {2017}
}
@article{Gridseth2019,
abstract = {Vision-based path following allows robots to autonomously repeat manually taught paths. Stereo Visual Teach and Repeat (VT&R) [1] accomplishes robust long-range path following in unstructured outdoor environments. VT&R uses sparse features to match images for visual odometry (VO) and localization. This paper describes our first implementation of direct localization for VT&R. Instead of using sparse visual features for image matching, we minimize a photometric residual cost over the whole image. We compare the performance of feature-based and direct localization using challenging offroad driving datasets. The results show that direct localization consistently achieves more accurate pose estimation under nominal conditions, but further work is required to increase robustness to large lighting change between the teach and repeat images.},
author = {Gridseth, Mona and Barfoot, Timothy},
doi = {10.1109/CRV.2019.00021},
file = {:home/matias/Documents/Mendeley Desktop/Gridseth, Barfoot/2019/Gridseth, Barfoot - 2019 - Towards direct localization for visual teach and repeat.pdf:pdf},
isbn = {9781728118383},
journal = {Proceedings - 2019 16th Conference on Computer and Robot Vision, CRV 2019},
keywords = {Computer Vision,Localization},
pages = {97--104},
publisher = {IEEE},
title = {{Towards direct localization for visual teach and repeat}},
year = {2019}
}
@inproceedings{Pizzoli2014,
abstract = {In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms state-of-the-art techniques in terms of accuracy, while exhibiting high efficiency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation). Our CUDA-based implementation runs at 30Hz on a laptop computer and is released as open-source software.},
author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6907233},
isbn = {978-1-4799-3685-4},
issn = {10504729},
month = {may},
pages = {2609--2616},
publisher = {IEEE},
title = {{REMODE: Probabilistic, monocular dense reconstruction in real time}},
url = {http://ieeexplore.ieee.org/document/6907233/},
year = {2014}
}
@inproceedings{Roston1992,
abstract = {Autonomous and teleoperated mobile robots require an accurate knowledge of their spatial location in order to acctrmplish many tasks. Many mobile robots make use of dead reckoning navigation because of its simplicity, low cost and robustness. Although dead reckoning navigation has been used for centuries for ships and wheeled vehicles, the application to a walking machine is novel. Since walking machines differ greatly from ships and wheeled vehicles, a new approach to dead reckoning was developed to solve this problem. This paper discusses the problem, a solution, preliminary test results and future goals for dead reckoning navigation. Experiments were done with the Amhler, an autonomous, six-legged walking robot, hut the results are general and apply to any statically stable walking robot. The current results show a systematic bias of two percent of body advance in the direction of travel. Although the cause of this bias is unknown, it is corrected in the position estimation routines.},
author = {Roston, G. P. and Krotkov, E. P.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.1992.587401},
file = {::},
isbn = {0780307372},
issn = {21530866},
pages = {607--612},
publisher = {IEEE},
title = {{Dead reckoning navigation for walking robots}},
url = {http://ieeexplore.ieee.org/document/587401/},
volume = {1},
year = {1992}
}
@article{Shewchuk1994,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
doi = {10.1.1.110.418},
eprint = {1102.0183},
file = {::},
isbn = {9781479928934},
issn = {14708728},
journal = {Science},
keywords = {1,2,5,agonizing pain,conjugate gradient method,convergence analysis,eigen do,eigenvalues,i try,jacobi iterations,preconditioning,thinking with eigenvectors},
number = {CS-94-125},
pages = {64},
pmid = {17348934},
title = {{High-Performance Neural Networks for Visual Object Classification}},
url = {http://arxiv.org/abs/1102.0183},
volume = {49},
year = {2011}
}
@article{Jin,
archivePrefix = {arXiv},
arxivId = {arXiv:2003.01587v3},
author = {Jin, Yuhe and Mishkin, Dmytro and Mishchuk, Anastasiia and Matas, Jiri and Fua, Pascal and Aug, C V},
eprint = {arXiv:2003.01587v3},
file = {:home/matias/Documents/Mendeley Desktop/Jin et al/Unknown/Jin et al. - Unknown - Image Matching Across Wide Baselines From Paper to Practice.pdf:pdf},
title = {{Image Matching Across Wide Baselines : From Paper to Practice}}
}
@article{Sprowitz2013,
abstract = {We present the design of a novel compliant quadruped robot, called Cheetah-cub, and a series of locomotion experiments with fast trotting gaits. The robot's leg configuration is based on a spring-loaded, pantograph mechanism with multiple segments. A dedicated open-loop locomotion controller was derived and implemented. Experiments were run in simulation and in hardware on flat terrain and with a step down, demonstrating the robot's self-stabilizing properties. The robot reached a running trot with short flight phases with a maximum Froude number of FR = 1.30, or 6.9 body lengths per second. Morphological parameters such as the leg design also played a role. By adding distal in-series elasticity, self-stability and maximum robot speed improved. Our robot has several advantages, especially when compared with larger and stiffer quadruped robot designs. (1) It is, to the best of the authors' knowledge, the fastest of all quadruped robots below 30kg (in terms of Froude number and body lengths per second). (2) It shows self-stabilizing behavior over a large range of speeds with open-loop control. (3) It is lightweight, compact, and electrically powered. (4) It is cheap, easy to reproduce, robust, and safe to handle. This makes it an excellent tool for research of multi-segment legs in quadruped robots. {\textcopyright} The Author(s) 2013.},
author = {Spr{\"{o}}witz, Alexander and Tuleu, Alexandre and Vespignani, Massimo and Ajallooeian, Mostafa and Badri, Emilie and Ijspeert, Auke Jan},
doi = {10.1177/0278364913489205},
file = {::},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {coupled oscillator,drive function,pantograph,passive compliance,robotic leg design,robust locomotion,self-stabilizing quadruped locomotion,three-segment},
number = {8},
pages = {932--950},
title = {{Towards dynamic trot gait locomotion: Design, control, and experiments with Cheetah-cub, a compliant quadruped robot}},
volume = {32},
year = {2013}
}
@article{Blackman2017,
abstract = {Dynamic legged robots are capable of a wide range of behaviors, such as running, climbing, and jumping. Often the leg morphology and compliance are tailored to these specific behaviors. In this paper, we examine the design modifications to a 5-bar closed-loop kinematic leg design that enable both fast, stable running as well as energetic jumping. After investigating the running and jumping dynamics of the 5-bar leg design, control was implemented to transition between the desired configurations for each dynamic behavior in order to produce a desired maximum obstacle negotiation.},
author = {Blackman, Daniel J. and Nicholson, John V. and Pusey, Jason L. and Austin, Max P. and Young, Charles and Brown, Jason M. and Clark, Jonathan E.},
doi = {10.1109/ROBIO.2017.8324814},
file = {:home/matias/Documents/Mendeley Desktop/Blackman et al/2018/Blackman et al. - 2018 - Leg design for running and jumping dynamics.pdf:pdf},
isbn = {9781538637418},
journal = {2017 IEEE International Conference on Robotics and Biomimetics, ROBIO 2017},
keywords = {Biologically-Inspired Robots,Multilegged Robots,Passive Walking},
month = {dec},
pages = {2617--2623},
publisher = {IEEE},
title = {{Leg design for running and jumping dynamics}},
url = {http://ieeexplore.ieee.org/document/8324814/},
volume = {2018-Janua},
year = {2018}
}
@article{Newcombe2011,
abstract = {DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application. {\textcopyright} 2011 IEEE.},
author = {Newcombe, Richard A. and Lovegrove, Steven J. and Davison, Andrew J.},
doi = {10.1109/ICCV.2011.6126513},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2320--2327},
pmid = {6126513},
title = {{DTAM: Dense tracking and mapping in real-time}},
year = {2011}
}
@article{Johnson2015,
abstract = {This article is a summary of the experiences of the Florida Institute for Human & Machine Cognition (IHMC) team during the DARPA Robotics Challenge (DRC) Trials. The primary goal of the DRC is to develop robots capable of assisting humans in responding to natural and manmade disasters. The robots are expected to use standard tools and equipment to accomplish the mission. The DRC Trials consisted of eight different challenges that tested robot mobility, manipulation, and control under degraded communications and time constraints. Team IHMC competed using the Atlas humanoid robot made by Boston Dynamics. We competed against 16 international teams and placed second in the competition. This article discusses the challenges we faced in transitioning from simulation to hardware. It also discusses the lessons learned both during the competition and in the months of preparation leading up to it. The lessons address the value of reliable hardware and solid software practices. They also cover effective approaches to bipedal walking and designing for human-robot teamwork. Lastly, the lessons present a philosophical discussion about choices related to designing robotic systems.},
author = {Johnson, Matthew and Shrewsbury, Brandon and Bertrand, Sylvain and Wu, Tingfan and Duran, Daniel and Floyd, Marshall and Abeles, Peter and Stephen, Douglas and Mertins, Nathan and Lesman, Alex and Carff, John and Rifenburgh, William and Kaveti, Pushyami and Straatman, Wessel and Smith, Jesper and Griffioen, Maarten and Layton, Brooke and {De Boer}, Tomas and Koolen, Twan and Neuhaus, Peter and Pratt, Jerry},
doi = {10.1002/rob.21571},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {192--208},
pmid = {22164016},
title = {{Team IHMC's lessons learned from the DARPA robotics challenge trials}},
url = {http://doi.wiley.com/10.1002/rob.21571},
volume = {32},
year = {2015}
}
@article{Camara2019,
author = {Camara, Luis G and G{\"{a}}bert, Carl and Pˇ, Libor},
file = {:home/matias/Documents/Mendeley Desktop/Camara, G{\"{a}}bert, Pˇ/2020/Camara, G{\"{a}}bert, Pˇ - 2020 - Highly Robust Visual Place Recognition Through Spatial Matching of CNN Highly Robust Visual Place Recognit.pdf:pdf},
isbn = {9781728173955},
number = {September},
pages = {3748--3755},
title = {{Highly Robust Visual Place Recognition Through Spatial Matching of CNN Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features}},
year = {2019}
}
@article{Labbe2018,
abstract = {For long-term simultaneous planning, localization and mapping (SPLAM), a robot should be able to continuously update its map according to the dynamic changes of the environment and the new areas explored. With limited onboard computation capabilities, a robot should also be able to limit the size of the map used for online localization and mapping. This paper addresses these challenges using a memory management mechanism, which identifies locations that should remain in a Working Memory (WM) for online processing from locations that should be transferred to a Long-Term Memory (LTM). When revisiting previously mapped areas that are in LTM, the mechanism can retrieve these locations and place them back in WM for online SPLAM. The approach is tested on a robot equipped with a short-range laser rangefinder and a RGB-D camera, patrolling autonomously 10.5 km in an indoor environment over 11 sessions while having encountered 139 people.},
author = {Labb{\'{e}}, Mathieu and Michaud, Fran{\c{c}}ois},
doi = {10.1007/s10514-017-9682-5},
file = {:home/matias/Documents/Mendeley Desktop/Labb{\'{e}}, Michaud/2018/Labb{\'{e}}, Michaud - 2018 - Long-term online multi-session graph-based SPLAM with memory management.pdf:pdf},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {Loop closure detection,Multi-session,Path planning,Pose graph,SLAM},
number = {6},
pages = {1133--1150},
title = {{Long-term online multi-session graph-based SPLAM with memory management}},
volume = {42},
year = {2018}
}
@article{Lang2017,
abstract = {In this letter, we address the modeling and learning of complex nonlinear rigid-body motions employing Gaussian processes. As the common procedure of using Euler angles in the Gaussian process results in inaccurate predictions for large rotations, we represent the input data by axis-angle pseudovectors for rotations and Euclidean vectors for translation. Our decision in favor of this representation of the special Euclidean group SE(3) is due to its computational efficiency. To allow Gaussian process estimation on a non-Euclidean input domain, such as the space of rigid motions, we generalize the model by introducing novel mean and covariance functions on SE(3). We prove that those functions fulfill the requirements of Gaussian processes. The proposed approach is validated on simulated and on real human motion data. Our results demonstrate significant benefits of the proposed rigid-body Gaussian process with respect to alternative variants in terms of regression performance and computational efficiency.},
author = {Lang, Muriel and Hirche, Sandra},
doi = {10.1109/LRA.2017.2677469},
file = {::},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Behaviour-based systems,learning and adaptive systems,probability and statistical methods},
month = {jul},
number = {3},
pages = {1601--1608},
title = {{Computationally Efficient Rigid-Body Gaussian Process for Motion Dynamics}},
url = {http://ieeexplore.ieee.org/document/7869309/},
volume = {2},
year = {2017}
}
@book{Sastry1999,
abstract = {Acrolein, a reactive, $\alpha$$\beta$-unsaturated aldehyde which is ubiquitous in the environment, forms DNA adducts, is mutagenic, and is teratogenic. However, studies have not indicated a carcinogenic effect in rodent bioassays. Since it is present in cigarette smoke and is the toxic metabolite of cyclophosphamide with respect to the urinary tract, we investigated the possibility that acrolein might have carcinogenic activity toward the rat urinary bladder. We also evaluated whether it possessed initiating and/or promoting activity. To evaluate initiating activity, acrolein was administered at a dose of 2 mg/kg i.p. twice a week for 6 weeks followed by uracil as 3% of the diet for 20 weeks and then control diet for 6 weeks. yV-4-(5-Nitro-2-furyl)-2-thiazolyl]fonnamide (FANFT) as 0.2% of the diet followed by uracil was used as a positive control, and a negative control group was administered solvent control (water) i.p. during the 6-week initiation period followed by uracil. Acrolein followed by uracil produced an incidence of 18 of 30 rats (60%) with papilloma compared to 8 of 30 rats (27%) treated with solvent control followed by uracil. FANFT followed by uracil produced an incidence of 70% carcinomas and 30% papillomas, clearly indicating that it is a much more potent initiating agent than acrolein. Acrolein for 6 weeks followed by control diet produced no tumors. To evaluate promoting activity, groups of rats were fed FANFT for 6 weeks followed by acrolein. Acrolein administered during the initial 6 weeks and continued for the second phase of the experiment (to evaluate complete carcinogenic activity) resulted in severe toxicity. Administration of acrolein had to be terminated after 21 weeks of the experiment. The animals were maintained for 53 weeks of the experiment without further chemical treatment, and there was no evidence of papilloma or carcinoma development. This study clearly indicates that acrolein has initiating activity for the urinary bladder when administered by i.p. injection to the male F344 rat, but toxicity precluded evaluation of its promoting or complete carcinogenic activity. {\textcopyright} 1992, American Association for Cancer Research. All rights reserved.},
address = {New York, NY},
author = {Okamura, Takehiko and Garland, Emily M. and John, Margaret St and Okamura, Takehiko and Smith, Raymond A.},
booktitle = {Cancer Research},
doi = {10.1007/978-1-4757-3108-8},
file = {::},
isbn = {978-1-4419-3132-0},
issn = {15387445},
number = {13},
pages = {3577--3581},
pmid = {1617627},
publisher = {Springer New York},
series = {Interdisciplinary Applied Mathematics},
title = {{Acrolein Initiates Rat Urinary Bladder Carcinogenesis}},
url = {http://link.springer.com/10.1007/978-1-4757-3108-8},
volume = {52},
year = {1992}
}
@article{Rodriguez2020,
author = {Rodriguez, Eder and Caron, Guillaume and Pegard, Claude and Lara, Alabazares David},
file = {:home/matias/Documents/Mendeley Desktop/Rodriguez et al/2020/Rodriguez et al. - 2020 - Photometric Path Planning for Vision-Based Navigation.pdf:pdf},
isbn = {9781728173955},
journal = {IEEE International Conference on Robotics and Automation},
pages = {9007--9013},
title = {{Photometric Path Planning for Vision-Based Navigation}},
year = {2020}
}
@article{Roelfsema2017,
abstract = {Humans and many other animals have an enormous capacity to learn about sensory stimuli and to master new skills. However, many of the mechanisms that enable us to learn remain to be understood. One of the greatest challenges of systems neuroscience is to explain how synaptic connections change to support maximally adaptive behaviour. Here, we provide an overview of factors that determine the change in the strength of synapses, with a focus on synaptic plasticity in sensory cortices. We review the influence of neuromodulators and feedback connections in synaptic plasticity and suggest a specific framework in which these factors can interact to improve the functioning of the entire network.},
author = {Roelfsema, Pieter R. and Holtmaat, Anthony},
doi = {10.1038/nrn.2018.6},
file = {::},
issn = {14710048},
journal = {Nature Reviews Neuroscience},
number = {3},
pages = {166--180},
pmid = {29449713},
publisher = {Nature Publishing Group},
title = {{Control of synaptic plasticity in deep cortical networks}},
url = {http://dx.doi.org/10.1038/nrn.2018.6},
volume = {19},
year = {2018}
}
@article{,
title = {{Global localization from a single feature correspondence Friedrich Fraundorfer and Horst Bischof Institute for Computer Graphics and Vision Graz University of Technology}}
}
@article{Sprunk2015,
author = {Sprunk, Cristoph},
file = {:home/matias/Documents/Mendeley Desktop/Sprunk/2015/Sprunk - 2015 - Highly Accurate Mobile Robot Navigation.pdf:pdf},
pages = {180},
title = {{Highly Accurate Mobile Robot Navigation}},
url = {http://ais.informatik.uni-freiburg.de/publications/papers/sprunk15phd.pdf},
year = {2015}
}
@article{Vasco2017,
abstract = {Unlike standard cameras that send intensity images at a constant frame rate, event-driven cameras asynchronously report pixel-level brightness changes, offering low latency and high temporal resolution (both in the order of micro-seconds). As such, they have great potential for fast and low power vision algorithms for robots. Visual tracking, for example, is easily achieved even for very fast stimuli, as only moving objects cause brightness changes. However, cameras mounted on a moving robot are typically non-stationary and the same tracking problem becomes confounded by background clutter events due to the robot ego-motion. In this paper, we propose a method for segmenting the motion of an independently moving object for event-driven cameras. Our method detects and tracks corners in the event stream and learns the statistics of their motion as a function of the robot's joint velocities when no independently moving objects are present. During robot operation, independently moving objects are identified by discrepancies between the predicted corner velocities from ego-motion and the measured corner velocities. We validate the algorithm on data collected from the neuromorphic iCub robot. We achieve a precision of ∼ 90% and show that the method is robust to changes in speed of both the head and the target.},
archivePrefix = {arXiv},
arxivId = {1706.08713},
author = {Vasco, V. and Glover, A. and Mueggler, E. and Scaramuzza, D. and Natale, L. and Bartolozzi, C.},
doi = {10.1109/ICAR.2017.8023661},
eprint = {1706.08713},
file = {::},
isbn = {9781538631577},
journal = {2017 18th International Conference on Advanced Robotics, ICAR 2017},
keywords = {Event-driven cameras,Independent motion,Neuromorphic processing},
pages = {530--536},
title = {{Independent motion detection with event-driven cameras}},
year = {2017}
}
@article{Bronstein2017,
abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
file = {::},
isbn = {9781450345385},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
month = {jul},
number = {4},
pages = {18--42},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
url = {http://dl.acm.org/citation.cfm?doid=2988458.2988485 http://ieeexplore.ieee.org/document/7974879/},
volume = {34},
year = {2017}
}
@article{Botvinick2019,
abstract = {Deep reinforcement learning (RL)methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
doi = {10.1016/j.tics.2019.02.006},
file = {:home/matias/Documents/Mendeley Desktop/Botvinick et al/2019/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf:pdf},
issn = {1879307X},
journal = {Trends in Cognitive Sciences},
number = {5},
pages = {408--422},
pmid = {31003893},
publisher = {Elsevier Ltd},
title = {{Reinforcement Learning, Fast and Slow}},
url = {https://doi.org/10.1016/j.tics.2019.02.006},
volume = {23},
year = {2019}
}
@inproceedings{Camara,
author = {Camara, Luis G and Pivonka, Tomas and Jilek, Martin and Gabert, Carl and Kosnar, Karel and Preucil, Libor},
booktitle = {IEEE/RSJ 2020 International Conference on Intelligent Robots and Systems, IROS 2020 - Conference Proceedings},
file = {:home/matias/Documents/Mendeley Desktop/Camara et al/Unknown/Camara et al. - Unknown - Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition A Accurate and Robust Teach and R.pdf:pdf},
number = {October},
title = {{Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition : A CNN Approach}},
year = {2020}
}
@article{Azkarate2019,
abstract = {This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (low) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long range, low supervision and fast planetary exploration. Both navigation levels have been validated on planetary analogue field test campaigns.},
archivePrefix = {arXiv},
arxivId = {1911.09975},
author = {Azkarate, Martin and Gerdes, Levin and Joudrier, Luc and P{\'{e}}rez-del-Pulgar, Carlos J.},
eprint = {1911.09975},
file = {:home/matias/Documents/Mendeley Desktop/Azkarate et al/2019/Azkarate et al. - 2019 - A GNC Architecture for Planetary Rovers with Autonomous Navigation Capabilities.pdf:pdf},
isbn = {9781728173955},
pages = {3003--3009},
title = {{A GNC Architecture for Planetary Rovers with Autonomous Navigation Capabilities}},
url = {http://arxiv.org/abs/1911.09975},
year = {2019}
}
@article{Martinez-Cantin2014,
abstract = {BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlin-ear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization characterized for being sample efficient as it builds a posterior distribution to capture the evidence and prior knowledge of the target function. Built in standard C++, the library is extremely efficient while being portable and exible. It includes a common interface for C, C++, Python, Matlab and Octave.},
archivePrefix = {arXiv},
arxivId = {1405.7430},
author = {Martinez-Cantin, Ruben},
eprint = {1405.7430},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian optimization,Efficient global optimization,Gaussian processes,Sequential experimental design,Sequential model-based optimization},
pages = {3735--3739},
title = {{BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits}},
url = {http://arxiv.org/abs/1405.7430},
volume = {15},
year = {2015}
}
@book{Corke2017a,
abstract = {In binary tomography, the goal is to reconstruct binary images from a small set of their projections. This task can be underdetermined, meaning that several binary images can have the same projections, especially when only one or two projections are given. On the other hand, it is known that a binary image can be exactly reconstructed from its morphological skeleton when all skeletal labels are known. However, if only the skeletal points are given, different labellings yield different reconstructed images. In this paper, we consider a mixture of the above problems, reconstructing a binary image from few projections and the morphological skeleton. We show that the problem is NP-complete, yet a result with low projection and pixel error usually can be achieved, even if only a single projection is available. Three different variants of a method based on Simulated Annealing are developed and compared with respect to reconstruction time and error using artificial binary images.},
address = {Cham},
author = {Corke, Peter},
doi = {10.1007/978-3-319-54413-7},
file = {:home/matias/Documents/Mendeley Desktop/Corke/2017/Corke - 2017 - Robotics, Vision and Control.pdf:pdf},
isbn = {978-3-319-54412-0},
issn = {15737470},
keywords = {Binary tomography,Morphological skeleton,NP-completeness,Reconstruction,Simulated annealing},
number = {1-2},
pages = {195--216},
publisher = {Springer International Publishing},
series = {Springer Tracts in Advanced Robotics},
title = {{Robotics, Vision and Control}},
url = {http://link.springer.com/10.1007/978-3-319-54413-7},
volume = {118},
year = {2017}
}
@article{Ratliff2018,
abstract = {We introduce the Riemannian Motion Policy (RMP), a new mathematical object for modular motion generation. An RMP is a second-order dynamical system (acceleration field or motion policy) coupled with a corresponding Riemannian metric. The motion policy maps positions and velocities to accelerations, while the metric captures the directions in the space important to the policy. We show that RMPs provide a straightforward and convenient method for combining multiple motion policies and transforming such policies from one space (such as the task space) to another (such as the configuration space) in geometrically consistent ways. The operators we derive for these combinations and transformations are provably optimal, have linearity properties making them agnostic to the order of application, and are strongly analogous to the covariant transformations of natural gradients popular in the machine learning literature. The RMP framework enables the fusion of motion policies from different motion generation paradigms, such as dynamical systems, dynamic movement primitives (DMPs), optimal control, operational space control, nonlinear reactive controllers, motion optimization, and model predictive control (MPC), thus unifying these disparate techniques from the literature. RMPs are easy to implement and manipulate, facilitate controller design, simplify handling of joint limits, and clarify a number of open questions regarding the proper fusion of motion generation methods (such as incorporating local reactive policies into long-horizon optimizers). We demonstrate the effectiveness of RMPs on both simulation and real robots, including their ability to naturally and efficiently solve complicated collision avoidance problems previously handled by more complex planners.},
archivePrefix = {arXiv},
arxivId = {1801.02854},
author = {Ratliff, Nathan D. and Issac, Jan and Kappler, Daniel and Birchfield, Stan and Fox, Dieter},
eprint = {1801.02854},
file = {:home/matias/Documents/Mendeley Desktop/Ratliff et al/2018/Ratliff et al. - 2018 - Riemannian Motion Policies.pdf:pdf},
title = {{Riemannian Motion Policies}},
url = {http://arxiv.org/abs/1801.02854},
year = {2018}
}
@article{Kumar2018,
abstract = {Humans routinely retrace paths in a novel environment both forwards and backwards despite uncertainty in their motion. This paper presents an approach for doing so. Given a demonstration of a path, a first network generates a path abstraction. Equipped with this abstraction, a second network observes the world and decides how to act to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following and homing under actuation noise and environmental changes. Our experiments show that our approach outperforms classical approaches and other learning based baselines.},
archivePrefix = {arXiv},
arxivId = {1812.00940},
author = {Kumar, Ashish and Gupta, Saurabh and Fouhey, David and Levine, Sergey and Malik, Jitendra},
eprint = {1812.00940},
file = {:home/matias/Documents/Mendeley Desktop/Kumar et al/2018/Kumar et al. - 2018 - Visual memory for robust path following.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {765--774},
title = {{Visual memory for robust path following}},
volume = {2018-Decem},
year = {2018}
}
@article{Kahn2020,
abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with self-supervised off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr},
archivePrefix = {arXiv},
arxivId = {2002.05700},
author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
eprint = {2002.05700},
file = {:home/matias/Documents/Mendeley Desktop/Kahn, Abbeel, Levine/2020/Kahn, Abbeel, Levine - 2020 - BADGR An Autonomous Self-Supervised Learning-Based Navigation System.pdf:pdf},
title = {{BADGR: An Autonomous Self-Supervised Learning-Based Navigation System}},
url = {http://arxiv.org/abs/2002.05700},
year = {2020}
}
@book{DeBerg2008,
abstract = {This well-accepted introduction to computational geometry is a textbook for high-level undergraduate and low-level graduate courses. The focus is on algorithms and hence the book is well suited for students in computer science and engineering. Motivation is provided from the application areas: all solutions and techniques from computational geometry are related to particular applications in robotics, graphics, CAD/CAM, and geographic information systems. For students this motivation will be especially welcome. Modern insights in computational geometry are used to provide solutions that are both efficient and easy to understand and implement. All the basic techniques and topics from computational geometry, as well as several more advanced topics, are covered. The book is largely self-contained and can be used for self-study by anyone with a basic background in algorithms. In this third edition, besides revisions to the second edition, new sections discussing Voronoi diagrams of line segments, farthest-point Voronoi diagrams, and realistic input models have been added. {\textcopyright} 2008, 2000, 1997 Springer-Verlag Berlin Heidelberg.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{De Berg}, Mark and Cheong, Otfried and {Van Kreveld}, Marc and Overmars, Mark},
booktitle = {Computational Geometry: Algorithms and Applications},
doi = {10.1007/978-3-540-77974-2},
eprint = {arXiv:1011.1669v3},
file = {:home/matias/Documents/Mendeley Desktop/De Berg et al/2008/De Berg et al. - 2008 - Computational geometry Algorithms and applications.pdf:pdf},
isbn = {9783540779735},
issn = {14337851},
pages = {1--386},
pmid = {25246403},
publisher = {Springer Berlin Heidelberg},
title = {{Computational geometry: Algorithms and applications}},
url = {http://link.springer.com/10.1007/978-3-540-77974-2},
year = {2008}
}
@book{Skiena2012,
author = {Skiena, Steven S. and Skiena, Steven S.},
booktitle = {The Algorithm Design Manual},
doi = {10.1007/978-1-84800-070-4_1},
file = {:home/matias/Documents/Mendeley Desktop/Skiena, Skiena/2012/Skiena, Skiena - 2012 - Introduction to Algorithm Design.pdf:pdf},
isbn = {9781848000698},
pages = {3--30},
title = {{Introduction to Algorithm Design}},
year = {2012}
}
@book{Featherstone1987,
abstract = {Rigid Body Dynamics Algorithms presents the subject of computational rigid-body dynamics through the medium of spatial 6D vector notation. It explains how to model a rigid-body system and how to analyze it, and it presents the most comprehensive collection of the best rigid-body dynamics algorithms to be found in a single source. The use of spatial vector notation greatly reduces the volume of algebra which allows systems to be described using fewer equations and fewer quantities. It also allows problems to be solved in fewer steps, and solutions to be expressed more succinctly. In addition algorithms are explained simply and clearly, and are expressed in a compact form. The use of spatial vector notation facilitates the implementation of dynamics algorithms on a computer: shorter, simpler code that is easier to write, understand and debug, with no loss of efficiency. Unique features include: A comprehensive collection of the best rigid-body dynamics algorithms Use of spatial (6D) vectors to greatly reduce the volume of algebra, to simplify the treatment of the subject, and to simplify the computer code that implements the algorithms Algorithms expressed both mathematically and in pseudocode for easy translation into computer programs Source code for many algorithms available on the internet Rigid Body Dynamics Algorithms is aimed at readers who already have some elementary knowledge of rigid-body dynamics, and are interested in calculating the dynamics of a rigid-body system. This book serves as an algorithms recipe book as well as a guide to the analysis and deeper understanding of rigid-body systems.},
address = {Boston, MA},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Featherstone, Roy},
booktitle = {Robot Dynamics Algorithms},
doi = {10.1007/978-0-387-74315-8},
eprint = {arXiv:1011.1669v3},
file = {:home/matias/Documents/Mendeley Desktop/Featherstone/1987/Featherstone - 1987 - Robot Dynamics Algorithms.pdf:pdf},
isbn = {978-1-4757-6437-6},
issn = {1881-6096},
pages = {272},
pmid = {21987560},
publisher = {Springer US},
title = {{Robot Dynamics Algorithms}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-74315-8 http://link.springer.com/10.1007/978-0-387-74315-8},
year = {1987}
}
@book{Gallier2011a,
abstract = {In this chapter, we discuss the representation of rotations of R 3 and R 4 in terms of quaternions. Such a representation is not only concise and elegant, it also yields a very efficient way of handling composition of rotations. It also tends to be numerically more stable than the representation in terms of orthogonal matrices. 431},
address = {New York, NY},
author = {Algebra, The and Quaternions, H},
booktitle = {Science},
doi = {10.1007/978-1-4419-9961-0},
file = {:home/matias/Documents/Mendeley Desktop/Algebra, Quaternions/2011/Algebra, Quaternions - 2011 - The Quaternions and the Spaces S 3 , SU ( 2 ),.pdf:pdf},
isbn = {9781441999610},
issn = {0939-2475},
number = {2},
pages = {281--300},
publisher = {Springer New York},
series = {Texts in Applied Mathematics},
title = {{The Quaternions and the Spaces S 3 , SU ( 2 ),}},
url = {http://link.springer.com/10.1007/978-1-4419-9961-0},
volume = {38},
year = {2011}
}
@book{Nocedal2006,
abstract = {Summary: Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
author = {Nocedal, Jorge and Wright, Stephen J.},
booktitle = {Numerical Optimization},
doi = {10.1007/978-0-387-40065-5},
file = {:home/matias/Documents/Mendeley Desktop/Nocedal, Wright/2006/Nocedal, Wright - 2006 - Numerical Optimization.pdf:pdf},
isbn = {978-0-387-30303-1},
publisher = {Springer New York},
series = {Springer Series in Operations Research and Financial Engineering},
title = {{Numerical Optimization}},
url = {http://link.springer.com/10.1007/978-0-387-40065-5},
year = {2006}
}
@article{Pandala2019,
abstract = {In this letter, we present qpSWIFT, a real-time quadratic program (QP) solver. Motivated by the need for a robust embedded QP solver in robotic applications, qpSWIFT employs standard primal-dual interior-point method, along with Mehrotra predictor-corrector steps and Nesterov-Todd scaling. The sparse structure of the resulting Karush-Kuhn-Tucker linear system in the QP formulation is exploited, and sparse direct methods are utilized to solve the linear system of equations. To further accelerate the factorization process, we only modify the corresponding rows of the matrix factors that change during iterations and cache the nonzero Cholesky pattern. qpSWIFT is library free, written in ANSI-C and its performance is benchmarked through standard problems that could be cast as QP. Numerical results show that qpSWIFT outperforms state-of-the-art solvers for small scale problems. To evaluate the performance of the solver, a real-time implementation of the solver in the model predictive control framework through experiments on a quadrupedal robot are presented.},
author = {Pandala, Abhishek Goud and Ding, Yanran and Park, Hae Won},
doi = {10.1109/LRA.2019.2926664},
file = {:home/matias/Documents/Mendeley Desktop/Pandala, Ding, Park/2019/Pandala, Ding, Park - 2019 - QpSWIFT A Real-Time Sparse Quadratic Program Solver for Robotic Applications.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Legged robots,motion control,optimization and optimal control},
number = {4},
pages = {3355--3362},
publisher = {IEEE},
title = {{QpSWIFT: A Real-Time Sparse Quadratic Program Solver for Robotic Applications}},
volume = {4},
year = {2019}
}
@article{Mangelson2019,
abstract = {An accurate characterization of pose uncertainty is essential for safe autonomous navigation. Early pose uncertainty characterization methods proposed by Smith, Self, and Cheeseman (SCC), used coordinate-based first-order methods to propagate uncertainty through non-linear functions such as pose composition (head-to-tail), pose inversion, and relative pose extraction (tail-to-tail). Characterizing uncertainty in the Lie Algebra of the special Euclidean group results in better uncertainty estimates. However, existing approaches assume that individual poses are independent. Since factors in a pose graph induce correlation, this independence assumption is usually not reflected in reality. In addition, prior work has focused primarily on the pose composition operation. This paper develops a framework for modeling the uncertainty of jointly distributed poses and describes how to perform the equivalent of the SSC pose operations while characterizing uncertainty in the Lie Algebra. Evaluation on simulated and open-source datasets shows that the proposed methods result in more accurate uncertainty estimates. An accompanying C++ library implementation is also released. This is a pre-print of a paper submitted to IEEE TRO in 2019.},
archivePrefix = {arXiv},
arxivId = {1906.07795},
author = {Mangelson, Joshua G. and Ghaffari, Maani and Vasudevan, Ram and Eustice, Ryan M.},
doi = {10.1109/tro.2020.2994457},
eprint = {1906.07795},
file = {:home/matias/Documents/Mendeley Desktop/Mangelson et al/2020/Mangelson et al. - 2020 - Characterizing the Uncertainty of Jointly Distributed Poses in the Lie Algebra.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
number = {3},
pages = {1--18},
title = {{Characterizing the Uncertainty of Jointly Distributed Poses in the Lie Algebra}},
url = {http://arxiv.org/abs/1906.07795},
year = {2020}
}
@article{Barfoot2014,
abstract = {In this paper, we provide specific and practical approaches to associate uncertainty with 4 × 4 transformation matrices, which is a common representation for pose variables in 3-D space. We show constraint-sensitive means of perturbing transformation matrices using their associated exponential-map generators and demonstrate these tools on three simple-yet-important estimation problems: 1) propagating uncertainty through a compound pose change, 2) fusing multiple measurements of a pose (e.g., for use in pose-graph relaxation), and 3) propagating uncertainty on poses (and landmarks) through a nonlinear camera model. The contribution of the paper is the presentation of the theoretical tools, which can be applied in the analysis of many problems involving 3-D pose and point variables. {\textcopyright} 2004-2012 IEEE.},
author = {Barfoot, Timothy D. and Furgale, Paul T.},
doi = {10.1109/TRO.2014.2298059},
file = {:home/matias/Documents/Mendeley Desktop/Barfoot, Furgale/2014/Barfoot, Furgale - 2014 - Associating uncertainty with three-dimensional poses for use in estimation problems.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Exponential maps,homogeneous points,matrix Lie groups,pose uncertainty,transformation matrices},
number = {3},
pages = {679--693},
publisher = {IEEE},
title = {{Associating uncertainty with three-dimensional poses for use in estimation problems}},
volume = {30},
year = {2014}
}
@article{Giusti2015,
abstract = {We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.},
author = {Giusti, Alessandro and Guzzi, Jerome and Ciresan, Dan C. and He, Fang Lin and Rodriguez, Juan P. and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M.},
doi = {10.1109/LRA.2015.2509024},
file = {:home/matias/Documents/Mendeley Desktop/Giusti et al/2016/Giusti et al. - 2016 - A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots.pdf:pdf},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial Robotics,Deep Learning,Machine Learning,Visual-Based Navigation},
number = {2},
pages = {661--667},
pmid = {12546789},
title = {{A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7358076 http://ieeexplore.ieee.org/document/7358076/},
volume = {1},
year = {2016}
}
@article{Suwajanakorn2018,
abstract = {This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet [6] are visualized at keypointnet.github.io.},
archivePrefix = {arXiv},
arxivId = {1807.03146},
author = {Suwajanakorn, Supasorn and Snavely, Noah and Tompson, Jonathan and Norouzi, Mohammad},
eprint = {1807.03146},
file = {:home/matias/Documents/Mendeley Desktop/Suwajanakorn et al/2018/Suwajanakorn et al. - 2018 - Discovery of latent 3D keypoints via end-to-end geometric reasoning.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {2059--2070},
title = {{Discovery of latent 3D keypoints via end-to-end geometric reasoning}},
volume = {2018-Decem},
year = {2018}
}
@article{Xie2020,
author = {Xie, Linhai and Markham, Andrew and Trigoni, Niki},
file = {:home/matias/Documents/Mendeley Desktop/Xie, Markham, Trigoni/2020/Xie, Markham, Trigoni - 2020 - SnapNav Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference.pdf:pdf},
isbn = {9781728173955},
pages = {1682--1688},
title = {{SnapNav : Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference}},
year = {2020}
}
@article{VonStumberg2020,
abstract = {Direct SLAM methods have shown exceptional performance on odometry tasks. However, they are susceptible to dynamic lighting and weather changes while also suffering from a bad initialization on large baselines. To overcome this, we propose GN-Net: a network optimized with the novel Gauss-Newton loss for training weather invariant deep features, tailored for direct image alignment. Our network can be trained with pixel correspondences between images taken from different sequences. Experiments on both simulated and real-world datasets demonstrate that our approach is more robust against bad initialization, variations in day-time, and weather changes thereby outperforming state-of-the-art direct and indirect methods. Furthermore, we release an evaluation benchmark for relocalization tracking against different types of weather. Our benchmark is available at https://vision.in.tum.de/gn-net.},
archivePrefix = {arXiv},
arxivId = {1904.11932},
author = {{Von Stumberg}, Lukas and Wenzel, Patrick and Khan, Qadeer and Cremers, Daniel},
doi = {10.1109/LRA.2020.2965031},
eprint = {1904.11932},
file = {:home/matias/Documents/Mendeley Desktop/Von Stumberg et al/2020/Von Stumberg et al. - 2020 - GN-Net The Gauss-Newton Loss for Multi-Weather Relocalization.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Localization,SLAM,visual learning},
month = {apr},
number = {2},
pages = {890--897},
title = {{GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization}},
url = {https://ieeexplore.ieee.org/document/8954808/},
volume = {5},
year = {2020}
}
@phdthesis{Booij2011,
author = {Booij, Olaf},
file = {:home/matias/Documents/Mendeley Desktop/Booij/2011/Booij - 2011 - View-based mapping for wheeled robots.pdf:pdf},
school = {University of Amsterdam},
title = {{View-based mapping for wheeled robots}},
year = {2011}
}
@inproceedings{Gridseth2020,
abstract = {Vision-based path following allows robots to autonomously repeat manually taught paths. Stereo Visual Teach and Repeat (VT\&R) accomplishes accurate and robust long-range path following in unstructured outdoor environments across changing lighting, weather, and seasons by relying on colour-constant imaging and multi-experience localization. We leverage multi-experience VT\&R together with two datasets of outdoor driving on two separate paths spanning different times of day, weather, and seasons to teach a deep neural network to predict relative pose for visual odometry (VO) and for localization with respect to a path. In this paper we run experiments exclusively on datasets to study how the network generalizes across environmental conditions. Based on the results we believe that our system achieves relative pose estimates sufficiently accurate for in-the-loop path following and that it is able to localize radically different conditions against each other directly (i.e. winter to spring and day to night), a capability that our hand-engineered system does not have.},
archivePrefix = {arXiv},
arxivId = {2003.02946},
author = {Gridseth, Mona and Barfoot, Timothy D.},
booktitle = {ICRA},
eprint = {2003.02946},
file = {:home/matias/Documents/Mendeley Desktop/Gridseth, Barfoot/2020/Gridseth, Barfoot - 2020 - DeepMEL Compiling Visual Multi-Experience Localization into a Deep Neural Network.pdf:pdf},
title = {{DeepMEL: Compiling Visual Multi-Experience Localization into a Deep Neural Network}},
url = {http://arxiv.org/abs/2003.02946},
year = {2020}
}
@inproceedings{Meng2019,
abstract = {Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive solution to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.},
archivePrefix = {arXiv},
arxivId = {1909.12329},
author = {Meng, Xiangyun and Ratliff, Nathan and Xiang, Yu and Fox, Dieter},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
eprint = {1909.12329},
file = {:home/matias/Documents/Mendeley Desktop/Meng et al/2019/Meng et al. - 2019 - Scaling Local Control to Large-Scale Topological Navigation.pdf:pdf},
title = {{Scaling Local Control to Large-Scale Topological Navigation}},
url = {http://arxiv.org/abs/1909.12329},
year = {2019}
}
@article{Wiles2020,
abstract = {We propose a new approach to determining correspondences between image pairs under large changes in illumination, viewpoint, context, and material. While most approaches seek to extract a set of reliably detectable regions in each image which are then compared (sparse-to-sparse) using increasingly complicated or specialized pipelines, we propose a simple approach for matching all points between the images (dense-to-dense) and subsequently selecting the best matches. The two key parts of our approach are: (i) to condition the learned features on both images, and (ii) to learn a distinctiveness score which is used to choose the best matches at test time. We demonstrate that our model can be used to achieve state of the art or competitive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization.},
archivePrefix = {arXiv},
arxivId = {2007.08480},
author = {Wiles, Olivia and Ehrhardt, Sebastien and Zisserman, Andrew},
eprint = {2007.08480},
file = {:home/matias/Documents/Mendeley Desktop/Wiles, Ehrhardt, Zisserman/2020/Wiles, Ehrhardt, Zisserman - 2020 - D2D Learning to find good correspondences for image matching and manipulation.pdf:pdf},
month = {jul},
title = {{D2D: Learning to find good correspondences for image matching and manipulation}},
url = {http://arxiv.org/abs/2007.08480},
year = {2020}
}
@article{Banino2018,
abstract = {Deep neural networks have achieved impressive successes in fields ranging from object recognition to complex games such as Go 1,2 . Navigation, however, remains a substantial challenge for artificial agents, with deep neural networks trained by reinforcement learning 3-5 failing to rival the proficiency of mammalian spatial behaviour, which is underpinned by grid cells in the entorhinal cortex 6 . Grid cells are thought to provide a multi-scale periodic representation that functions as a metric for coding space 7,8 and is critical for integrating self-motion (path integration) 6,7,9 and planning direct trajectories to goals (vector-based navigation) 7,10,11 . Here we set out to leverage the computational functions of grid cells to develop a deep reinforcement learning agent with mammal-like navigational abilities. We first trained a recurrent network to perform path integration, leading to the emergence of representations resembling grid cells, as well as other entorhinal cell types 12 . We then showed that this representation provided an effective basis for an agent to locate goals in challenging, unfamiliar, and changeable environments - optimizing the primary objective of navigation through deep reinforcement learning. The performance of agents endowed with grid-like representations surpassed that of an expert human and comparison agents, with the metric quantities necessary for vector-based navigation derived from grid-like units within the network. Furthermore, grid-like representations enabled agents to conduct shortcut behaviours reminiscent of those performed by mammals. Our findings show that emergent grid-like representations furnish agents with a Euclidean spatial metric and associated vector operations, providing a foundation for proficient navigation. As such, our results support neuroscientific theories that see grid cells as critical for vector-based navigation 7,10,11, demonstrating that the latter can be combined with path-based strategies to support navigation in challenging environments.},
author = {Banino, Andrea and Barry, Caswell and Uria, Benigno and Blundell, Charles and Lillicrap, Timothy and Mirowski, Piotr and Pritzel, Alexander and Chadwick, Martin J. and Degris, Thomas and Modayil, Joseph and Wayne, Greg and Soyer, Hubert and Viola, Fabio and Zhang, Brian and Goroshin, Ross and Rabinowitz, Neil and Pascanu, Razvan and Beattie, Charlie and Petersen, Stig and Sadik, Amir and Gaffney, Stephen and King, Helen and Kavukcuoglu, Koray and Hassabis, Demis and Hadsell, Raia and Kumaran, Dharshan},
doi = {10.1038/s41586-018-0102-6},
file = {:home/matias/Documents/Mendeley Desktop/Banino et al/2018/Banino et al. - 2018 - Vector-based navigation using grid-like representations in artificial agents.pdf:pdf},
issn = {14764687},
journal = {Nature},
month = {may},
number = {7705},
pages = {429--433},
pmid = {29743670},
publisher = {Springer US},
title = {{Vector-based navigation using grid-like representations in artificial agents}},
url = {http://dx.doi.org/10.1038/s41586-018-0102-6 http://www.nature.com/articles/s41586-018-0102-6},
volume = {557},
year = {2018}
}
@book{Vukobratovic1990,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility. {\textcopyright} J Can Dent Assoc 2004.},
address = {Berlin, Heidelberg},
author = {Bressmann, Tim},
booktitle = {Journal of the Canadian Dental Association},
doi = {10.1007/978-3-642-83006-8},
file = {:home/matias/Documents/Mendeley Desktop/Bressmann/2004/Bressmann - 2004 - Self-inflicted cosmetic tongue split A case report.pdf:pdf},
isbn = {978-3-642-83008-2},
issn = {14882159},
keywords = {Self mutilation/complications,Speech,Tongue/injuries},
number = {3},
pages = {156--157},
pmid = {15003161},
publisher = {Springer Berlin Heidelberg},
title = {{Self-inflicted cosmetic tongue split: A case report}},
url = {http://link.springer.com/10.1007/978-3-642-83006-8},
volume = {70},
year = {2004}
}
@inproceedings{Brooks1985,
abstract = {Mobile robots sense their environment and receive error laden readings. They try to move a certain distance and direction, and do so only approximately. Rather than try to engineer these problems away it may be possible, and may be necessary, to develop map making and navigation algorithms which explicitly represent these uncertainties, but still provide robust performance. The key idea is to use a relational map, which is rubbery and stretchy, rather than try to place observations in a 2-d coordinate system.},
author = {Brooks, Rodney A.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1985.1087348},
file = {:home/matias/Documents/Mendeley Desktop/Brooks/1985/Brooks - 1985 - Visual map making for a mobile robot.pdf:pdf},
isbn = {0818606150},
issn = {10504729},
pages = {824--829},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Visual map making for a mobile robot}},
url = {http://ieeexplore.ieee.org/document/1087348/},
volume = {2},
year = {1985}
}
@book{Siegwart2011,
abstract = {Mobile robots range from the teleoperated Sojourner on the Mars Pathfinder mission to cleaning robots in the Paris Metro. Introduction to Autonomous Mobile Robots offers students and other interested readers an overview of the technology of mobilitythe mechanisms that allow a mobile robot to move through a real world environment to perform its tasksincluding locomotion, sensing, localization, and motion planning. It discusses all facets of mobile robotics, including hardware design, wheel design, kinematics analysis, sensors and perception, localization, mapping, and robot control architectures.The design of any successful robot involves the integration of many different disciplines, among them kinematics, signal analysis, information theory, artificial intelligence, and probability theory. Reflecting this, the book presents the techniques and technology that enable mobility in a series of interacting modules. Each chapter covers a different aspect of mobility, as the book moves from low-level to high-level details. The first two chapters explore low-level locomotory ability, examining robots' wheels and legs and the principles of kinematics. This is followed by an in-depth view of perception, including descriptions of many "off-the-shelf" sensors and an analysis of the interpretation of sensed data. The final two chapters consider the higher-level challenges of localization and cognition, discussing successful localization strategies, autonomous mapping, and navigation competence. Bringing together all aspects of mobile robotics into one volume, Introduction to Autonomous Mobile Robots can serve as a textbook for coursework or a working tool for beginners in the field.},
author = {Siegwart, Roland and Nourbakhsh, Ilah Reza and Scaramuzza, Davide},
booktitle = {Choice Reviews Online},
doi = {10.5860/choice.49-1492},
file = {:home/matias/Documents/Mendeley Desktop/Siegwart, Nourbakhsh, Scaramuzza/2011/Siegwart, Nourbakhsh, Scaramuzza - 2011 - Introduction to autonomous mobile robots.pdf:pdf},
isbn = {9780262015356},
issn = {0009-4978},
number = {03},
pages = {49--1492--49--1492},
publisher = {MIT Press},
title = {{Introduction to autonomous mobile robots}},
volume = {49},
year = {2011}
}
@inproceedings{Tarrio2015,
abstract = {In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based visual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image intensities as direct methods. In particular, the information extracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the structural information provided when pixels are treated as edges. Edge extraction is an efficient and higly parallelizable operation. The edge depth information extracted is dense enough to allow acceptable surface fitting, similar to modern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has similar drift than state of the art feature-based and direct methods, and is a simple algorithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that successfully stabilizes an unmanned air vehicle in complex indoor environments using only a frontal camera, while running the complete solution in the embedded hardware on board the vehicle.},
author = {Tarrio, Juan Jose and Pedre, Sol},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.87},
file = {:home/matias/Documents/Mendeley Desktop/Tarrio, Pedre/2015/Tarrio, Pedre - 2015 - Realtime edge-based visual odometry for a monocular camera.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {702--710},
publisher = {IEEE},
title = {{Realtime edge-based visual odometry for a monocular camera}},
url = {http://ieeexplore.ieee.org/document/7410444/},
volume = {2015 Inter},
year = {2015}
}
@article{Rull1993,
abstract = {We have taken a set of well-understood algorithms and combined them in a commercial application: train-spotting. An autonomous train-spotting system requires expertise in active vision, tracking and model-based recognition. Results are demonstrated from real image sequences showing that BARRY can indeed perform its task satisfactorily for many hours without the sustenance that human spotters require (flasks of hot tea, sandwiches etc.). The maximum train velocity is at present limited, but we demonstrate theoretically that the system could in principle operate with trains travelling at relativistic speeds, with obvious application to future space-based spotting.},
author = {Rull, Pippa Hilary},
file = {:home/matias/Documents/Mendeley Desktop/Rull/1993/Rull - 1993 - {BARRY} An Autonomous Train-Spotter.pdf:pdf},
journal = {Image & Vision Computing NZ},
pages = {499--506},
title = {{{BARRY}: An Autonomous Train-Spotter}},
year = {1993}
}
@article{Holladay2017,
abstract = {We pioneer a new future in robotic dust collection by introducing passive dust-collecting robots that, unlike their predecessors, do not require locomotion to collect dust. While previous research has exclusively focused on active dust-collecting robots, we show that these robots fail with respect to practical and theoretical aspects, as well as human factors. By contrast, passive robots, through their unconstrained versatility, shine brilliantly in all three metrics. We present a mathematical formalism of both paradigms followed by a user study and field study.},
archivePrefix = {arXiv},
arxivId = {1703.08736},
author = {Holladay, Rachel M. and Srinivasa, Siddhartha S.},
eprint = {1703.08736},
file = {:home/matias/Documents/Mendeley Desktop/Holladay, Srinivasa/2017/Holladay, Srinivasa - 2017 - A New Paradigm for Robotic Dust Collection Theorems, User Studies, and a Field Study.pdf:pdf},
title = {{A New Paradigm for Robotic Dust Collection: Theorems, User Studies, and a Field Study}},
url = {http://arxiv.org/abs/1703.08736},
year = {2017}
}
@article{Cadena2016,
abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved.},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/matias/Documents/Mendeley Desktop/Cadena et al/2016/Cadena et al. - 2016 - Past, present, and future of simultaneous localization and mapping Toward the robust-perception age(2).pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
url = {http://arxiv.org/abs/1606.05830},
volume = {32},
year = {2016}
}
@article{Cadena2016a,
abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved.},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/matias/Documents/Mendeley Desktop/Cadena et al/2016/Cadena et al. - 2016 - Past, present, and future of simultaneous localization and mapping Toward the robust-perception age.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
url = {http://ieeexplore.ieee.org/document/7747236/},
volume = {32},
year = {2016}
}
@article{Thrun1999,
abstract = {Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/matias/Documents/Mendeley Desktop/Thrun, Burgard, Fox/2002/Thrun, Burgard, Fox - 2002 - Probabilistic Robotics Draft.pdf:pdf},
isbn = {0262201623},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {52--57},
pmid = {20926156},
title = {{Probabilistic robotics}},
volume = {45},
year = {2002}
}
@article{Dong2017a,
abstract = {Continuous-time trajectory representations are a powerful tool that can be used to address several issues in many practical simultaneous localization and mapping (SLAM) scenarios, like continuously collected measurements distorted by robot motion, or during with asynchronous sensor measurements. Sparse Gaussian processes (GP) allow for a probabilistic non-parametric trajectory representation that enables fast trajectory estimation by sparse GP regression. However, previous approaches are limited to dealing with vector space representations of state only. In this technical report we extend the work by Barfoot et al. [1] to general matrix Lie groups, by applying constant-velocity prior, and defining locally linear GP. This enables using sparse GP approach in a large space of practical SLAM settings. In this report we give the theory and leave the experimental evaluation in future publications.},
archivePrefix = {arXiv},
arxivId = {1705.06020},
author = {Dong, Jing and Boots, Byron and Dellaert, Frank},
eprint = {1705.06020},
file = {:home/matias/Documents/Mendeley Desktop/Dong, Boots, Dellaert/2017/Dong, Boots, Dellaert - 2017 - Sparse Gaussian Processes for Continuous-Time Trajectory Estimation on Matrix Lie Groups.pdf:pdf},
journal = {arXiv},
title = {{Sparse Gaussian Processes for Continuous-Time Trajectory Estimation on Matrix Lie Groups}},
url = {http://arxiv.org/abs/1705.06020},
year = {2017}
}
@book{Seeger2004,
abstract = {In this book we will be concerned with supervised learning, which is the problem of learning input-output mappings from empirical data (the training dataset). Depending on the characteristics of the output, this problem is known as either regression, for continuous outputs, or classification, when outputs are discrete.},
author = {Rasmussen, Carl Edward and Williams, Chris K. I.},
doi = {10.1142/S0129065704001899},
file = {:home/matias/Documents/Mendeley Desktop/Rasmussen, Williams/2006/Rasmussen, Williams - 2006 - Gaussian processes for machine learning.pdf:pdf},
isbn = {9780262182539},
issn = {01290657},
pmid = {15112367},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning.}},
year = {2006}
}
@book{Hartley2004,
author = {Hartley, Richard and Zisserman, Andrew},
doi = {10.1016/S0143-8166(01)00145-2},
edition = {Second},
file = {:home/matias/Documents/Mendeley Desktop/Hartley, Zisserman/2004/Hartley, Zisserman - 2004 - Multiple View Geometry in Computer Vision.pdf:pdf},
isbn = {978-0-521-54051-3},
issn = {0368492X},
keywords = {Artificial intelligence,Computer,Cybernetics,Machine vision,Publication,Robotics},
pages = {673},
publisher = {Cambridge University Press},
title = {{Multiple View Geometry in Computer Vision}},
year = {2004}
}
@incollection{P.Murphy2012,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
address = {Berlin/Heidelberg},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:home/matias/Documents/Mendeley Desktop/P. Murphy/1991/P. Murphy - 1991 - (book)Machine Learning A Probabilistic Perspective.pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
pages = {73--78,216--244},
pmid = {20236947},
publisher = {Springer-Verlag},
title = {{Machine Learning}},
url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0_2 http://www.springerreference.com/index/doi/10.1007/SpringerReference_35834},
year = {1991}
}
@article{Forster2015a,
abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1512.02363},
author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2597321},
eprint = {1512.02363},
file = {:home/matias/Documents/Mendeley Desktop/Forster et al/2017/Forster et al. - 2017 - On-Manifold Preintegration for Real-Time Visual-Inertial Odometry.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Computer vision,sensor fusion,visual-inertial odometry (VIO)},
number = {1},
pages = {1--21},
title = {{On-Manifold Preintegration for Real-Time Visual-Inertial Odometry}},
url = {http://arxiv.org/abs/1512.02363},
volume = {33},
year = {2017}
}
@article{Galvez-Lopez2012,
abstract = {We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22 ms/frame in a sequence with 26300 images that is one order of magnitude faster than previous approaches. {\textcopyright} 2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {1610.06475},
author = {G{\'{a}}lvez-L{\'{o}}pez, Dorian and Tard{\'{o}}s, Juan D.},
doi = {10.1109/TRO.2012.2197158},
eprint = {1610.06475},
file = {:home/matias/Documents/Mendeley Desktop/G{\'{a}}lvez-López, Tardos/2012/G{\'{a}}lvez-López, Tardos - 2012 - Bags of binary words for fast place recognition in image sequences.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Bag of binary words,computer vision,place recognition,simultaneous localization and mapping (SLAM)},
month = {oct},
number = {5},
pages = {1188--1197},
pmid = {309728700020},
title = {{Bags of binary words for fast place recognition in image sequences}},
url = {http://ieeexplore.ieee.org/document/6202705/},
volume = {28},
year = {2012}
}
@article{David2011,
abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Es- tablished leaders in the field are the SIFT and SURF al- gorithms which exhibit great performance under a variety of image transformations, with SURF in particular consid- ered as the most computationally efficient amongst the high- performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A compre- hensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art al- gorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
author = {Tovaglieri, Alessio},
doi = {10.3929/ethz-a-010782581},
file = {:home/matias/Documents/Mendeley Desktop/David et al/2011/David et al. - 2011 - Research Collection.pdf:pdf},
isbn = {8610828378018},
journal = {BRISK Binary Robust Invariant Scalable Keypoints},
number = {3},
pages = {12--19},
title = {{Research Collection}},
url = {https://doi.org/10.3929/ethz-a-010025751},
volume = {15},
year = {2011}
}
@book{Chirikjian2001,
abstract = {An experimental study of the detectability of an object embedded in optically tissue-equivalent media by frequency-domain image reconstruction is presented. The experiments were performed in an 86-mm-diameter cylindrical phantom containing an optically homogeneous cylindrical target whose absorption and scattering properties presented a 2:1 contrast with the background medium. The parameter space explored during experimentation involved object size (15-, 8-, and 4-mm targets) and location (centered, 20-mm off-centered, and 35-mm off-centered) variations. Image reconstruction was achieved with a previously reported regularized least-squares approach that incorporates finite-element solutions of the diffusion equation and Newton's method solutions of the nonlinear minimization problem. Also included during image formation were image enhancement schemes - (1) total variation minimization, (2) dual meshing, and (3) spatial low-pass filtering - which have recently been added. Quantitative measures of image quality including the size, location, and shape of the heterogeneity along with errors in its recovered optical property values are used to quantify the image reconstructions. The results show that a near 22:1 ratio of tissue thickness relative to detectable object size has been achieved with this approach in the laboratory conditions and parameter space that have been investigated. {\^{A}}{\textcopyright} 1997 Optical Society of America.},
author = {Chirikjian, Gregory S.},
booktitle = {Engineering Applications of Noncommutative Harmonic Analysis},
doi = {10.1201/9781420041767},
file = {:home/matias/Documents/Mendeley Desktop/Chirikjian/2000/Chirikjian - 2000 - Engineering Applications of Noncommutative Harmonic Analysis.pdf:pdf},
isbn = {0849307481},
publisher = {CRC Press},
title = {{Engineering Applications of Noncommutative Harmonic Analysis}},
year = {2000}
}
