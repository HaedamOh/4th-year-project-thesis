\chapter{Background}
\label{chap:background}

% This chapter introduces the main concepts and methods used in the remainder of this thesis. I begin by presenting basic notation and mathematical definitions in \secref{sec:preliminaries}. Then I present the core ideas of least squares optimisation in \secref{sec:optimisation}, as it is the core machinery used for the systems developed in this work. \secref{sec:platforms-sensors} introduces the main sensors used in this work, providing further discussion on camera modelling. I close this chapter with \secref{sec:algorithms-navigation} introducing algorithms for robot navigation, which will be further discussed in the Literature Review (\chapref{chap:lit-review}).

% % =============================================================================
% \section{Preliminaries}
% \label{sec:preliminaries}

% \subsection{Notation}
% To refer to different mathematical objects, such as vectors, matrices or reference frames, we will use different typography styles. The conventions adopted in this thesis are summarised in \tabref{tab:notation}.

% \begin{table}[h]
% 	\centering
% 	\caption{Notation used for mathematical objects.}
% 	\begin{tabular}{lll}
% 		\toprule
% 		\textbf{Quantity} & \textbf{Description}   & \textbf{Example}                         \\
% 		\midrule
% 		Scalars           & Upper/Lowercase italic & The traversability score $s$             \\
% 		Vectors           & Lowercase bold         & The 3D point $\mathbf{p}$                \\
% 		Matrices          & Uppercase bold         & The rotation matrix $\mathbf{R}$         \\
% 		Sets/Manifolds    & Uppercase calligraphic & The map points $\mathcal{P}$             \\
% 		Reference Frames  & Uppercase typewriter   & The \emph{map} frame $\M{}$            \\
% 		                  & (legacy expression)    & The \emph{world} frame $\legacyframe{W}$ \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:notation}
% \end{table}

% \subsection{Rotations and Poses}
% Rotation and rigid-body matrices are particularly important in robotics, as they are widely used to describe how objects are placed and moved in space. In this thesis we adopt the formalisation of \emph{Lie groups} to represent them.

% In brief, Lie groups are defined as both a \emph{group} and a \emph{differentiable manifold}. By being a group: (1) they define a binary operation, (2) the operation is associative, (3) there is an identity element, and (4) each element has an inverse. 
% For example, for rotation and rigid-body matrices (1) is defined by matrix multiplication, which also satifies (2). For (3) they have the identity matrix $\identity$, and (4) is the matrix inverse.

% Their differentiable manifold side reflects the smooth geometric constraints that define a valid member of the group. For example, a 3D orientation has 3 \gls{dof} and it can be represented by a 3D vector using Euler angles or an axis-angle representation~\cite{Barfoot2017}. However, not all vectors in $\R{3}$ are valid orientations, and the constraints that define a valid Euler angle are not encoded in the representation. In contrast, a rotation matrix is an over-parametrised representation ($9$ entries) but with orthonormality constraints; this allows them to effectively represent the intrinsic $3$ \gls{dof} of a valid orientation in a smooth manner---which defines the manifold.

% This section provides basic definitions used across the manuscript; additional machinery is further explained in the specific chapters when required. For an in-depth, technical presentation refer to \textcite{Sola2018} or \textcite{Calinon2020}; for a practical overview, see my tutorial~\cite{Mattamala2021b}.

% \subsubsection{Rotation Matrices}
% Rotation matrices are formally part of the \emph{Special Orthogonal Group} or $\SOn{}$. It is the set of all matrices $\rotation{}{}$ that satisfy orthonormality $\rotation{}{}^{\top}\rotation{}{} = \identity$, as well as $\det{\rotation{}{}} = 1$.

% In this thesis we are interested in the space of 2D rotations on the plane $\SOtwo$, as well as the space of 3D rotations $\SOthree{}$.

% \subsubsection{Rigid-body Matrices}
% The group of rigid-body matrices is known as \emph{Special Euclidean Group} or $\SEn$. They represent a relative 6 \gls{dof} transformation or \emph{pose}, i.e. traslation and rotation. Their matrix structure is given by:
% \begin{equation}
% 	\pose{}{} = \matrixSE{ \rotation{}{} }{ \nvec{t} }
% \end{equation}

% In this thesis we are concerned specifically about the group of planar transformations $\SEtwo$, where $\rotation{}{} \in \SOtwo$ and $\nvec{t} \in \R{2}$, as well as the group of 3D transformations $\SEthree$, where $\rotation{}{} \in \SOthree$ and $\nvec{t} \in \R{3}$.

% \subsection{Frames}
% The mathematical quantities we will use are not abstract concepts but they are associated to the physical world. Concretely, they are represented with respect to \emph{reference frames} that define a \emph{point-of-view} used to describe them. This is particularly important not only for modelling~\cite{Furgale2014} but also the definitions used in software tools widely used in robotics such as the \gls{ros}~\cite{Meeussen2010}. The main reference frames we use in this work are shown in \tabref{tab:frames}.
% \begin{table}[h]
% 	\centering
% 	\caption{Main reference frames used in this work.}
% 	\begin{tabular}{cl}
% 		\toprule
% 		\textbf{Frame} & \textbf{Description}                                                         \\
% 		\midrule
% 		$\B$           & \emph{base} or \emph{body} frame specified at the center of the robot's body \\
% 		$\I$           & \emph{odometry} or \emph{inertial} fixed frame used by odometry estimators   \\
% 		$\M$           & \emph{map} fixed frame to specify the origin of the robot's map              \\
% 		$\Sensor$      & \emph{sensor} frame usually defined with respect to the base frame           \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:frames}
% \end{table}

% We can then endow the aforementioned mathematical notation with special subindices to specify their relationship with the relevant frames. This is graphically illustrated in \figref{fig:frames}.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics{figures/inkscape/frames}
% 	\caption[short version]{Main reference frames and frame-endowed notation used in this thesis. We adopt the axes colour convention with the \textcolor{CustomRed}{x-axis in \textbf{red}}, \textcolor{CustomGreen}{y-axis in \textbf{green}}, and \textcolor{CustomBlue}{z-axis in \textbf{blue}}. The inertial frame $\I$ and the map frame $\M$ are fixed, the base frame $\B$ moves with the robot, while the sensor frame $\Sensor$ is fixed relative to $\B$ through the rigid-body transformation $\pose{\M}{\B}$. The linear velocity of the base is given by the vector $\point{v}{\B}$. The fixed 3D point in the frame $\I$ is denoted by $\point{p}{\I}$.}
% 	\label{fig:frames}
% \end{figure}

% % =============================================================================
% \section{Optimisation}
% \label{sec:optimisation}
% Optimisation is at the core of many robotics problems. In particular, squared cost minimisation problems, known as \gls{ls}, are one of the most commonly used formulations to solve state estimation, motion planning, and learning~\cite{Dellaert2017,Bishop2006}. We review the basic concepts and methods that will be used in the following chapters of the thesis.

% \subsection{Linear Least Squares}
% We begin with the simplest \gls{ls} formulation. Let us consider a linear model:
% \begin{equation}
% 	f(\nvec{x}) = \nmat{A}\nvec{x} + \nvec{b}
% \end{equation}
% with $\nvec{x} \in \R{n}$ an unknown variable, and $\nvec{b} \in \R{m}$, and $\nmat{A} \in \R{m\times n}$ hyper-parameters of the model.

% We would like to estimate the value of the variable $\nvec{x}$, given empirical samples of the linear model $\nvec{y}_{i} \in \R{m}$, with $i=1,\ldots,N$. For some value of $\nvec{x}$, we can characterise how well the linear model approximates each sample $\nvec{y}_{i}$ by means of the \emph{residual function}:
% \begin{equation}
% 	\nvec{r}_{i} (\nvec{x}) =  \nmat{y}_{i} - \left(\nmat{A}_{i}\nvec{x} + \nvec{b}_{i} \right)
% \end{equation}

% Then, the \gls{ls} estimate of $\nvec{x}$, denoted $\nest{\nvec{x}}$, is the estimate that minimises the \emph{cost} or \emph{loss} function $\loss{}\left( \nvec{x}\right)$ defined as the sum of the squared residuals for all the data samples:
% \begin{align}
% 	\nest{\nvec{x}} & = \argmin_{\nvec{x}} \loss{}\left( \nvec{x}\right)                                                                         \\
% 	                & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsq{\nvec{r}_{i} (\nvec{x})}{}                                                          \\
% 	                & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsqexp{\left( \nmat{y}_{i} - \left(\nmat{A}_{i}\nvec{x} + \nvec{b}_{i} \right) \right)}
% \end{align}

% Sometimes not all the data samples have the same importance, and we would like to scale the contribution of the set of samples or specific dimensions. We introduce an additional matrix $\nmat{\Sigma}_{i}$, usually diagonal, as a hyper-parameter to control this:
% \begin{align}
% 	\label{eq:weighted-ls}
% 	\nest{\nvec{x}} & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsq{\nvec{r}_{i} (\nvec{x})}{\nmat{\Sigma}_i}                                                                   \\
% 	                & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsqexpcov{\nvec{r}_{i} (\nvec{x}) }{\nmat{\Sigma}_{i}}                                                          \\
% 	                & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsqexpcov{\left( \nmat{y}_{i} - \left(\nmat{A}_{i} \nvec{x} + \nvec{b}_{i} \right) \right) }{\nmat{\Sigma}_{i}}
% \end{align}
% which corresponds to the most general version of linear \gls{ls} we will consider in the rest of the manuscript.

% \subsubsection{Solving the Problem}
% To solve \eqref{eq:weighted-ls}, it is necessary to linearise the quadratic cost. As the problem is convex, the linearisation produces a set of linear equations for each residual known as \emph{normal equations}:
% \begin{align}
% 	\nmat{A}_{1}^{\top}\, \nmat{\Sigma}_{1}^{-1}\, \nmat{A}_{1}\, \nest{\nvec{x}} & = \nmat{A}_{1}^{\top}\, \nmat{\Sigma}_{1}^{-1}\, \nvec{b}_{1} \\
% 	\cdots                                                                        &                                                               \\
% 	\nmat{A}_{N}^{\top}\, \nmat{\Sigma}_{N}^{-1}\, \nmat{A}_{N}\, \nest{\nvec{x}} & = \nmat{A}_{N}^{\top}\, \nmat{\Sigma}_{N}^{-1}\, \nvec{b}_{N}
% \end{align}

% which can be rearranged and stacked together as a single linear system:
% \begin{equation}
% 	\label{eq:normal-equations}
% 	\left( \nmat{A}^{\top}\, \nmat{\Sigma}^{-1}\, \nmat{A}\, \right) \nest{\nvec{x}} = \nmat{A}^{\top}\, \nmat{\Sigma}^{-1}\, \nvec{b}
% \end{equation}

% The matrix on the left-hand side is known as the \emph{Hessian}:
% \begin{equation}
% 	\label{eq:hessian}
% 	\nmat{A}^{\top} \nmat{\Sigma}^{-1} \nmat{A}
% \end{equation}

% By inverting the Hessian, we can recover the estimate of $\nest{\nvec{x}}$ in closed-form:
% \begin{equation}
% 	\nest{\nvec{x}} = \left( \nmat{A}^{\top} \nmat{\Sigma}^{-1} \nmat{A} \right)^{-1} \nmat{A}^{\top} \nmat{\Sigma}^{-1} \nvec{b}
% \end{equation}

% While this process is straightforward, it can be expensive to compute when the dimensionality of $\nvec{x}$ is large. Standard procedures to solve \gls{ls} include factorising the Hessian in the normal equations (\eqref{eq:normal-equations}) using Cholesky or QR factorisarion~\cite{Golub2013}.

% Exploiting the structure of the Hessian matrix with problem-specific knowledge has also been heavily exploited in robotics to achieve real-time performance. Some examples are Square Root SAM~\cite{Dellaert2006} and \gls{isam}~\cite{Kaess2008} for incremental state estimation, or KOMO for receding-horizon motion planning~\cite{Toussaint2017}.

% \subsubsection{Probabilistic Interpretation}
% The formulation of \gls{ls} used in \eqref{eq:weighted-ls} has a probabilistic interpretation: It is the \gls{map} formulation of an estimation problem with random variables that follow a Gaussian distribution. In particular, if we consider the residual to follow a zero-mean Gaussian distribution with covariance $\nmat{\Sigma}_{i}$:
% \begin{equation}
% 	\label{eq:gaussian-factor}
% 	\nvec{r}_{i} (\nvec{x}) \sim \gaussian{\nvec{0}}{\nmat{\Sigma}_{i}}
% \end{equation}

% Then the \gls{map} is given by:
% \begin{equation}
% 	\label{eq:map}
% 	\nest{\nvec{x}} = \argmax_{\nvec{x}} \prod_{i}^{N} { \fullgaussian{ -\frac{1}{2} \lsqexpcov{ \nvec{r}_{i} (\nvec{x}) }{\nmat{\Sigma}_{i} } }{ \nmat{\Sigma}_{i} }{m} }
% \end{equation}

% To show that it is equivalent to \gls{ls}, we simply compute the negative logarithm of \eqref{eq:map}:
% \begin{equation}
% 	\label{eq:map-sq}
% 	\nest{\nvec{x}} = \argmin_{\nvec{x}} \sum^{N}_{i} \frac{1}{2} \lsqexpcov{\nvec{r}_{i} (\nvec{x}) }{\nmat{\Sigma}_{i}} + \ln{\left( \gaussiannorm{\nmat{\Sigma}_{i}}{m} \right)}
% \end{equation}
% As we are only interested in the terms that depend on $\nvec{x}$, we can disregard the term on the right, as well as the scaling $\frac{1}{2}$. This optimisation is then equivalent to \eqref{eq:weighted-ls}.

% Additionally, we can rewrite \eqref{eq:map} as:
% \begin{equation}
% 	\label{eq:factors}
% 	\nest{\nvec{x}} = \argmax_{\nvec{x}} \prod_{i}^{N} { \phi(\nvec{x}) }
% \end{equation}
% Which expresses the problem as a product of factors. This has an equivalent representation in the graphical models literature, known as a \emph{factor graph}~\cite{Dellaert2017}. Hence, for the particular case of Gaussian factor graphs, where factors are defined by \eqref{eq:gaussian-factor}, solving an inference problem on the factor graph is equivalent to \gls{ls}.


% \subsubsection{Interpretations of the Hessian}
% The Hessian (\eqref{eq:hessian}) has important interpretations. Geometrically, the Hessian is the second derivative of the \gls{ls} cost function. Hence, it represents the curvature around the minimum. If we consider the cost function as a manifold, then the Hessian also defines the \emph{Riemannian metric} of the manifold~\cite{Toussaint2017}---this idea will be important for the local planner presented in \chapref{chap:local-planning}. Probabilistically, it is the \emph{Fisher Information Matrix} $\nmat{I}_{\mathrm{Fisher}}$. Its inverse corresponds to the covariance matrix of the \gls{map} estimate $\nmat{I}_{\mathrm{Fisher}}^{-1} = \nest{\nmat{\Sigma}}$. When the posterior of the \gls{map} is fitted with $\gaussian{ \nest{\nvec{x}} }{\nest{\nmat{\Sigma}}}$, the approximation is known as \emph{Laplace approximation}~\cite{Bishop2006}.

% The probabilistic interpretation allows us to study the \gls{ls} solution further. For example, under the Gaussian approximation we can determine the \emph{entropy} $H$ of the estimate, which represents its quality. It is defined as:
% \begin{equation}
% 	H\left(\nest{\nvec{x}} \right) = \frac{1}{2}n \left(1 + \ln{(2\pi)}) \right) + \frac{1}{2} \ln{\left( \det{\nest{\nmat{\Sigma}}} \right)}
% \end{equation}

% We can use the entropy to compare the quality different \gls{ls} estimates. However, we observe that (1) it only depends on $\nest{\nmat{\Sigma}}$, so we can ignore the constant terms, (2) we require to invert $\nmat{I}_{\mathrm{Fisher}}$ to obtain $\nest{\nmat{\Sigma}}$. Hence, we can use the \emph{negative entropy} as proposed by \textcite{Kuo2020}:
% \begin{equation}
% 	E\left( {\nest{\nvec{x}}} \right) = \ln{\left( \det{ \nmat{I}_{\mathrm{Fisher}} } \right)} = \ln{\left( \det{ \left( \nmat{A}^{\top} \nmat{\Sigma}^{-1} \nmat{A} \right) } \right) }
% \end{equation}
% This is a positive scalar value that will be used in \chapref{chap:visual-localisation} to compare different estimates obtained from \gls{ls}.

% \subsection{Non-linear Least Squares}
% The previous formulation of \gls{ls} is useful to understand the basic principles but it is not the one usually found in real robotics problems. A more general approach is minimise a cost $\loss{}\left( \nvec{x}\right)$ with squared non-linear residuals, known as \gls{nls}:
% \begin{align}
% 	\label{eq:weighted-nls}
% 	\nest{\nvec{x}} & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsq{\nvec{r}_{i} (\nvec{x})}{\nmat{\Sigma}_i}     \\
% 	                & = \argmin_{\nvec{x}} \sum^{N}_{i} \lsq{ f(\nvec{x}, \nvec{y}_{i}) }{\nmat{\Sigma}_i}
% \end{align}
% with $f$ a non-linear function of the variables $\nvec{x}$ and the data $\nvec{y}_{i}$.

% \subsubsection{Solving the Problem}
% To solve \gls{nls}, the most standard procedure is linearising the residuals in \eqref{eq:weighted-nls} with respect to an operation point $\nlin{\nvec{x}}$:
% \begin{equation}
% 	\label{eq:linearisation}
% 	\nvec{r}_{i} (\nvec{x}) \approx \nvec{r}_{i} (\nlin{\nvec{x}} + \delta\nvec{x}) \approx \nvec{r}_{i} (\nlin{\nvec{x}}) + \jac{\nvec{r}_{i}} \left( \nlin{\nvec{x}} \right) \delta\nvec{x}
% \end{equation}
% where $\jac{\nvec{r}_{i}}$ is the \emph{Jacobian} of the residual function with respect to $\nvec{x}$.
% Then, \eqref{eq:weighted-nls} becomes a \gls{ls} problem on the increment $\delta\nvec{x}$ that we can solve in closed-form:
% \begin{align}
% 	\label{eq:linearised-nls}
% 	\delta\nest{\nvec{x}} & = \argmin_{\delta\nvec{x}} \sum^{N}_{i} \lsq{ \nvec{r}_{i} (\nlin{\nvec{x}}) + \jac{\nvec{r}_{i}} \left( \nlin{\nvec{x}} \right) \delta\nvec{x} }{\nmat{\Sigma}_{i}} \\
% 	                      & = \left( \jac{}^{\top} \nmat{\Sigma}^{-1} \jac{} \right)^{-1} \jac{}^{\top} \nmat{\Sigma}^{-1} \nlin{\nvec{x}}
% \end{align}
% with $\jac{}$ a matrix built by stacking the Jacobians of all the residuals $\jac{\nvec{r}_{i}}$.
% The increment $\delta\nest{\nvec{x}}$ is used to update the current estimate of $\nvec{x}$ by using the rule:
% \begin{equation}
% 	\label{eq:update-rule}
% 	\nvec{x} \leftarrow \nlin{\nvec{x}} + \delta\nest{\nvec{x}}
% \end{equation}

% This is the basis of the \gls{gn} algorithm. It solves the \gls{nls} problem by starting from an initial estimate, linearising the cost around it (\eqref{eq:linearisation}), solving the normal equations (\eqref{eq:linearised-nls}), and iteratively updating the estimate (\eqref{eq:update-rule}) until convergence.

% \subsubsection{Alternative Optimisation Approaches}
% Although \gls{gn} provides an straightforward framework to solve \gls{nls}, it has some drawbacks, as computing and inverting the Hessian can be expensive. This is particularly the case when the nonlinear function $f$ is parametrised by a neural network. An alternative approach is defining a simpler rule based on the gradient of the cost function $\grad{\loss{}\left( \nvec{x}\right)}$, known as \emph{gradient descent}:
% \begin{equation}
% 	\label{eq:gradient-descent-rule}
% 	\nvec{x} \leftarrow \nlin{\nvec{x}} - \gamma \grad{\loss{}} \left( \nlin{\nvec{x}} \right)
% \end{equation}

% Where $\gamma$ is known as \emph{step size} or \emph{learning rate}.
% In practice, due to the non-convexity of neural networks, \gls{gd} will not lead to the steepest descent direction. Instead, we can sample random data batches of size $B$ to evaluate the gradient for each sample $i$, $\grad{\loss{i}}\left( \nvec{x}\right)$. Then, we can use the average gradient for the update rule, which is known as \gls{sgd}:
% \begin{equation}
% 	\label{eq:sgd-rule}
% 	\nvec{x} \leftarrow \nlin{\nvec{x}} -  \frac{\gamma}{B} \sum_{i}^{B}{\grad{\loss{i}}\left( \nlin{\nvec{x}} \right)}
% \end{equation}

% Nowadays more sophisticated optimisation methods exists, such as Adam~\cite{Kingma2015}, which are based on \gls{sgd}. Adam is later used to solve an online learning problem in \chapref{chap:traversability-learning}.

% \subsubsection{On the Interpretations of \gls{nls}}
% As a last remark, the same geometric and probabilistic interpretations of \gls{ls} hold for \gls{nls} but only for the current linearisation point. Further, as the optimisation is non-convex, the estimates of the mean and covariance from the Laplace approximation do not hold globally, and they can under or over estimate the covariance of the estimate~\cite[109]{Barfoot2017}.

% % =============================================================================
% \section{Platforms and Sensors}
% \label{sec:platforms-sensors}
% In this section we review the main sensors and platforms used in this work. We first discuss \emph{exteroceptive} sensors, which measure the environment external to the robot. We focus on visual sensors, i.e. cameras, as they are the main sensor used in this thesis. \gls{lidar} sensing is also discussed as it is used in later chapters. \emph{Proprioceptive} sensing, which provides information about the robot's internal state, is just briefly presented.

% We then provide a general overview about legged robots, their distinctive features, and how they differ from other platforms such as wheeled and aerial. This will prepare us for the last section of this chapter, that discusses robot navigation systems.

% \subsection{Cameras}
% Cameras are ubiquitous sensors: they are low-cost, provide rich appearance information about the external world, and operate at reasonable frequency (\SIrange{10}{30}{\hertz}) given the typical speed of the majority of robotics platforms. When combined with other cameras (stereo systems) or active lighting (e.g. structured light), they can also measure 3D information. The main cameras used in this thesis are the Intel Realsense camera, and the Sevensense Core Research multi-camera unit, shown in \figref{fig:cameras}.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[height=3cm,clip,trim={0cm -0.2cm 0cm -0.2cm}]{figures/cameras/realsense.png}
% 	\hspace{1cm}
% 	\includegraphics[height=2.7cm,clip,trim={0cm -0.2cm 0cm -0.2cm}]{figures/cameras/alphasense.png}
% 	\caption{Main camera units used in this work. \textbf{Left:} Intel \emph{Realsense D435i} stereo and depth visual-inertial unit. Source: Intel. \textbf{Right:} Sevensense \emph{Core Research} multi-camera visual-inertial sensor (formerly \emph{Alphasense Core}). Source: Sevensense.}
% 	\label{fig:cameras}
% \end{figure}

% Their basic operating principle is based on a regular array of light-sensitive \emph{pixels}, known as \emph{digital image sensor}. Each pixel outputs a signal that is proportional to the number of photons received in a fixed \emph{exposure time}, which also determines the maximum operating frequency (\SIrange{10}{30}{\hertz}). Colour cameras additionally have colour filters for each pixel (red, green, blue); their arrangement is known as \emph{Bayer pattern}.

% Producing a visually appealing image from the pixels' signals and the Bayer pattern requires low-level processing provided by a \gls{isp}. The \gls{isp} is usually implemented on dedicated hardware, and it performs different operations that include demosaicing, gamma correction, white balance, and tone mapping~\cite{Delbracio2021}. While this is usually provided by manufacturers for commercial cameras, this is not necessarily the case for experimental kits such as the Alphasense.

% As part this thesis, a \gls{ros}-compatible \gls{isp} for the Alphasense was developed, which performs debayering, white balancing, and lens undistortion, among other steps. The package is open-source and available as \texttt{raw\_image\_pipeline}\footnote{\url{https://github.com/leggedrobotics/raw_image_pipeline}}.

% Once monochromatic (greyscale) or colour (RGB) images are produced, we can perform further processing~\cite{Corke2011}. As we are interested in obtaining 3D information from the world, we need to model the geometry of the image projection process. This is explained in the following sections.

% \subsubsection{Camera Projection Geometry}
% We begin by modelling the geometry of the projection process, as it has a primary role in \chapref{chap:visual-localisation} and \chapref{chap:traversability-learning}.

% We consider that the camera has a pose $\pose{\M}{\C} \in \SEthree$ in a fixed frame $\M$. In computer vision textbooks~\cite{Szeliski2022,Hartley2004,Ma2004}, this matrix is known as the \emph{camera extrinsics}. In robotics we usually estimate the pose of the robot body's instead of the sensor's, so we define the extrinsics as the product $\pose{\M}{\C} = \pose{\M}{\B} \pose{\B}{\C}$, where $\pose{\M}{\B}$ is the \emph{robot's body pose}, and $\pose{\B}{\C}$ is the \emph{extrinsic calibration matrix of the camera with respect to the robot's body}, which is obtained from prior calibration.

% Then, a 3D point in homogeneous coordinates $\point{p}{\M} = \left[ X, Y, Z, 1 \right] \in \R{4}$ in the map frame $\M$ is related to the point $\point{p}{\Sensor}$ in sensor frame $\Sensor$ by the projection model $\mathbf{\pi}\left(\point{p}{\Sensor}, \point{p}{\M}\right)$, which is illustrated in \figref{fig:pinhole-model}. The specific steps to achieve this are explained as follows.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=\textwidth]{figures/inkscape/pinhole.pdf}
% 	\caption{The pinhole camera model $\mathbf{\pi}\left(\point{p}{\Sensor}, \point{p}{\M}\right)$ projects the 3D point $\point{p}{\M}$ in the map frame, to the image cordinates $\point{p}{\Sensor}$, given the extrinsic matrix $\pose{\M}{\C}$.}
% 	\label{fig:pinhole-model}
% \end{figure}

% \paragraph{Step 1: Represent the Point in the Camera Frame}
% We first change the reference frame of $\point{p}{\M}$ from $\M$ to the \emph{camera} frame $\C$ (also called \emph{optical frame}):
% \begin{equation}
% 	\label{eq:pinhole-extrinsics}
% 	\point{p}{\C} = \pose{\M}{\C}^{-1}\, \point{p}{\M}
% \end{equation}
% The resulting point $\point{p}{\C}$ is also in homogeneous coordinates but now with respect to the frame $\C$.

% \paragraph{Step 2: Pinhole Projection}
% We then apply the \emph{pinhole projection model}:
% \begin{equation}
% 	\label{eq:pinhole-proj}
% 	\point{p}{\Sensor}' = \Kmat{\Sensor}{\C}\, \point{p}{\C}
% 	%
% 	= \begin{bmatrix}
% 		f_x & 0   & c_x & 0 \\
% 		0   & f_y & c_y & 0 \\
% 		0   & 0   & 1   & 0 \\
% 		0   & 0   & 0   & 1
% 	\end{bmatrix}
% 	\begin{bmatrix}
% 		X\ \\
% 		Y\ \\
% 		Z\ \\
% 		1\
% 	\end{bmatrix}
% 	%
% 	= \begin{bmatrix}
% 		f_x X + c_x Z\ \\
% 		f_y Y + c_y Z\ \\
% 		Z\             \\
% 		1\
% 	\end{bmatrix}
% 	=
% 	\begin{bmatrix}
% 		x \\
% 		y \\
% 		z \\
% 		1
% 	\end{bmatrix}
% \end{equation}

% Where $\Kmat{\Sensor}{\C} \in \R{4\times4}$ is the \emph{intrinsic calibration} matrix. $f_x$ and $f_y$ are the camera focal lengths, while $c_x$ and $c_y$ are the coordinates of the center of the image. They are obtained used standard camera calibration packages, such as Kalibr~\cite{Furgale2013,Rehder2016}.

% \paragraph{Step 3: Normalise the Coordinates}
% The point on the image plane $\I$, denoted $\point{p}{\Sensor}$, is obtained by dividing $\point{p}{\Sensor}'$ by the third component. This produces the normalised image coordinates (\textcite{Szeliski2022}):
% \begin{equation}
% 	\label{eq:pinhole-proj-norm}
% 	\point{p}{\Sensor} = \frac{\point{p}{\Sensor}'}{\point{p}{\Sensor}'_{\left[ 3 \right]}}
% 	=
% 	\frac{1}{z}
% 	\begin{bmatrix}
% 		x \\
% 		y \\
% 		z \\
% 		1
% 	\end{bmatrix}
% 	=
% 	\begin{bmatrix}
% 		x / z \\
% 		y / z \\
% 		1     \\
% 		1 / z
% 	\end{bmatrix}
% 	%
% 	= \begin{bmatrix}
% 		u \\
% 		v \\
% 		1 \\
% 		d
% 	\end{bmatrix}
% \end{equation}

% Here $(u,v)$ denote the pixel coordinates of the projected point on the image plane, and $d$ its \emph{disparity} (inverse depth).

% \subsubsection{Moving Back from Images to 3D}
% An advantage of this formulation, is that in order to \emph{back-project} an image point back into the 3D world, we can easily invert the operations:

% \paragraph{Step 1: De-normalisation} Using the disparity $d$:
% \begin{equation}
% 	\label{eq:pinhole-backproj-norm}
% 	\point{p}{\Sensor}' = \frac{\point{p}{\Sensor}}{\point{p}{\Sensor}_{\left[ 4 \right]}}
% 	=
% 	\frac{1}{d}
% 	\begin{bmatrix}
% 		u \\
% 		v \\
% 		1 \\
% 		d
% 	\end{bmatrix}
% 	=
% 	\begin{bmatrix}
% 		u / d \\
% 		v / d \\
% 		1 / d \\
% 		1
% 	\end{bmatrix}
% \end{equation}

% \paragraph{Step 2: Back-project} Using the inverse of the camera instrinsics matrix:
% \begin{equation}
% 	\label{eq:pinhole-backproj}
% 	\point{p}{\C} = \Kmat{\Sensor}{\C}^{-1}\, \point{p}{\I}'
% 	%
% 	= \begin{bmatrix}
% 		1 / f_x & 0       & -c_x / f_x & 0 \\
% 		0       & 1 / f_y & -c_y / f_y & 0 \\
% 		0       & 0       & 1          & 0 \\
% 		0       & 0       & 0          & 1
% 	\end{bmatrix}
% 	\begin{bmatrix}
% 		u / d\ \\
% 		v / d\ \\
% 		1 / d\ \\
% 		1\
% 	\end{bmatrix}
% 	%
% 	= \begin{bmatrix}
% 		f_x / d \left( u - c_x \right) \ \\
% 		f_y / d \left( v - c_y \right)   \\
% 		1 / d\                           \\
% 		1\
% 	\end{bmatrix}
% \end{equation}

% Please note that this is not possible for all kinds of cameras. Monocular cameras only provide the image coordinates $(u,v)$ and the disparity $d$ is not observable. In this case, the resulting $\point{p}{\C}$ is valid \emph{up to a scale factor} $1/d$. This is also interpreted as monocular cameras being \emph{bearing sensors}: they can only measure a ray (direction) to the true 3D point.

% For stereo cameras, we can estimate the disparity by using the \emph{baseline} $b$ of the camera, which is typically provided by the manufacturer or custom calibration. Given the measurements $(u_l, v_r)$ and $(u_r, v_r)$ from the left and right cameras of a stereo pair, the disparity is determined by:
% \begin{equation}
% 	\label{eq:disparity-stereo}
% 	d = \frac{u_l - u_r}{b}
% \end{equation}

% For depth cameras, the distance to the 3D point, denoted $z$, is measured directly via structured light or \gls{tof} imaging:
% \begin{equation}
% 	\label{eq:disparity:depth}
% 	d = \frac{1}{z}
% \end{equation}

% \paragraph{Step 3: Back to World Frame} Finally we obtain the 3D point in the world frame $\M$ using the extrinsics matrix:
% \begin{equation}
% 	\label{eq:pinhole-backproj-extrinsics}
% 	\point{p}{\M} = \pose{\M}{\C}\, \point{p}{\C}
% \end{equation}

% The aforementioned steps are usually summarised by the \emph{projection function} $\mathbf{\pi}(\cdot)$ and \emph{back-projection function} $\mathbf{\pi}^{-1}(\cdot)$.

% \subsubsection{Applications}
% The pinhole model expresses the fundamental principles that relate images with the 3D structure of the environment. This enables different applications for robotics that are used in the subsequent chapters of this thesis, which are briefly discussed here.

% \paragraph{Visual Localisation} In \chapref{chap:visual-localisation} we will apply the pinhole model for visual localisation using the \gls{pnp}~\cite{Fischler1981} algorithm and factor graph-based pose estimation. These methods minimise the \emph{reprojection error} between a set of image measurements $\point{z}{\Sensor}_{i}$ and the 2D projection of 3D map points $\point{p}{\M}_{i}$, as illustrated in \figref{fig:pinhole-application-localisation} and further discussed in \secref{sec:navigation-state-estimation}.
% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/inkscape/visual_localisation.pdf}
% 	\caption{\emph{Visual localisation} methods minimise the reprojection error the idealised projection of 3D points $\point{p}{\M}_{i}$ and image measurements $\point{z}{\Sensor}_{i}$. We use these principles in \chapref{chap:visual-localisation}.}
% 	\label{fig:pinhole-application-localisation}
% \end{figure}

% \paragraph{Elevation Mapping} The principles of camera geometry can be applied to produce dense point clouds using stereo cameras or \gls{tof} imaging, and then generate a local 2.5D elevation map, as shown in \figref{fig:pinhole-application-elevation}. This local representation is used for efficient local planning in \chapref{chap:local-planning}.
% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/inkscape/elevation_mapping.pdf}
% 	\caption{\emph{Elevation mapping} relies on the back-projected image points using stereo or depth information to ray-cast them onto a planar grid that encodes the point's height. This grid representation is used for local planning in \chapref{chap:local-planning}. }
% 	\label{fig:pinhole-application-elevation}
% \end{figure}

% \paragraph{Self-supervision} In \chapref{chap:traversability-learning}, we will use the projection model to project the path traversed by the robot by integrating the footprint over time (see \figref{fig:pinhole-application-selfsupervision}). We will use this information as a supervision signal to learn a model to predict which areas are accessible by the robot. 
% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/inkscape/self_supervision.pdf}
% 	\caption{By integrating the robot's traversed path over time and projecting it onto past images, we can generate a \emph{supervision signal} used to train models of terrain traversability, as will be shown in \chapref{chap:traversability-learning}}.
% 	\label{fig:pinhole-application-selfsupervision}
% \end{figure}

% \subsubsection{Lens Distortion}
% In the previous applications we did not consider the non-linear effects produced by the camera lenses, which distort how the points are projected on the image plane. Lens distortion models compensate for these effects, so the projections and other operations stay invariant.

% For cameras such as the Realsense (\figref{fig:cameras}, left), these effects can be compensated using a \emph{radial-tangential} model~\cite{Brown1966}. Wide-angle cameras such as the Alphasense unit (\figref{fig:cameras}, right) are modelled by \emph{equidistant} distortion~\cite{Xiong1997} instead. Both models are available in camera calibration packages such as Kalibr~\cite{Furgale2013,Rehder2016}, which enable us to use the same projection principles regardless of the specific camera being used.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[height=5cm]{figures/cameras/alphasense_color.png}
% 	\hspace{0.4cm}
% 	\includegraphics[height=5cm]{figures/cameras/alphasense_undistorted.png}
% 	\caption{Fisheye lens distortion in the Sevensense Core Research sensor. \textbf{Left:} Distorted output before fisheye correction, henceforth displaying curved trees. \textbf{Right:} Undistorted image, correctly displaying the trees standing straight. The empty pixels (black) represent areas that are not observed in the original input; these are introduced to match the specified intrinsics after undistortion.}
% 	\label{fig:fisheye-distortion}
% \end{figure}

% \subsection{LiDAR}
% \gls{lidar} is another common exteroceptive sensor in robotics, which provides 3D information about the environment. In contrast to (monocular) cameras, they can directly measure distance, achieving centimetre-accurate range measurement up to \SI{100}{\meter}. 

% Distance measurements are obtained by projecting a laser beam into the environment and measuring the time it takes to return. As this process occurs along a single ray, \glspl{lidar} use fixed laser arrays and rotating mirrors to generate 2D or 3D scans. As a result, \glspl{lidar} operate at slightly lower frequencies than cameras, up to \SI{20}{\hertz}. 

% In the remainder of the thesis, \glspl{lidar} are used only as a complementary sensor for evaluation purposes (\chapref{chap:visual-localisation}), or as another complementary source of 3D information (\chapref{chap:local-planning} and \chapref{chap:traversability-learning}). In \chapref{chap:autonomous-forest-inventory} they will acquire a more important role as they provide the main data input for autonomous forest inventory. Some of the devices discussed in this thesis are shown in \figref{fig:lidars}.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[height=3cm,clip,trim={0cm -0.2cm 0cm -0.2cm}]{figures/lidars/velodyne.png}
% 	\hspace{2cm}
% 	\includegraphics[height=3cm,clip,trim={0cm -0.2cm 0cm -0.2cm}]{figures/lidars/bpearl.png}
% 	\hspace{2cm}
% 	\includegraphics[height=3cm,clip,trim={0cm -0.2cm 0cm -0.2cm}]{figures/lidars/tls.png}
% 	% \caption{Some \gls{lidar} units relevant for the thesis. \textbf{Left:} Velodyne \emph{VLP-16} \SI{360}{\degree} \gls{lidar}. \textbf{Center:} RoboSense \emph{BPearl} \SI{360}{\degree} $\times$ \SI{90}{\degree} unit. \textbf{Right:} Leica \emph{RTC360} \gls{tls} laser scanner.}
% 	\caption{\glspl{lidar} relevant to this thesis. \textbf{Left:} Velodyne \emph{VLP-16} \gls{lidar}. \textbf{Center:} RoboSense \emph{BPearl}. \textbf{Right:} Leica \emph{RTC360} \gls{tls}.}
% 	\label{fig:lidars}
% \end{figure}


% \subsection{Proprioceptive Sensing}
% Proprioceptive sensors provide information about the robot's internal state. The main examples we consider are \glspl{imu} and joint sensors, as they give important context for the navigation algorithms that will be discussed in \secref{sec:algorithms-navigation}, although their are not the main focus of this thesis.

% \emph{\glspl{imu}} provide high-frequency ($>$\SI{100}{\hertz}) angular velocity and linear acceleration measurements. This enables robots to have access to their attitude with respect to gravity, as well as an estimate of the relative translation by time-integration of the measurements~\cite{Woodman2007}. However, they can suffer from thermo-mechanical effects that degrade this estimate, which depends on the \gls{imu} grade, and additional actions need to be taken to compensate for this effects, such as sensor fusion~\cite{Corke2007}.

% \emph{Joint sensors} provide the position, velocity, or torque of a specific joint. As they are usually part of the actuators' hardware for joint-level control, they also operate at higher frequencies ($>$\SI{1}{\kilo\hertz}). In combination with the robot description (e.g. \gls{urdf}), they can provide real-time information about the kinematic configuration of the robot or interacting forces.

% \subsection{Legged Robots}
% Legged robots are a type of mobile robot characterised by the use of limbs to locomote. Among legged platforms, the main distinctive feature is the number of legs used for locomotion. Engineers have developed single-leg hoppers (1 leg), bipeds and humanoids (2 legs), quadrupeds (4 legs), and hexapods (6 legs)~\cite{Kajita2016}.

% In this thesis we focus on quadrupedal platforms. Quadrupeds offer a good balance between robustness, cost, and control complexity compared to other legged robots, as they have the minimal number of legs to statically locomote~\cite{Hutter2013}.

% The main advantage legged robots offer over other ground platforms, such as wheeled robots, is their advanced mobility skills to traverse different types of terrain, including human-made and natural ones. Compared to aerial platforms such as small-size drones, they can carry heavier payloads in longer missions.

% In \tabref{tab:robot-comparison} we compare the main legged platforms commercially available (as of the writing of this manuscript), to other popular platforms such as wheeled robots and drones. This will be relevant in \chapref{chap:autonomous-forest-inventory} to contextualise the use of legged robot in forestry. It also summarises the general characteristics of the ANYbotics ANYmal~\cite{Hutter2017, Anybotics} robot, which is the main platform used for the development of this thesis.

% % Add table
% \begin{table}[h]
% 	\centering
% 	\caption{Specifications of different commercial robotic platforms. ANYmal C is the main robot used in this thesis.}
% 	\input{chapters/robots_table}
% 	\label{tab:robot-comparison}
% \end{table}

% % =============================================================================
% \section{Algorithms for Robot Navigation}
% \label{sec:algorithms-navigation}

% In this last section we discuss algorithms for robot navigation. We consider the robot navigation pipeline shown in \figref{fig:navigation-pipeline}. It involves the interaction of these modules that address capabilities and representations a robot needs to move in the environment. We will discuss them in the following sections.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/flowcharts/navigation.pdf}
% 	\caption{Modular robot navigation pipeline. We have highlighted in blue the main modules we are concerned with in this thesis. Please refer to \secref{sec:algorithms-navigation} for more details.}
% 	\label{fig:navigation-pipeline}
% \end{figure}

% \subsection{State Estimation}
% \label{sec:navigation-state-estimation}
% State estimation creates representations from sensor data, which are used by the downstream tasks. They include internal state representations (odometry, localisation) as well as scene representations (global and local maps). This is illustrated in \figref{fig:navigation-state-estimation}.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/inkscape/state_estimation.pdf}
% 	\caption{Odometry vs localisation systems as explained in \secref{sec:navigation-state-estimation}. \textbf{(a) Odometry:} It determines the robot's state, usually its pose in the inertial frame $\I$ and the body velocity. \textbf{(b) Localisation:} It is concerned with estimating a consistent pose of the robot with respect to a map in the frame $\M$.}
% 	\label{fig:navigation-state-estimation}
% \end{figure}

% \subsubsection{Odometry}
% \emph{Odometry} systems mainly provide a high-frequency ($>$\SI{100}{\hertz}) estimate of the robot's pose and velocity. The pose estimate is smooth but only locally accurate as it suffers from incremental drift. For this reason, it is represented in a fixed, inertial frame $\I$, known as \emph{odom} frame. This frame will change every time the robot is deployed, as it is usually set at the starting location.

% For legged robots, the odometry is estimated via multi-sensor fusion, in which high-frequency proprioceptive sensing and legged kinematics are combined with low-rate exteroceptive sensors using filtering algorithms~\cite{Bloesch2012,Bloesch2017} or factor graph-based smoothing~\cite{Hartley2018,Wisth2023}. In this thesis, we assume that the robot odometry is given, hence we always have access to the robot's pose in the $\I$ frame as well as the base velocity.

% \subsubsection{Localisation}
% On the other hand, \emph{localisation} systems provide a pose estimate that is consistent with the environment and repeatable with every deployment. Hence they rely on a representation of the environment---a map---which defines its own reference frame $\M$. Localisation systems use exteroceptive sensing to find the robot's location within the map, which is usually implemented in two steps, first finding initial candidates of its approximate location (\emph{place recognition}), and then a metric pose verification from the place candidates (\emph{registration}).

% Compared to odometry systems, localisation is less robot-specific, as it mainly relies on exteroceptive sensing. Still, the sensor setup of each robot is important to define the localisation strategy. In \chapref{chap:visual-localisation} we exploit the different cameras available on a legged robot to achieve robust visual localisation.

% \subsubsection{Global and Local Mapping}
% \emph{Global mapping} systems build large-scale maps, which are used for localisation and global planning. Global maps do not necessarily need to be globally accurate but they must connect all the overlapping areas.
% \emph{Local mapping} systems instead only represent the local neighborhood around the robot, being particularly useful for local planning. For this reason, they usually keep a rolling estimate of the map around the robot, which updates as the robot moves.

% Mapping systems can produce different kinds of maps, which are tailored to different applications. We describe some possible distinctions as follows, following the presentation of \textcite{Burgard2016}.

% \paragraph{Metric vs Topological:} \emph{Metric maps} represent the world with precise coordinates, while \emph{topological maps} represent places and relationships between them. \emph{Topo-metric maps} combine elements of both approaches by using a topological representation for large-scale that defines local reference frames for each place~\cite{Howard2006}.

% \paragraph{2D vs 2.5D vs 3D:} \emph{2D maps} are better suited for flat areas, single-floor indoor environments, such as occupancy-based floor maps. \emph{2.5D} maps also include elevation or height information, which is useful for field applications but cannot characterise overhanging obstacles. \emph{3D maps}, such as voxel or octree maps, fully describe the space and are the most general though more computationally expensive.

% \paragraph{Sparse vs Dense:} \emph{Sparse maps} only store landmarks, i.e. features that are mainly useful for localisation (e.g. ORB-SLAM's map~\cite{MurArtal2015}). \emph{Dense maps} characterise surfaces and volumes, which can be easier for humans to understand, as well as being more suitable for collision avoidance and navigation (e.g. OctoMaps~\cite{Hornung2013}).

% \paragraph{Discrete vs Continuous:} Most of the maps we are familiar with are \emph{discrete}, as they explicitly represent the space as nodes, points, 2D cells, or voxels, among others. Maps can also parametrise the space using continuous functions such as a mixture of Gaussian distributions, making them \emph{continuous maps} (e.g. Hilbert maps~\cite{Ramos2015}).

% \paragraph{Geometric vs Semantic:}	\emph{Geometric maps} only represent geometric features such as volume, normals, or occupancy. \emph{Semantic maps} store human-understandable features, such as classes, object labels, or other task-dependent features, such as energy consumption or \emph{traversability}.

% \subsection{Local Planning}
% Local planning systems aim to safely drive the robot through the environment to reach a goal. To achieve this they typically use the local map, and execute path planning or reactive control algorithms to output a \emph{twist} command (linear and angular velocity) to be executed by the locomotion controller.  \figref{fig:navigation-local-planning} illustrates this process.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics{figures/inkscape/local_planning.pdf}
% 	\caption{Local planning systems aim to reach a goal while avoiding obstacles. To achieve this, they use the traversability information encoded in the local map to find a safe path (white) used to determine the twist command which would allow the locomotion controller to follow it.}
% 	\label{fig:navigation-local-planning}
% \end{figure}

% \subsubsection{Traversability}
% In order to determine what effort is required to reach the goal and which hazards exist along the route, local planners must know which areas are accessible by the robot, and what is the effort and hazards associated to move there as well as user preferences---this is known as \emph{traversability}~\cite{Borges2022}. The concept of traversability is abstract, and for it to be useful we need to represent it numerically. In the literature, it is more common to use \emph{occupancy}, which corresponds to a binary metric of traversability. More broadly, the concepts of \emph{cost}, \emph{risk}, \emph{reward}, or simply \emph{traversability score}, are continuous metrics of traversability used to distinguish between different levels of navigation preference. 

% \figref{fig:background-traversability} presents a taxonomy for traversability estimation approaches we use in \secref{sec:traversability-estimation} to review the state of the art. This will be useful to contextualise the contributions of \chapref{chap:traversability-learning}, where we present an online learning approach to determine traversability in legged platforms.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics{figures/inkscape/traversability.pdf}
% 	\caption{Different approaches to determine the traversability score in a local map. \textbf{(a) From geometry:} It is obtained from geometric features of the local map, such as surface normals. \textbf{(b) From semantics:} By using semantic segmentation approaches, it is possible to determine what is traversable by assigning costs to different semantic classes. \textbf{(c) From self-supervision:} It uses information that the robot generates itself from past trajectories or future predictions. \textbf{(d) From anomalies:} It considers the visited places as traversable, and any out-of-distribution sample as untraversable. \textbf{(e) From demonstrations:} It obtains a cost map from demonstrations in an inverse reinforcement learning fashion.}
% 	\label{fig:background-traversability}
% \end{figure}

% \subsubsection{Motion Generation Strategies}
% The traversability map is used to find a safe path or control command to steer the robot towards the goal. There are different strategies to generate the desired motion:

% \paragraph{Grid-based Local Planning:} It uses a grid representation of the environment that encodes the traversability. By considering the grid as a lattice graph, we plan using algorithms such as Dijkstra~\cite{Dijkstra1959}, A$^\ast$~\cite{Hart1968}, or \gls{fmm}~\cite{Sethian1996}. The path is then tracked by a dedicated module that produces twists commands. 

% \paragraph{Sampling-based Local Planning:} This approach finds explicit paths by sampling the space and checking the validity of the paths by evaluating its traversability and other costs. If the sampling is guided to find a path towards the goal, it is known as a \emph{single-query planner}; RRT~\cite{LaValle1998} is a classical example. If the search covers the full local map and the path network can be reused if the goal changes, it is a \emph{multi-query planner}---such as PRMs~\cite{Kavraki1996}. These methods also need a dedicated module to track the found path.

% \paragraph{Optimisation-based Local Planning:} Instead parametrises the path with a continuous function and formulates an optimisation that enforces a minimum cost, defined by a combination of the traversability along the path, plus other criteria like path smoothness. Some examples are CHOMP~\cite{Zucker2013} and GPMP2~\cite{Mukadam2018}.

% \paragraph{Motion Primitives:} Also known as \emph{lattice planners}, these methods directly define candidate paths by kinematic models (e.g. Dynamic Window Approach ~\cite{Fox1997}), sampling (e.g. Model Predictive Path Integral (MPPI) control~\cite{Williams2016}), or pre-computed trajectories (e.g. FALCO~\textcite{Zhang2020}).

% \paragraph{Reactive Approaches:} These approaches do not find an explicit path but determine the output twist based on the instantaneous local map information or raw sensing data. This is achieved by generating potential functions or manifolds for navigation, such as \glspl{apf}~\cite{Khatib1985} and \glspl{rmp}~\cite{Ratliff2018}, or via end-to-end neural networks~\cite{Kahn2021}.

% Deciding which local planning approach to use ultimately depends on the robot platform, its sensing capabilities, computation budget, and locomotion strategy. Grid-based, sampling, and optimisation approaches are usually preferred when the robot has slow dynamics and the planning horizon is larger, so finding an optimal path that leverages all the costs is more important that the time it takes to determine it. Motion primitives and reactive approaches are instead preferred when the platforms are highly dynamic, and staying safe is a stronger requirement than the optimality of the path. In \chapref{chap:local-planning} we present a local planner that enables a legged robot to navigate in challenging narrow spaces leveraging reactive and grid-based ideas.


% \subsection{Locomotion Control}
% The twist command produced by the local planner is executed by a \emph{low-level locomotion controller}, which translates the reference into position and torque commands for the actuators using robot-specific models. For legged robots, these are \emph{whole-body locomotion} controllers that fully specify the body configuration to generate the walking motion. If the controllers integrate exteroceptive sensing or local maps they are named \emph{perceptive}; otherwise, they are called \emph{blind} controllers.

% \paragraph{Model-based Controllers:} They implement the walking behaviour as an optimisation problem~\cite{Wieber2016, Farshidian2017, Mastalli2020}. Given a desired gait pattern and reference base pose, they solve a \gls{to} or \gls{mpc} problem to determine the footstep sequence and base configuration. This allows for precise control of the gait and feet positions but they are less robust to unmodelled perturbations or perception noise.

% \paragraph{Reinforcement Learning-based Controllers:} They frame legged locomotion as learning a \emph{policy} that maximises a reward function that motivates walking behaviour~\cite{Hwangbo2019, Siekmann2021}. They are usually trained in simulation through millions of trials, where the robot specifications, perception input, and environment conditions are heavily randomised to handle unexpected situations during real deployment. Although they have demonstrated impressive performance in real-world applications, they still lack the precise locomotion control from model-based approaches.

% In this thesis we do not approach the legged locomotion problem and we assume that controllers are available from the manufacturer or research collaborators~\cite{Lee2020, Miki2022a}. However, the solutions we present are designed to comply with their specifications and build on top of their capabilities to achieve autonomous navigation. 

% \subsection{Mission Planning}
% For the purposes of this thesis, we will consider as a \emph{mission} a list of potential goals the robot must navigate to. Then, \emph{mission planners} are the interface to design and execute missions: on the one hand, they enable human operators to specify the places that must be visited; on the other, they schedule the goals and interact with the local planner to execute the navigation behaviour.

% In \chapref{chap:visual-localisation} and \chapref{chap:local-planning}, the mission planner will be specified by a \gls{vtr} system, in which a human operator defines a path in advance, and then the robot has to track it for inspection and monitoring applications. Later, in \chapref{chap:autonomous-forest-inventory} we will present a mission planner to autonomously survey forests.