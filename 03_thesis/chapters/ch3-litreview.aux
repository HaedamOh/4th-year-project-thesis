\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Literature Review}{34}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Literature Review}{34}{chapter.3}\protected@file@percent }
\@writefile{lot}{\contentsline {xchapter}{Literature Review}{34}{chapter.3}\protected@file@percent }
\newlabel{chap:lit-review}{{3}{34}{Literature Review}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Localisation and Mapping}{34}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Metric}{34}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Metric localisation methods such as ORB-SLAM~\cite {MurArtal2015,MurArtal2017a,MurArtal2017b,Campos2021} build a 3D map that is globally accurate. Source: University of Zaragoza.\relax }}{35}{figure.caption.63}\protected@file@percent }
\newlabel{fig:literature-localisation-metric}{{3.1}{35}{Metric localisation methods such as ORB-SLAM~\cite {MurArtal2015,MurArtal2017a,MurArtal2017b,Campos2021} build a 3D map that is globally accurate. Source: University of Zaragoza.\relax }{figure.caption.63}{}}
\@gls@reference{acronym}{sfm}{\glsnoidxdisplayloc{}{page}{glsnumberformat}{35}}
\@gls@reference{acronym}{slam}{\glsnoidxdisplayloc{}{page}{glsnumberformat}{35}}
\@writefile{toc}{\contentsline {subsubsection}{Place Recognition}{36}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Registration}{36}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Alternative Methods}{37}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Topological}{37}{subsection.3.1.2}\protected@file@percent }
\newlabel{sec:topological-localisation}{{3.1.2}{37}{Topological}{subsection.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Topological methods, e.g. SeqSLAM~\cite {Milford2012}, only rely on sequences of images without metric information. \textbf  {Left:} The similarity matrix is obtained by computing the global descriptor distance between all the images in a sequence. \textbf  {Right}: Examples of same-place localisation matches across different seasons that can be obtained by methods such as SeqSLAM. Source: \textcite {Neubert2019}.\relax }}{38}{figure.caption.67}\protected@file@percent }
\newlabel{fig:literature-localisation-topological}{{3.2}{38}{Topological methods, e.g. SeqSLAM~\cite {Milford2012}, only rely on sequences of images without metric information. \textbf {Left:} The similarity matrix is obtained by computing the global descriptor distance between all the images in a sequence. \textbf {Right}: Examples of same-place localisation matches across different seasons that can be obtained by methods such as SeqSLAM. Source: \textcite {Neubert2019}.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Dub4~\cite {Linegar2015,Linegar2015} is an example of topo-metric localisation as it builds a topological map at large-scale that stores metric information locally. Source: \textcite {Linegar2016b}.\relax }}{39}{figure.caption.68}\protected@file@percent }
\newlabel{fig:literature-localisation-topometric}{{3.3}{39}{Dub4~\cite {Linegar2015,Linegar2015} is an example of topo-metric localisation as it builds a topological map at large-scale that stores metric information locally. Source: \textcite {Linegar2016b}.\relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Topo-metric}{39}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Local Planning}{40}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Classical}{40}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Classical local planning uses traditional planning techniques to direct the robot towards a goal defined by a human or mission planning system. \textbf  {Left:} A$^{\ast }$-based planning on the BigDog quadruped. Source: \textcite {Wooden2010}. \textbf  {Right:} Potential field-based local planner for the MiniCheetah robot. Source: \textcite {Kim2020}.\relax }}{41}{figure.caption.69}\protected@file@percent }
\newlabel{fig:literature-local-planning-classical}{{3.4}{41}{Classical local planning uses traditional planning techniques to direct the robot towards a goal defined by a human or mission planning system. \textbf {Left:} A$^{\ast }$-based planning on the BigDog quadruped. Source: \textcite {Wooden2010}. \textbf {Right:} Potential field-based local planner for the MiniCheetah robot. Source: \textcite {Kim2020}.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces ArtPlanner is a hybrid local planning method that uses learned edge costs within a classical LazyPRM$^{\ast }$ planning scheme. Source: \textcite {Wellhausen2023}.\relax }}{42}{figure.caption.70}\protected@file@percent }
\newlabel{fig:literature-local-planning-hybrid}{{3.5}{42}{ArtPlanner is a hybrid local planning method that uses learned edge costs within a classical LazyPRM$^{\ast }$ planning scheme. Source: \textcite {Wellhausen2023}.\relax }{figure.caption.70}{}}
\@gls@reference{acronym}{cvar}{\glsnoidxdisplayloc{}{page}{glsnumberformat}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Hybrid}{42}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}End-to-end}{43}{subsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces End-to-end methods use neural models from raw-sensing to control commands. \textbf  {Left:} ALVINN is one of the earliest applications of this approach for autonomous driving. Source: \textcite {Pomerleau1988}. \textbf  {Right:} An application of end-to-end methods for navigation using sparse \gls {lidar} data. Source: \textcite {Kim2022}.\relax }}{44}{figure.caption.71}\protected@file@percent }
\newlabel{fig:literature-local-planning-end-to-end}{{3.6}{44}{End-to-end methods use neural models from raw-sensing to control commands. \textbf {Left:} ALVINN is one of the earliest applications of this approach for autonomous driving. Source: \textcite {Pomerleau1988}. \textbf {Right:} An application of end-to-end methods for navigation using sparse \gls {lidar} data. Source: \textcite {Kim2022}.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Geometric and semantic approaches to traversability estimation. \textbf  {Left:} STEP~\cite {Fan2021} uses different geometric features to get a risk metric (\gls {cvar}). \textbf  {Right:} SELMap~\cite {Ewen2022} predicts terrain parameters using a semantic segmentation network.\relax }}{45}{figure.caption.72}\protected@file@percent }
\newlabel{fig:literature-geometric-semantic}{{3.7}{45}{Geometric and semantic approaches to traversability estimation. \textbf {Left:} STEP~\cite {Fan2021} uses different geometric features to get a risk metric (\gls {cvar}). \textbf {Right:} SELMap~\cite {Ewen2022} predicts terrain parameters using a semantic segmentation network.\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Traversability Estimation}{45}{section.3.3}\protected@file@percent }
\newlabel{sec:traversability-estimation}{{3.3}{45}{Traversability Estimation}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Geometry}{45}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Semantics}{46}{subsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Traversability from self-supervision and demonstrations. \textbf  {Left:} Self-supervision based on the reprojection of footholds into images, presented by \textcite {Wellhausen2019}. \textbf  {Right:} Explicit demonstrations can be leveraged in an inverse reinforcement learning framework, as carried out by \textcite {Gan2022}.\relax }}{47}{figure.caption.73}\protected@file@percent }
\newlabel{fig:review-self-supervised}{{3.8}{47}{Traversability from self-supervision and demonstrations. \textbf {Left:} Self-supervision based on the reprojection of footholds into images, presented by \textcite {Wellhausen2019}. \textbf {Right:} Explicit demonstrations can be leveraged in an inverse reinforcement learning framework, as carried out by \textcite {Gan2022}.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Self-supervised}{47}{subsection.3.3.3}\protected@file@percent }
\@gls@reference{acronym}{lagr}{\glsnoidxdisplayloc{}{page}{glsnumberformat}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Anomaly Detection}{48}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Learning from Demonstrations}{49}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Real-world Deployment of Legged Robots}{50}{section.3.4}\protected@file@percent }
\newlabel{sec:real-world-deployment}{{3.4}{50}{Real-world Deployment of Legged Robots}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Competitions}{50}{subsection.3.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Worldwide competitions have fostered the development of legged robots over the years. \textbf  {Left:} RoboCup Standard Platform League (SPL). Source: ubahnverleih, Wikimedia Commons. \textbf  {Center:} \gls {darpa} Robotics Challenge. Source: MIT DRC team. \textbf  {Right:} \gls {darpa} SubT Challenge. Source: NASA/JPL-Caltech.\relax }}{50}{figure.caption.74}\protected@file@percent }
\newlabel{fig:literature-competitions}{{3.9}{50}{Worldwide competitions have fostered the development of legged robots over the years. \textbf {Left:} RoboCup Standard Platform League (SPL). Source: ubahnverleih, Wikimedia Commons. \textbf {Center:} \gls {darpa} Robotics Challenge. Source: MIT DRC team. \textbf {Right:} \gls {darpa} SubT Challenge. Source: NASA/JPL-Caltech.\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Industrial Deployments}{51}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Industrial deployments of legged platforms. \textbf  {Left:} Spot used for radiation mapping. Source: Boston Dynamics. \textbf  {Center:} ANYmal X is a platform certified for explosive atmospheres. Source: ANYbotics. \textbf  {Right:} Digit robots executing a warehousing demo at ProMat 2023. Source: Agility Robotics.\relax }}{52}{figure.caption.75}\protected@file@percent }
\newlabel{fig:literature-companies}{{3.10}{52}{Industrial deployments of legged platforms. \textbf {Left:} Spot used for radiation mapping. Source: Boston Dynamics. \textbf {Center:} ANYmal X is a platform certified for explosive atmospheres. Source: ANYbotics. \textbf {Right:} Digit robots executing a warehousing demo at ProMat 2023. Source: Agility Robotics.\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Natural Deployments}{52}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Natural deployments of legged platforms. \textbf  {Left:} BigDog robot autonomously navigating in a forest. Source: BostonDynamics~\cite {Wooden2010}. \textbf  {Center:} RHex robot collecting soil data in White Sands National Monument. Source: University of Pennsylvania. \textbf  {Right:} K-Rock in crocodile mode in Uganda. Source: EPFL.\relax }}{53}{figure.caption.76}\protected@file@percent }
\newlabel{fig:literature-natural-deployments}{{3.11}{53}{Natural deployments of legged platforms. \textbf {Left:} BigDog robot autonomously navigating in a forest. Source: BostonDynamics~\cite {Wooden2010}. \textbf {Center:} RHex robot collecting soil data in White Sands National Monument. Source: University of Pennsylvania. \textbf {Right:} K-Rock in crocodile mode in Uganda. Source: EPFL.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion}{53}{section.3.5}\protected@file@percent }
\@setckpt{chapters/ch3-litreview}{
\setcounter{page}{54}
\setcounter{equation}{0}
\setcounter{enumi}{6}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{r@tfl@t}{0}
\setcounter{AM@survey}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{9}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{47}
\setcounter{mtc}{4}
\setcounter{minitocdepth}{2}
\setcounter{ptc}{0}
\setcounter{parttocdepth}{2}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{500}
\setcounter{highnamepenalty}{500}
\setcounter{lownamepenalty}{250}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{su@anzahl}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{section@level}{1}
\setcounter{minilofdepth}{2}
\setcounter{minilotdepth}{2}
\setcounter{partlofdepth}{2}
\setcounter{partlotdepth}{2}
\setcounter{sectlofdepth}{2}
\setcounter{sectlotdepth}{2}
}
